{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "2\n",
      "0\n",
      "2\n",
      "c\n",
      "e\n",
      "D\n",
      "4\n",
      "2\n",
      "\n",
      "]\n",
      "L\n",
      "C\n",
      ".\n",
      "s\n",
      "c\n",
      "[\n",
      "\n",
      "2\n",
      "v\n",
      "3\n",
      "4\n",
      "7\n",
      "7\n",
      "1\n",
      ".\n",
      "2\n",
      "1\n",
      "4\n",
      "2\n",
      ":\n",
      "v\n",
      "i\n",
      "X\n",
      "r\n",
      "a\n",
      "\n",
      "YuLan-Mini: An Open Data-efficient Language Model\n",
      "\n",
      "Yiwen Hu∗, Huatong Song∗\n",
      "Jia Deng, Jiapeng Wang, Jie Chen\n",
      "Kun Zhou, Yutao Zhu, Jinhao Jiang, Zican Dong\n",
      "Wayne Xin Zhao†, Ji-Rong Wen\n",
      "Gaoling School of Artificial Intelligence\n",
      "Renmin University of China\n",
      "batmanfly@gmail.com, jrwen@ruc.edu.cn\n",
      "\n",
      "Abstract\n",
      "\n",
      "Effective pre-training of large language models (LLMs) has been challenging\n",
      "due to the immense resource demands and the complexity of the technical pro-\n",
      "cesses involved. This paper presents a detailed technical report on YuLan-Mini,\n",
      "a highly capable base model with 2.42B parameters that achieves top-tier per-\n",
      "formance among models of similar parameter scale. Our pre-training approach\n",
      "focuses on enhancing training efficacy through three key technical contributions:\n",
      "an elaborate data pipeline combines data cleaning with data schedule strategies,\n",
      "a robust optimization method to mitigate training instability, and an effective\n",
      "annealing approach that incorporates targeted data selection and long context\n",
      "training. Remarkably, YuLan-Mini, trained on 1.08T tokens, achieves perfor-\n",
      "mance comparable to industry-leading models that require significantly more data.\n",
      "To facilitate reproduction, we release the full details of the data composition\n",
      "for each training phase. Project details can be accessed at the following link:\n",
      "https://github.com/RUC-GSAI/YuLan-Mini.\n",
      "\n",
      "1\n",
      "\n",
      "Introduction\n",
      "\n",
      "In recent years, large language models (LLMs) [Zhao et al., 2023, Dubey et al., 2024, Qwen-\n",
      "Team, 2024] have significantly advanced the frontier of AI technology. Unlike traditional machine\n",
      "learning methods, which are often specialized, LLMs excel across a diverse range of domains and\n",
      "tasks, showcasing their potential as versatile generalists. Typically, LLMs are developed through a\n",
      "combination of pre-training and post-training techniques. It is widely recognized that pre-training is\n",
      "crucial for building the foundational capabilities of the LLMs [OpenAI, 2023, Touvron et al., 2023,\n",
      "DeepSeek-AI et al., 2024, Yang et al., 2024b].\n",
      "\n",
      "For transformer language models, the prevailing pre-training approach involves next-token predic-\n",
      "tion over large-scale unlabeled texts. Although this approach is conceptually straightforward, its\n",
      "implementation is technically complex. First, researchers must design an effective and efficient\n",
      "data pipeline to support pre-training, typically involving data collection, data cleaning, data mixing,\n",
      "and data curriculum. It is widely recognized that “data” is the most crucial element in enhancing\n",
      "model capabilities. Second, given that LLMs contain a vast number of parameters, the training\n",
      "process is challenging to stabilize and optimize. Common issues such as loss spikes or gradient\n",
      "explosions can occur during training, potentially leading to a failed process. Despite the availability\n",
      "of extensive model checkpoints released by industry companies, the core technical details often\n",
      "remain undisclosed in public reports. As a result, we know very little about how top-tier language\n",
      "models are developed in the industry.\n",
      "\n",
      "∗Team leaders.\n",
      "†Correspondence to Wayne Xin Zhao.\n",
      "\n",
      "1\n",
      "\n",
      "\fFigure 1: Performance comparison of YuLan-Mini against other base models, based on the average\n",
      "scores across eight benchmarks: GSM8K, MATH-500, HumanEval, MBPP, MMLU, ARC-Challenge,\n",
      "HellaSwag, and CEval. Floating Point Operations (FLOPs) are estimated using the scaling law\n",
      "formula C = 6N D proposed by Kaplan et al. [2020], where N is the model size and D is the size of\n",
      "the dataset. The models with a size larger than 3B are plotted in gray.\n",
      "\n",
      "Fortunately, the research community has made significant efforts to enhance the availability of\n",
      "data resources and the openness of training methodologies for LLM pre-training [Allal et al., 2024,\n",
      "Groeneveld et al., 2024, Zhang et al., 2024a]. First, well-curated datasets have been released to\n",
      "support the data preparation required for LLM pre-training. Additionally, various open research\n",
      "publications have documented the overall training procedures, providing a foundational understanding\n",
      "of LLM pre-training [Hu et al., 2024, Li et al., 2024b]. These contributions offer basic technical\n",
      "approaches and essential resources for pre-training an LLM.\n",
      "\n",
      "Despite these advancements, open LLMs—those with fully disclosed technical details—still face\n",
      "two main limitations. First, most of these models tend to underperform compared to their industry\n",
      "counterparts due to constraints in data and computational resources. While some open LLMs achieve\n",
      "performance levels comparable to industry models, they also require similar amounts of resources,\n",
      "making them difficult to replicate within the research community. Therefore, developing competitive\n",
      "LLMs with limited training resources remains a challenge, particularly in university-level laboratories.\n",
      "Given these constraints, we aim to advance the openness of LLM pre-training by significantly\n",
      "enhancing both the performance ceiling and training efficiency of open models. Specifically, we focus\n",
      "on developing relatively small-scale language models, that is, models with a parameter scale ranging\n",
      "from 1B to 3B with a restricted compute budget.3 Our goal is to build a small yet powerful language\n",
      "model using only publicly available data, while sharing experiences or insights on improving training\n",
      "efficiency with limited computational resources.\n",
      "\n",
      "In this paper, we present a comprehensive technical report on the development of a highly capable\n",
      "2.42B-parameter language model (the base model) that achieves top-tier performance among models\n",
      "of similar parameter scale. Building upon the transformer architecture, we devise a data-efficient\n",
      "pre-training approach for LLMs. Our approach includes three major contributions to enhance training\n",
      "efficacy: (1) an elaborately designed data pipeline that combines data cleaning with data schedule\n",
      "strategies; (2) a systematic optimization method that can effectively mitigate training instability;\n",
      "(3) an effective annealing approach that integrate targeted data selection and long context training.\n",
      "We explore a variety of techniques to enhance the performance of YuLan-Mini.\n",
      "In particular,\n",
      "we extensively leverage synthetic data for model training, including o1-like long-thought data.\n",
      "\n",
      "3Some papers refer to pre-trained language models of this size as “small language models”. However, due to\n",
      "\n",
      "the technical similarities, we continue to refer to them as large language models in this paper.\n",
      "\n",
      "2\n",
      "\n",
      "1023Approximate FLOPs40455055606570Avg performance (%)YuLan-Mini-2.4BQwen2-1.5B  Qwen2.5-0.5BOLMo2-7BGemma2-9BQwen2.5-1.5B Qwen2.5-3BQwen2-7BQwen2.5-7BSmolLM2-1.7BLlama3-8BGemma2-2.6B\fAdditionally, we investigate multiple factors that may contribute to training instability. We provide\n",
      "two versions of the checkpoints, supporting 4K and 28K contexts, respectively.4\n",
      "\n",
      "To demonstrate the effectiveness of our pre-trained base model, we conduct extensive experiments on\n",
      "a variety of benchmarks, and compare it with a few competitive base models from both research and\n",
      "industry. Experimental results show that our base model, YuLan-Mini, can achieve very promising\n",
      "results among these compared models. For instance, it (the 28K version) achieves scores of 37.80 on\n",
      "MATH-500 (four-shot), 64.00 on HumanEval (zero-shot), and 49.10 on MMLU (five-shot). Figure 1\n",
      "presents a comparison of YuLan-Mini with other industry models.5\n",
      "\n",
      "To facilitate reproduction, we report the complete training details for YuLan-Mini, and also\n",
      "release the data composition for all training phases (Appendix E). More supporting resources can\n",
      "be accessed at our project link: https://github.com/RUC-GSAI/YuLan-Mini.\n",
      "\n",
      "2 Overall Pre-Training Configuration\n",
      "\n",
      "In this section, we will provide an overview of the pre-training configuration, introducing its key\n",
      "components and the algorithms involved in the process. For a more detailed discussion of the major\n",
      "contributions made in this work, please refer to Section 3, Section 4, and Section 5.\n",
      "\n",
      "2.1 Model Architecture\n",
      "\n",
      "Our model is based on a decoder-only transformer with a tall and narrow architecture, inspired by\n",
      "previous studies [Liu et al., 2024c, Hu et al., 2024]. It comprises a total of 2.42B parameters, of which\n",
      "2.23B are non-embedding parameters. The hyperparameter configurations for our model architecture\n",
      "are provided in Table 1. Additionally, we re-parameterize each weight matrix of different modules\n",
      "with an extra learnable parameter [Nishida et al., 2024], enhancing the model’s training stability\n",
      "(discussed in Section 3). Next, we briefly introduce the main components in our architecture.\n",
      "\n",
      "Embedding tying We utilize embedding tying [Press and Wolf, 2017] to reduce the model’s\n",
      "parameter size and stabilize training. In our preliminary experiments, we find that sharing the\n",
      "embedding and unembedding matrices improves model convergence. Furthermore, when these\n",
      "matrices are not shared, they often necessitate different initialization strategies, which we will discuss\n",
      "in Section 3.\n",
      "\n",
      "Pre-RMSNorm Layer normalization (LN) has been shown to enhance numerical stability and\n",
      "accelerate learning speed [Ba et al., 2016]. We integrate Pre-LN into our model architecture to\n",
      "improve convergence stability and speed compared to Post-LN [Xiong et al., 2020]. Regarding the\n",
      "form of normalization, we opt for RMSNorm over the conventional LayerNorm, as it conserves CUDA\n",
      "memory while attaining a comparable effect [Zhang and Sennrich, 2019].\n",
      "\n",
      "SwiGLU Our model introduces non-linearity using a gated linear unit (GLU) with the Swish\n",
      "activation function, known as SwiGLU Shazeer [2020]. This method effectively captures complex\n",
      "data relationships and has proven to be effective in relatively small language models, as demonstrated\n",
      "by [Liu et al., 2024c].\n",
      "\n",
      "Attention mechanism We adopt the grouped-query attention (GQA, Ainslie et al. [2023]), which\n",
      "enables the model to reduce KV cache usage while maintaining high performance. Specifically, we\n",
      "employ 30 heads for query attention and 6 groups for key-value heads. We opt not to make the\n",
      "KV head size divisible by 8 since small language models rarely require tensor parallelism during\n",
      "inference.\n",
      "\n",
      "Rotary Embedding We adopt rotary positional embedding (ROPE) to capture the positional\n",
      "information in our model, since it integrates absolute and relative positioning in an unified way.\n",
      "\n",
      "4Due to resource constraints, we were only able to train a model with up to 28K context.\n",
      "5We select eight popular benchmarks to cover math, coding, general, and language capabilities. This figure is\n",
      "primarily intended to illustrate the training efficacy of our model, rather than to serve as an accurate capacity\n",
      "ranking of existing models.\n",
      "\n",
      "3\n",
      "\n",
      "\fTable 1: Hyperparameter settings of diffrent models. rffn is the ratio of the feed-forward network’s\n",
      "hidden size to the model’s hidden size. The definition of the symbols is available at Table 8\n",
      "\n",
      "Model\n",
      "\n",
      "nlayers\n",
      "\n",
      "dmodel\n",
      "\n",
      "LLaMA-3.2-3B\n",
      "Phi-3-mini-4k-instruct\n",
      "MiniCPM-2B\n",
      "MiniCPM3-4B\n",
      "Qwen2.5-1.5B\n",
      "MobileLLM-1B\n",
      "\n",
      "YuLan-Mini\n",
      "\n",
      "28\n",
      "32\n",
      "40\n",
      "62\n",
      "28\n",
      "54\n",
      "\n",
      "56\n",
      "\n",
      "3,072\n",
      "3,072\n",
      "2,304\n",
      "2,560\n",
      "1,536\n",
      "1,280\n",
      "\n",
      "1,920\n",
      "\n",
      "rffn\n",
      "\n",
      "2.7\n",
      "2.7\n",
      "2.5\n",
      "2.5\n",
      "5.8\n",
      "2.8\n",
      "\n",
      "2.5\n",
      "\n",
      "nheads\n",
      "\n",
      "nkv_heads\n",
      "\n",
      "24\n",
      "32\n",
      "36\n",
      "40\n",
      "12\n",
      "20\n",
      "\n",
      "30\n",
      "\n",
      "8\n",
      "32\n",
      "36\n",
      "40\n",
      "2\n",
      "5\n",
      "\n",
      "6\n",
      "\n",
      "Table 2: Compression rate of different tokenizers. Higher values indicate more effective compression.\n",
      "\n",
      "Tokenizer\n",
      "\n",
      "Vocabulary Size Web\n",
      "\n",
      "Chinese Math Code\n",
      "\n",
      "Gemma2-2B\n",
      "Qwen2.5\n",
      "LLaMA-3.1\n",
      "MiniCPM-2.4B\n",
      "Phi-3.5-mini\n",
      "MiniCPM-1.2B\n",
      "\n",
      "YuLan-Mini\n",
      "+ Dropout\n",
      "\n",
      "256,000\n",
      "151,936\n",
      "128,000\n",
      "122,753\n",
      "100,352\n",
      "73,440\n",
      "\n",
      "99,000\n",
      "99,000\n",
      "\n",
      "4.928\n",
      "4.935\n",
      "4.994\n",
      "4.753\n",
      "4.311\n",
      "4.631\n",
      "\n",
      "4.687\n",
      "4.687\n",
      "\n",
      "3.808\n",
      "3.956\n",
      "3.263\n",
      "4.273\n",
      "1.914\n",
      "4.042\n",
      "\n",
      "4.147\n",
      "4.146\n",
      "\n",
      "2.865\n",
      "2.890\n",
      "3.326\n",
      "2.739\n",
      "2.654\n",
      "2.696\n",
      "\n",
      "2.716\n",
      "2.715\n",
      "\n",
      "3.309\n",
      "3.881\n",
      "3.911\n",
      "3.052\n",
      "3.110\n",
      "3.017\n",
      "\n",
      "3.033\n",
      "3.031\n",
      "\n",
      "During the stable training stage, we set the parameter θ to 10,000, and increase it to 49 000 during the\n",
      "annealing stage to extend the context length to 28,672 (28K) tokens using adjusted base frequency\n",
      "(ABF).\n",
      "\n",
      "2.2 Tokenizer\n",
      "\n",
      "Tokenization is a critical preprocessing step that splits input text into sequences of tokens. Below, we\n",
      "provide details of our tokenizer.\n",
      "\n",
      "Vocabulary size Generally, the vocabulary size should be chosen to balance its effects on the\n",
      "model’s parameter size and efficiency. We adopt the three approaches proposed by Dagan et al.\n",
      "[2024] to balance the compute budget and vocabulary capacity, yielding a final vocabulary size of\n",
      "around 99,000. For simplicity, we reuse the Byte Pair Encoding (BPE) tokenizer of MiniCPM [Hu\n",
      "et al., 2024]. Specifically, we truncate the vocabulary by applying the corresponding BPE merge\n",
      "rules to reduce the number of tokens. We also heuristically remove rare domain-specific tokens,\n",
      "while add some reserved tokens in the vocabulary. The statistics of the modified vocabulary and the\n",
      "compression rate are shown at Table 2. The test set for the tokenization experiments is sourced from\n",
      "a diverse array of datasets, as detailed in Section 6.3. Overall, our tokenization method achieves a\n",
      "well-balanced compression rate across different domains.\n",
      "\n",
      "BPE-dropout Existing sub-word tokenization methods prevent the language models from under-\n",
      "standing the alphabetic composition of a token. To mitigate this issue, BPE-dropout [Provilkov et al.,\n",
      "2020] has been proposed to help the model better learn the internal representation of a token, enabling\n",
      "it to more effectively capture possible subwords within a word. Specifically, we use a relatively low\n",
      "dropout rate of 0.2, and applying the dropout method results in only a slight increase in the number\n",
      "of tokens (0.07%), as shown in Table 2.\n",
      "\n",
      "Digit tokenization Digit tokenization plays a crucial role in mathematical tasks, including numerical\n",
      "calculation and complex reasoning. We follow the common practice of splitting numbers into\n",
      "individual digits [Bi et al., 2024, Yang et al., 2023]. Although other methods, such as three-digit\n",
      "\n",
      "4\n",
      "\n",
      "\ftokenization, may achieve higher compression rates, using individual-digit tokenization typically\n",
      "leads to improved numerical calculation accuracy [Wang et al., 2024a].\n",
      "\n",
      "2.3 Training Data Preparation\n",
      "\n",
      "Data serves as the foundation for developing the model’s capabilities, and we employ specially\n",
      "designed strategies for collecting and preparing the training dataset. Next, we briefly describe the\n",
      "general procedure for data preparation. A more detailed and comprehensive description of the data\n",
      "pipeline is provided in Section 4.\n",
      "\n",
      "Data collection and selection To ensure reproducibility, our pre-training data is primarily sourced\n",
      "from open-source pretraining datasets and synthetically generated data. The main open-source\n",
      "datasets include FineWeb-Edu [Lozhkov et al., 2024a], the-stack-v2 [Lozhkov et al., 2024b], open-\n",
      "web-math [Paster et al., 2024], Chinese-FineWeb-Edu [Opencsg], and OpenCoder-LLM [Huang\n",
      "et al., 2024]. The entire pre-training dataset has undergone rigorous preprocessing, with 1.08T tokens\n",
      "for training. Among them are 481B English web data, 138B general English knowledge, 227B\n",
      "code pre-training data, 16.7B code instruction data, 93.8B mathematics pre-training data, 15.5B\n",
      "mathematics instruction data, and 108B Chinese data.\n",
      "\n",
      "Data schedule Using the WSD scheduling method [Hu et al., 2024], the training process is divided\n",
      "into three main stages: warmup, stable training, and annealing. The warmup stage uses 10B tokens,\n",
      "the stable training stage utilizes 990B tokens, and the annealing stage uses 80B tokens. To better\n",
      "manage the training process, we divide the entire training trajectory into 27 consecutive curriculum\n",
      "phases, each consisting of 40B tokens. When transitioning between these curriculum phases, the\n",
      "dataset proportions are slightly adjusted based on the model’s performance on various benchmarks\n",
      "and the perplexity (PPL) of validation texts. However, the internal data distribution of each curriculum\n",
      "phase cannot be modified once it has been scheduled for training. During the annealing stage, the\n",
      "proportion of instruction data and long context data is increased.\n",
      "\n",
      "2.4 Model Optimization\n",
      "\n",
      "For model optimization, hyperparameters are crucial for training stability and model performance.\n",
      "\n",
      "Specifically, we adopt the WSD learning rate scheduler [Hu et al., 2024]. Maintaining a constant\n",
      "learning rate during the stable training stage eliminates the necessity to specify an ending step, as\n",
      "required by the cosine scheduler. This approach facilitates continuing pre-training from the last\n",
      "checkpoint during stable training. It also allows for more flexible data preparation: we can prepare the\n",
      "data while the preceding curriculum phase is running. Additionally, we estimate an optimal annealing\n",
      "ratio of 8% for the stable training stage using the scaling law of learning rate annealing [Tissue et al.,\n",
      "2024].\n",
      "\n",
      "For training stability, we combine a parameter initialization approach akin to µP [Dey et al., 2023b,\n",
      "Hu et al., 2024, Yang et al., 2022] with WeSaR re-parameterization [Nishida et al., 2024], using a\n",
      "relatively large global learning rate of 0.01. The rationale behind adopting a large learning rate is\n",
      "the expectation that the model will possess greater potential for enhancement during the annealing\n",
      "stage. We set the AdamW hyper-parameters as follows: β1 = 0.9, β2 = 0.95, ϵ = 10−15, with the\n",
      "weight_decay of 0.1 and the z-loss coefficient of 10−4 [de Brébisson and Vincent, 2016]. We\n",
      "use a variance of 5 × 10−5 for initialization. As found by Wortsman et al. [2024], extending the\n",
      "warm-up ratio enhances training stability, so we linearly warm up the model over 10B tokens. We\n",
      "use a batch size of 4.12M tokens with a sequence length of 4,096, extending the context length\n",
      "during the annealing stage while keeping the total token count in the batch size unchanged. We avoid\n",
      "using gradient accumulation to prevent numerical precision error of bfloat16. Detailed analysis of\n",
      "training stability can be found in Section 3.\n",
      "\n",
      "2.5 Training Infrastructure\n",
      "\n",
      "We build a simple yet efficient training framework based on the HuggingFace Trainer and other\n",
      "open-source libraries (DeepSpeed, flash-attention, and liger-kernel).\n",
      "\n",
      "5\n",
      "\n",
      "\f(a) Training loss.\n",
      "\n",
      "(b) Gradient norm.\n",
      "\n",
      "Figure 2: Training loss and gradients during pre-training process.\n",
      "\n",
      "Specifically, we first use ZeRO-1 [Rajbhandari et al., 2020] data parallelism provided by DeepSpeed\n",
      "intergration and then switch to ZeRO-2 after confirming that it does not cause training divergence in\n",
      "our model.6 We also leverage Flash Attention [Dao et al., 2022, Dao, 2024] and a triton kernel library\n",
      "liger-kernel [Hsu et al., 2024] to accelerate training processes. By employing fused kernels, we\n",
      "achieve a 30% reduction in training time and up to 70% savings in CUDA memory.7 We further\n",
      "optimize the balance between CUDA memory usage and training time by adjusting the number\n",
      "of layers through the activation checkpointing function. For enhanced training efficiency, we use\n",
      "bfloat16 precision for both model parameters and NCCL communications. The model’s FLOPs\n",
      "utilization (MFU) is estimated at 51.57%.\n",
      "\n",
      "Regarding the hardware setup, we initially employ a 56 A800-GPU cluster managed by the SLURM\n",
      "system [Yoo et al., 2003]. We later reduce the number of GPUs to 48 by transitioning the distributed\n",
      "optimizer to a universal checkpoint [Lian et al., 2024]. To maximize device utilization, we perform\n",
      "tokenization and packing asynchronously. Given the modest size of our cluster, the likelihood\n",
      "of encountering NCCL failures is relatively low. Therefore, after assessing the advantages and\n",
      "disadvantages, we decide to store a checkpoint every hour and implement automatic restarts.\n",
      "\n",
      "For efficient evaluation, we utilize LLMBox [Tang et al., 2024b] to integrate vLLM [Kwon et al.,\n",
      "2023] for generative tasks and employ KV cache scheduling for multiple-choice tasks. For a detailed\n",
      "description of the evaluation setup and results, please refer to Section 5.\n",
      "\n",
      "3 Training Stability\n",
      "\n",
      "Training stability is crucial for the efficient pre-training of LLMs. Under normal conditions, loss\n",
      "trajectories are expected to decrease smoothly and remain near their anticipated values consistently,\n",
      "such as those predicted by the scaling law curve, even in the presence of minor perturbations. However,\n",
      "models that are improperly initialized or trained with unsuitable architectures or hyper-parameters\n",
      "may experience marginal stability or outright instability. In a marginally stable network, even a\n",
      "minor abnormal perturbation from the data can push it into a non-steady state. If the network can\n",
      "self-correct, it will experience temporary loss spikes; otherwise, training may diverge.\n",
      "\n",
      "Training stability issue exists even in training relatively language models. For example, as observed\n",
      "empirically by Wortsman et al. [2024], under the same learning rate, the smaller the model, the larger\n",
      "the order of magnitude of the model logits, which is a typical factor for training instability. General\n",
      "methods, such as replacing the data that leads to spikes or reducing the learning rate, usually only\n",
      "palliate superficial problems.\n",
      "\n",
      "Despite significant efforts in the literature to mitigate training instability [Takase et al., 2023, Yang\n",
      "et al., 2022, Wortsman et al., 2024], prior studies typically focus on individual techniques or conduct\n",
      "\n",
      "6https://github.com/microsoft/DeepSpeed/issues/6351\n",
      "7Fused kernels include: SelfAttention, RMSNorm, RoPE, SwiGLU, FusedLinearCrossEntropy, and AdamW.\n",
      "\n",
      "torch.compile is also enabled in our implementation.\n",
      "\n",
      "6\n",
      "\n",
      "0510152025Number of steps (×104)12480510152025Number of steps (×104)0.00.20.40.60.81.0Grad norm\frelatively small-scale experiments. There remains a lack of systematic investigation into the effects\n",
      "of various potential techniques in large-scale pre-training experiments. During our pre-training\n",
      "process, we encounter severe training instability issues, prompting us to conduct an in-depth study\n",
      "on how to address this problem effectively. Our primary approach involves combining a µP-like\n",
      "initialization [Dey et al., 2023a] with a re-parametrization method [Nishida et al., 2024] to adjust the\n",
      "learning rate and stabilize training. In the following, we present a detailed approach for maintaining\n",
      "training stability.\n",
      "\n",
      "3.1 Exploring the Hidden States Variability and Training Instability\n",
      "\n",
      "To effectively mitigate training instability, it is crucial to examine potential indicators of abnormal\n",
      "states. Generally, loss, gradient, and hidden states are three interconnected factors that reflect the\n",
      "dynamics of training. Among these indicators, loss provides surface-level clues about instability, while\n",
      "gradients and hidden states often reveal deeper underlying factors that contribute to a pathological\n",
      "state. We begin by conducting a preliminary experiment to assess the impact of different indicators\n",
      "on training instability. We then analyze the potential reasons theoretically.\n",
      "\n",
      "3.1.1 Preliminary Experiment on Indicators\n",
      "\n",
      "We conduct a preliminary experiment to showcase the effects of tracking our indicators based on\n",
      "hidden states during training.\n",
      "\n",
      "Training setup Since it is resource-intense to perform extensive experiments on our model, we\n",
      "explore the training dynamics by conducting surrogate experiment with a small proxy model of 0.2B\n",
      "with similar architecture. We employ a relatively large learning rate of 0.01, to expose potential\n",
      "instabilities within the model. We keep this baseline model setup in the subsequent experiment, which\n",
      "we elaborate on in Appendix B. Specifically, our optimization goal is to achieve optimal performance\n",
      "while ensuring that the training process does not result in divergent loss or an increasing trend in\n",
      "gradient norm.\n",
      "\n",
      "Indicators setup In large-scale training, distributed optimizers are often used, which means that the\n",
      "gradients of different modules may be distributed across various data parallel ranks. This distribution\n",
      "makes it inefficient to directly obtain the gradients. As a result, we primarily track each module’s\n",
      "weight matrix and hidden states (i.e., their outputs). Specifically, we record the mean and variance\n",
      "of the weights and hidden states, as well as the root mean square (RMS), which is calculated using\n",
      "Var + Mean2. Note we consider the outputs of various modules in the\n",
      "the follow formula RMS =\n",
      "transformer (i.e., FFN, Attention, RMSNorm) as hidden states.\n",
      "\n",
      "(cid:112)\n",
      "\n",
      "Empirical findings Figure 3a illustrates our findings on the relationship between training instability\n",
      "and exploding hidden states. Across all layers, there is a consistent upward trend in both hidden states\n",
      "and gradient norms as the number of training steps increases. Interestingly, these intermediate trends\n",
      "are difficult to detect in the early stages of pre-training when focusing solely on the loss. Moreover,\n",
      "in addition to the temporal dimension, indicators related to hidden states also show an increasingly\n",
      "divergent trend across the depth of the model (i.e., as the number of layers increases). The ratio of\n",
      "the variance of the last layer to that of the first layer grows linearly, indicating a potential future\n",
      "explosion in hidden states and gradient norms. From these results, we can see that hidden states\n",
      "play a significant role in affecting training stability. Based on these observations, our core idea is to\n",
      "monitor and adjust hidden states to maintain training stability.\n",
      "\n",
      "In addition to monitoring hidden states, other indicators can help detect abnormal optimization\n",
      "issues. In our experiments, we also examine another stability indicator known as token embedding\n",
      "variability (TEV) [Chung et al., 2024], which measures the variance of data entries within a vector.\n",
      "By comparing the TEV of two input vectors, we can quantify how differently they are distributed.\n",
      "If the input vectors follow distributions that are significantly different from each other, the output\n",
      "vectors may of course exhibit large fluctuations. These output vectors are essentially a special kind of\n",
      "hidden states and therefore can detect training instability. However, since we are already recording\n",
      "hidden states in our experiments, we did not use TEV as an additional indicator.\n",
      "\n",
      "7\n",
      "\n",
      "\f(a) Exploding hidden states.\n",
      "\n",
      "(b) Convergent hidden states.\n",
      "\n",
      "(c) Loss prediction failure.\n",
      "\n",
      "Figure 3: Comparison of training dynamics between divergent and convergent trial. The y-axis\n",
      "denotes the value of the hidden states variance and gradient norm on a log-scale. Both trials have\n",
      "consistent loss, but different trends of hidden states variance and gradient norm.\n",
      "\n",
      "3.1.2 Theoretical Analysis and Empirical Evidence on Exploding Hidden States\n",
      "\n",
      "In this part, we first formalize the hidden states of transformer, and then derive three potential reasons\n",
      "for exploding hidden states theoretically. We also showcase corresponding empirical evidence, to\n",
      "verify the effectiveness of using hidden states as a training stability indicator.\n",
      "\n",
      "Formalization of hidden states To better analyze how hidden states can be utilized to indicate\n",
      "training stability, we begin by formally defining the hidden states in our model. Since Yulan-Mini is\n",
      "a multi-layer transformer model, and the hidden states of the l-th layer, denoted by zl ∈ Rd, can be\n",
      "specified as follows:\n",
      "\n",
      "zl = yl + FFN(RMSNorm(yl)),\n",
      "yl = xl + MHA(RMSNorm(xl)),\n",
      "where xl ∈ Rd denotes the input of the l-th layer. With sub-layer inputs u = RMSNorm(xl) and\n",
      "v = RMSNorm(yl), the FFN and MHA are defined as:\n",
      "\n",
      "FFN(u) = W2F(W1u),\n",
      "MHA(v) = concath\n",
      "\n",
      "i=1[headi(v)]Wo.\n",
      "\n",
      "(1)\n",
      "\n",
      "(2)\n",
      "\n",
      "It is worth noting that we focus on discussing a simplified version of the FFN layer above. Modern\n",
      "LLAMA-like architectures typically utilize GLU-style non-linearities, which we discuss in Section 3.2.\n",
      "Building on this definition, we specifically focus on the change in the variance of hidden states,\n",
      "as we empirically observe that it tends to increase during the training process. If not properly\n",
      "addressed, this increasing trend could lead to training instability. Considering the relationship\n",
      "var(a + b) = var(a) + var(b). We begin by analyzing the abnormal increase in variance of hidden\n",
      "states that arises inherently from residual connections. Furthermore, we demonstrate how layer\n",
      "normalization can contribute to the variance of hidden states increase, particularly when the inputs\n",
      "deviate significantly from their normal range. Finally, we show that certain abnormal updates in layer\n",
      "normalization are actually driven by the growing mean of attention logits.\n",
      "\n",
      "Exploding hidden states due to residual connection Figure 3a illustrates the exponential growth\n",
      "trend of the variance in the hidden states. To understand the underlying cause, we express the hidden\n",
      "states in terms of the model’s weights and inputs:\n",
      "\n",
      "var(zl) = var(yl) + var(FFN(RMSNorm(yl))),\n",
      "var(yl) = var(xl) + var(MHA(RMSNorm(xl))).\n",
      "\n",
      "For ease of analysis, we first assume that:\n",
      "\n",
      "x, y ∼ N (0, σ2).\n",
      "\n",
      "(3)\n",
      "\n",
      "Under this assumption, we can obtain var(u) = var(v) = 1. In this case, we can express the variance\n",
      "as the following form:\n",
      "\n",
      "var(zl) = var(xl) + var(FFN(u)) + var(MHA(v)),\n",
      "\n",
      "(4)\n",
      "\n",
      "8\n",
      "\n",
      "025005000750010000Training Steps110Layer  1Layer  7Layer 13Layer 19Layer 25Layer 30Grad Norm025005000750010000Training Steps11002000400060008000Training steps1011031051071091011Attention scoresAttention scoresLoss234610Loss\fFigure 4: Variance of LN output of each layers.\n",
      "\n",
      "Figure 5: Attention scores explodes before LN.\n",
      "\n",
      "which means, the hidden states will grow by the variance of MHA and FFN in each layer:\n",
      "var(headi(v)) = var(softmax(Z)V) · dmodel · var(Wv) < dmodel · var(Wv),\n",
      "\n",
      "var(FFN) = dffn · dmodel · var(W1) · var(W2),\n",
      "var(MHA) = var(head(v)) · dmodel · var(Wo) < d2\n",
      "\n",
      "model · var(Wv) · var(Wo),\n",
      "\n",
      "(7)\n",
      "where Z denotes the scaled attention scores. The base dimensionality dmodel of LLMs are often\n",
      "large (e.g., 1,920 in our model). Additionally, the vanilla setup in Hugging Face uses a default\n",
      "initialization standard deviation of 0.02 for the weight matrices. When training the proxy model,\n",
      "this initialization leads to significantly large variance values, exacerbated by the effects of the\n",
      "aforementioned Attention and FFN modules. As a result, the gradient norm becomes very large. A\n",
      "detailed derivation of the attention head can be found in Takase et al. [2023]. Common mitigation\n",
      "strategies include initializing the weights with a very small variance, typically inversely proportional\n",
      "to dmodel, which we will discuss in Section 3.2.\n",
      "\n",
      "(5)\n",
      "(6)\n",
      "\n",
      "Exploding hidden states due to layer normalization According to prior studies [Xiong et al.,\n",
      "2020, Takase et al., 2023, Wortsman et al., 2024], the variance of the input to layer normalization\n",
      "should not be significantly smaller than 1, as this can lead to an increase in the gradient norm:\n",
      "\n",
      "(cid:32) √\n",
      "\n",
      "(cid:33)\n",
      "\n",
      "(cid:13)\n",
      "(cid:13)\n",
      "(cid:13)\n",
      "(cid:13)\n",
      "\n",
      "= O\n",
      "\n",
      "d\n",
      "∥x∥2\n",
      "\n",
      "∂RMSNorm(x)\n",
      "∂x\n",
      "\n",
      "(cid:13)\n",
      "(cid:13)\n",
      "(cid:13)\n",
      "(cid:13)2\n",
      "To investigate the impact of layer normalization, we conduct an experiment examining the variance\n",
      "of the outputs from the LN layer in our model (Figure 4), where the embedding layer is followed\n",
      "by layer normalization. We empirically find, if the embedding layer is initialized with the default\n",
      "variance, we must multiply its output by a scaling factor to ensure the input to the layer normalization\n",
      "has a variance of 1. Therefore, we apply a scaling factor of scale_embed = 10 for model training.\n",
      "For experiments that do not employ embedding tying, we directly set the initialization standard\n",
      "deviation of the embeddings to 1.\n",
      "\n",
      ".\n",
      "\n",
      "Exploding hidden states due to attention scores When applying the scaled embedding method\n",
      "mentioned above, we have empirically observed that attention scores can also explode, resulting in\n",
      "the explosions of hidden states. We first examine the calculation process of attention scores, which is\n",
      "formally defined as:\n",
      "\n",
      "S = XT WT\n",
      "\n",
      "QWKX.\n",
      "\n",
      "Then, we calculate the gradient of the attention scores with respect to the query and key matrices as\n",
      "follows:\n",
      "\n",
      "∂S\n",
      "∂WQ\n",
      "∂S\n",
      "∂WK\n",
      "\n",
      "= XXT WT\n",
      "K,\n",
      "\n",
      "= WT\n",
      "\n",
      "QXXT .\n",
      "\n",
      "9\n",
      "\n",
      "(8)\n",
      "\n",
      "(9)\n",
      "\n",
      "02000400060008000\u00007\u0000U\u0000D\u0000L\u0000Q\u0000L\u0000Q\u0000J\u0000\u0003\u0000V\u0000W\u0000H\u0000S\u0000V01234\u00009\u0000D\u0000U\u0000L\u0000D\u0000Q\u0000F\u0000H\u0000\u0003\u0000R\u0000I\u0000\u0003\u0000/\u0000D\u0000\\\u0000H\u0000U\u00001\u0000R\u0000U\u0000P\u0000\u0003\u0000R\u0000X\u0000W\u0000S\u0000X\u0000WLayer 1Layer 2Layer 3Layer 4Layer 5Layer 602000400060008000Training steps101102103104105Mean of attention scoresMean of attention scoresVariance of LayerNorm output0.11Variance of LayerNorm output\fFigure 6: Ablation experiments on training instability mitigation methods are conducted. We report\n",
      "the average of LAMBADA accuracy of the last three checkpoints of the training and the estimated\n",
      "running time on our 48 A800-GPU cluster. Divergent gradient norm or spiking loss trajectories are\n",
      "shown in red bars, and convergent training is shown in green.\n",
      "\n",
      "From the above derivation, we find that the query-key multiplication term involved can lead to an\n",
      "unbounded gradient, potentially causing severe self-excitation. In such cases, the mean of the attention\n",
      "scores gradually increases as the number of layers increases, eventually resulting in exploding hidden\n",
      "states. Therefore, it is essential to monitor and regularize the attention scores during training.\n",
      "\n",
      "3.2 Training Instability Mitigation Methods\n",
      "\n",
      "After discussing the potential causes of training instability, we will introduce mitigation methods to\n",
      "enhance training stability.\n",
      "\n",
      "3.2.1 Scaled Initialization and Scaling Factor\n",
      "\n",
      "As discussed in Section 3.1.2, employing a carefully designed initialization method along with appro-\n",
      "priate scaling factors is a key strategy for addressing training instability. In this regard, we explore the\n",
      "initialization strategies proposed by Megatron-LM [Shoeybi et al., 2020] and BLOOM [Scao et al.,\n",
      "2022]. Specifically, we initialize W2 of the FFN layer (Equation (1)) and Wo of the Attention layer\n",
      "(Equation (2)) in accordance with N (0, σ2\n",
      "base/(2nlayers)), and initialize the remaining parameters\n",
      "according to N (0, σ2\n",
      "\n",
      "base). Here, we set the initialization standard deviation as σbase = (cid:112)2/(5d).\n",
      "\n",
      "To analyze the effect of the above initialization strategy, we plug the initialization standard deviation\n",
      "into Equation (6) and (7), and obtain the following formulas:\n",
      "1\n",
      "5dmodel · nlayers\n",
      "\n",
      "var(FFN) = rffn · d2\n",
      "\n",
      "2\n",
      "5dmodel\n",
      "\n",
      "model ·\n",
      "\n",
      "=\n",
      "\n",
      ",\n",
      "\n",
      "·\n",
      "\n",
      "var(MHA) = var(head(v)) · dmodel ·\n",
      "\n",
      "1\n",
      "5dmodel · nlayers\n",
      "\n",
      "<\n",
      "\n",
      "1\n",
      "5nlayers\n",
      "2\n",
      "25nlayers\n",
      "\n",
      ".\n",
      "\n",
      "Substituting the obtained results into Equation (4), we can measure the growth of the hidden states’\n",
      "variance throughout the entire network as:\n",
      "\n",
      "var(z) − var(x) = nlayersvar(FFN) + nlayersvar(MHA) < 7/25,\n",
      "\n",
      "(10)\n",
      "\n",
      "which successfully regularizes the growing trend of hidden states.\n",
      "\n",
      "In the modern LLaMA architecture, it is more common to use a GLU-style of non-linearity, which can\n",
      "be denoted as FFN(u) = [F(uWgate) ⊙ (uWup)] · Wdown. We initialize Wup and Wgate the same\n",
      "way as W1, and Wdown same as W2. The above derivation in Equation (10) still holds empirically\n",
      "in this setting.\n",
      "\n",
      "10\n",
      "\n",
      "01020304050LAMBADAALL (D=80B)+ WeSaR+ Depth µP+ Cerebras µP (LR=0.01) QK LayerNorm & + Weight Decay+ QK LayerNormBaseline (LR=0.001, D=20B)33.3629.3728.9327.5723.0622.1021.8633.3629.3728.9327.5723.0622.1021.86Time (h)4.073.873.873.574.793.57\fTable 3: Comparison of the used hyperparameter settings for training stability, where the detailed\n",
      "explanation for the variables are in Table 8. We include SI [Takase et al., 2023] for comparison,\n",
      "MiniCPM [Hu et al., 2024], CerebrasGPT [Dey et al., 2023a]. The definition of the symbols is\n",
      "available at Table 8 .\n",
      "\n",
      "Method\n",
      "\n",
      "SI\n",
      "\n",
      "MiniCPM CerebrasGPT YuLan-Mini\n",
      "\n",
      "Scale Embedding Output\n",
      "\n",
      "Scale MHA equation\n",
      "\n",
      "Scale Residual Connection\n",
      "\n",
      "QKV Weights LR\n",
      "\n",
      "QKV σ Init\n",
      "\n",
      "O Weights LR\n",
      "\n",
      "O σ Init\n",
      "\n",
      "FFN1 Weights LR\n",
      "\n",
      "FFN1 σ Init\n",
      "\n",
      "FFN2 Weights LR\n",
      "\n",
      "FFN2 σ Init\n",
      "\n",
      "Scale Output logits\n",
      "\n",
      "1\n",
      "√\n",
      "\n",
      "1/\n",
      "\n",
      "dhead\n",
      "1\n",
      "\n",
      "ηbase\n",
      "σ2\n",
      "base\n",
      "ηbase\n",
      "σ2\n",
      "base\n",
      "2nlayers\n",
      "ηbase\n",
      "σ2\n",
      "base\n",
      "ηbase\n",
      "σ2\n",
      "base\n",
      "2nlayers\n",
      "1\n",
      "\n",
      "12\n",
      "√\n",
      "\n",
      "√\n",
      "\n",
      "1/\n",
      "\n",
      "dhead\n",
      "1.4\n",
      "nlayers\n",
      "ηbase/mwidth\n",
      "σ2\n",
      "base/mwidth\n",
      "ηbase/mwidth\n",
      "σ2\n",
      "\n",
      "base/mwidth\n",
      "ηbase/mwidth\n",
      "σ2\n",
      "base/mwidth\n",
      "ηbase/mwidth\n",
      "σ2\n",
      "\n",
      "base/mwidth\n",
      "1/mwidth\n",
      "\n",
      "10\n",
      "\n",
      "1/dhead\n",
      "1\n",
      "\n",
      "ηbase/mwidth\n",
      "σ2\n",
      "base/mwidth\n",
      "ηbase/mwidth\n",
      "σ2\n",
      "base\n",
      "2mwidth·nlayers\n",
      "ηbase/mwidth\n",
      "σ2\n",
      "base/mwidth\n",
      "ηbase/mwidth\n",
      "σ2\n",
      "base\n",
      "2mwidth·nlayers\n",
      "1/mwidth\n",
      "\n",
      "10\n",
      "√\n",
      "\n",
      "√\n",
      "\n",
      "1/\n",
      "\n",
      "dhead\n",
      "1.4\n",
      "nlayers\n",
      "ηbase/mwidth\n",
      "σ2\n",
      "base/mwidth\n",
      "ηbase/mwidth\n",
      "σ2\n",
      "base\n",
      "2mwidth·nlayers\n",
      "ηbase/mwidth\n",
      "σ2\n",
      "base/mwidth\n",
      "ηbase/mwidth\n",
      "σ2\n",
      "base\n",
      "2mwidth·nlayers\n",
      "1\n",
      "\n",
      "By initializing in this manner, we empirically find that while there may still be a tendency for the\n",
      "hidden states at each layer to gradually increase, this increase remains within a reasonable range.\n",
      "\n",
      "3.2.2 Maximal Update Parametrization\n",
      "\n",
      "In the above, we have explored the importance and effectiveness of parameter initialization through\n",
      "theoretical analysis and empirical experiments. However, when we migrate the training configuration\n",
      "selected on the 0.05B proxy model to the model of the target size, instability still exists.\n",
      "\n",
      "the Maximal Update\n",
      "To ensure hyper-parameter consistency across different model scales,\n",
      "Parametrization (µP) has been proposed [Yang et al., 2022, 2024c], including width scaling and\n",
      "depth scaling, which facilitate transferring the hyperparameters from smaller models to the training\n",
      "of larger models. Typical use of this strategy can be found in CerebrasGPT [Dey et al., 2023a] and\n",
      "MiniCPM [Hu et al., 2024]. In particular, they have found that the optimal learning rate remains\n",
      "quite stable during migration. Compared to the basic method in Section 3.2.1, µP provides a more\n",
      "systematic approach for setting the initialization and scaling factor.\n",
      "\n",
      "We apply µP, which takes into account embedding parameters, model depth, and model width, for\n",
      "parameter initialization. Additionally, we conduct a comprehensive parameter search on proxy model\n",
      "to identify the optimal configuration, exploring parameters such as batch size and learning rate.\n",
      "\n",
      "3.2.3 Mitigating Instability through Re-Parametrization\n",
      "\n",
      "When using µP, we find that spikes in loss still occur under large learning rates. We speculate this is\n",
      "because, although µP alleviates instability during the initial stages of training, it may still deviate\n",
      "from a stable state during prolonged updates at large learning rates.\n",
      "\n",
      "Therefore, we apply a simple yet effective method WeSaR proposed by Nishida et al. [2024], which\n",
      "empirically decouples the update of the gradient norm from the gradient direction. This is achieved\n",
      "by re-parametrizing the matrix weights W with an additional learnable parameter α ∈ R as:\n",
      "\n",
      "W = α (cid:102)W.\n",
      "\n",
      "We find the above WeSaR method to be effective in addressing the reasons of exploding hidden states\n",
      "highlighted in the previous analysis, as illustrated in Figure 3b. It is likely due to re-parametrization,\n",
      "\n",
      "11\n",
      "\n",
      "\fwhich distributes the gradient of a single weight across multiple new re-parametrized weights, thereby\n",
      "reducing the abnormal updates caused by excessively large gradients.\n",
      "\n",
      "Re-parametrization is applied to matrices other than Layer Normalization as shown in Figure 6.\n",
      "Specifically, we initialize (cid:102)W ∼ N (0, σ2) and set α = 1/γ. In this setting, it is straightforward to\n",
      "verify that W ∼ N (0, (σ/γ)2), which satisfies the scaled initialization requirements described in\n",
      "Section 3.2.1.\n",
      "\n",
      "3.3 Discussion on Other Training Stabilization Methods\n",
      "\n",
      "During our training process, we thoroughly explore and utilize various training stabilization tech-\n",
      "niques. Below, we provide a brief introduction to these methods.\n",
      "\n",
      "3.3.1 Warmup Based Methods\n",
      "\n",
      "To ensure the model transitions smoothly from its initial state to a stable training phase, we empirically\n",
      "find that employing learning rate warmup and sequence length warmup is often effective, which are\n",
      "detailed below.\n",
      "\n",
      "Learning rate warmup Learning rate warmup involves gradually increasing the learning rate from\n",
      "a small initial value (e.g., 0) to the max learning rate in TLR steps. Wortsman et al. [2024] suggests\n",
      "that a longer learning rate warmup can reduce sensitivity to the learning rate, as measured by training\n",
      "stability across different learning rates. We empirically verify this conclusion and find increasing TLR\n",
      "indeed enhances training stability. For our final training, we set TLR = 2,433, which approximately\n",
      "corresponds to 10 billion tokens of data.\n",
      "\n",
      "Sequence length warmup Sequence length warmup starts training with short sequences (e.g., 64\n",
      "tokens) and gradually increases their length within the steps of TSL, which is typically set to a few\n",
      "multiples of TLR [Li et al., 2022]. The rationale behind this approach is that longer sequence lengths\n",
      "contribute significantly to extreme gradient variance, particularly in the early stages of training. In\n",
      "our experiments, we also observe similar fluctuations in loss during long context training (especially\n",
      "in the 27-th curriculum phase). However, since we have stabilized the training using other methods\n",
      "and this approach requires additional preparation of the data, we ultimately decided not to adopt it.\n",
      "\n",
      "3.3.2 Module Based Methods\n",
      "\n",
      "In this part, we introduce module-based methods which regularize the model states by adjusting\n",
      "specific components in it.\n",
      "\n",
      "QK LayerNorm QK LayerNorm and its variants (e.g., QKV LayerNorm or capped QK LayerNorm)\n",
      "have have been shown to effectively mitigate the growth of attention logits [Rybakov et al., 2024],\n",
      "which we also have identified in Section 3.1.2. We highlight the effectiveness of QK LayerNorm\n",
      "because it directly addresses the exponential growth of gradients caused by the interaction of hidden\n",
      "states (QKT ), whereas some other methods only attempt to control the downstream instability.\n",
      "Our empirical study, which is shown in Figure 7a and 7b, demonstrates the advantages of QK\n",
      "LayerNorm in terms of training stability. However, it significantly slows down the calculation in\n",
      "training: with the same acceleration configuration, using QK LayerNorm increases the training time\n",
      "by 34%. Considering that the previously mentioned methods have already demonstrated stability in\n",
      "our preliminary experiments, we ultimately decided not to use QK LayerNorm.\n",
      "\n",
      "Embedding tying Embedding tying aims to share the weights of embedding and unembedding\n",
      "(i.e., lm_head) parameters [Press and Wolf, 2017]. Our experiments demonstrate that the utilization\n",
      "of embedding sharing enables faster convergence and more stable training, and there is no significant\n",
      "degradation in training performance.\n",
      "\n",
      "Z-loss Z-loss was originally proposed to alleviate the shift and scale of logits in classification\n",
      "tasks [de Brébisson and Vincent, 2016]. Subsequently, it has been introduced to LLM and MoE\n",
      "training to mitigate the growth of the logits layer [Chowdhery et al., 2023, Zoph et al., 2022]. It adds an\n",
      "auxiliary term related to the softmax normalizer log Z to the original loss: L = lm_loss + ζ log2 Z.\n",
      "\n",
      "12\n",
      "\n",
      "\f(a) Variance of attention values and LN outputs\n",
      "\n",
      "(b) Gradient norm and loss trajectory\n",
      "\n",
      "Figure 7: The curves of attention value and LN output variances (left) and gradient norm and loss\n",
      "(right). After using QK LayerNorm, we prevent the explosion of attention logits and gradients,\n",
      "keeping the LN output stable around 1 and the loss consistent.\n",
      "\n",
      "In our experiments, we set the coefficient ζ = 10−4 to encourage the logits to be close to 0. Although\n",
      "ablation studies did not show significant effects, we incorporate it into the final training.\n",
      "\n",
      "3.3.3 Numerical Optimization Based Methods\n",
      "\n",
      "In addition, we consider using several commons methods to reduce abnormal updates during opti-\n",
      "mization, as described below.\n",
      "\n",
      "Weight decay To prevent abnormal model weights due to large gradient updates, weight decay\n",
      "functions by subtracting a penalty term from the weights during the update step, rather than directly\n",
      "modifying the gradients. Formally, we denote the AdamW update without learning rate or weight\n",
      "decay as:\n",
      "\n",
      "ˆvt + ϵ).\n",
      "\n",
      "∆ = α ˆmt/(\n",
      "\n",
      "(11)\n",
      "Then at update step t, the AdamW update with weight decay is given by θ → θ − stη(∆ − λθ),\n",
      "where λ is the weight decay coefficient, st is learning rate schedule and η is the max learning\n",
      "rate. Previous work has recommended using an independent weight decay for updates, expressed as\n",
      "θ → θ−st(η∆−λ′θ), which is claimed to be applicable to a wider range of learning rates [Loshchilov\n",
      "and Hutter, 2019, Wortsman et al., 2024]. In the PyTorch implementation, this approach can be\n",
      "achieved by tuning the weight decay coefficient λ in conjunction with the maximum learning rate,\n",
      "following the relationship λ′ = η · λ.\n",
      "\n",
      "(cid:112)\n",
      "\n",
      "In the update of AdamW (Equation (11)), ˆmt and ˆvt represent the\n",
      "Optimizer hyper-parameter\n",
      "first and second gradient moment exponential moving averages (EMA), respectively. If the gradient\n",
      "is of the same order of magnitude as ϵ, then the update value ∆ will be significantly reduced due to ϵ,\n",
      "which empirically leads to training instability inherent in embedding layer. A direct solution is to\n",
      "reduce ϵ from the default value of 10−8 to 10−15. Generally speaking, this method can alleviate the\n",
      "divergence caused by abnormal embedding gradient values in larger-scale models [Wortsman et al.,\n",
      "2024, Molybog et al., 2023].\n",
      "\n",
      "Numerical stability In practice, paying close attention to numerical stability is crucial, as it can\n",
      "be an important source of training instability. In large-scale model training, float32 often suffers\n",
      "from low computational efficiency. Although float16 offers comparable precision with higher\n",
      "computational efficiency, it has a limited numerical representation range (e.g., maximum positive\n",
      "number that can be represented is 65,504). Therefore, bfloat16 has been proposed as a trade-off\n",
      "between precision and representation range. It largely alleviates the training instability caused by\n",
      "exceeding the representable range. However, in practice, bfloat16 introduces precision problems\n",
      "compared to float16. In experiments conducted by Lee et al. [2024] using bfloat16 with 188\n",
      "\n",
      "13\n",
      "\n",
      "0200040006000800010000Training steps110Avg LN variance w/o QK LayerNormAvg attention logits w/o QK LayerNormAvg LN variance w/o QK LayerNormAvg attention logits w/ QK LayerNorm0200040006000800010000Training steps101100101Grad NormLoss w/o QK LayerNormLoss w/ QK LayerNormGrad Norm w/o QK LayerNormGrad Norm w/ QK LayerNorm110Loss\fTable 4: Statistical information of the entire pre-training corpus for YuLan-Mini. The data during the\n",
      "annealing process is detailed in Table 5. For model reproducibility, all curated datasets are placed in\n",
      "Appendix D, and the remaining synthetic data we generated is open-sourced.\n",
      "\n",
      "Type\n",
      "\n",
      "Source\n",
      "\n",
      "Web Pages\n",
      "Math (Pretrain)\n",
      "Code (Pretrain)\n",
      "General Knowledge\n",
      "Books\n",
      "Encyclopedia\n",
      "Open-Source Instruction\n",
      "Synthetic Pretrain Data (Ours)\n",
      "Synthetic Instruction (Ours)\n",
      "\n",
      "FineWeb-Edu, DCLM, Chinese-FineWeb-Edu\n",
      "AutoMathText, Proof-Pile-2, OpenWebMath Pro\n",
      "the-stack-v2, StarCoder\n",
      "arXiv, StackExchange, English News\n",
      "CBook, Gutenberg, LoC-PD-Books\n",
      "Wikipedia, Baidu-Baike\n",
      "SlimOrca, OpenMathInstruct-1, JiuZhang3.0\n",
      "Synthetic document (seed: AutoMathText, LeetCode)\n",
      "Reasoning (seed: MetaMathQA, DeepMind Math, ...)\n",
      "\n",
      "Total\n",
      "\n",
      "-\n",
      "\n",
      "Volume\n",
      "\n",
      "559.76B\n",
      "85.00B\n",
      "202.44B\n",
      "121.87B\n",
      "52.13B\n",
      "14.80B\n",
      "11.64B\n",
      "8.76B\n",
      "23.52B\n",
      "\n",
      "1,080B\n",
      "\n",
      "random seeds, 18 runs diverged, whereas using float32 under the same configuration resulted in\n",
      "all runs converging normally. To mitigate precision issues with bfloat16, Gemma [Mesnard et al.,\n",
      "2024] find that shifting the RMSNorm weight from 1 to 0 helps, considering that bfloat16 has\n",
      "symmetric numerical precision around 0 but greater inaccuracies near 1.\n",
      "\n",
      "Value clipping To further limit the gradient within certain range, we utilize a gradient clipping of 1.\n",
      "We find using a smaller limit does not help stabilize the training. In addition, initializing the LLM in\n",
      "accordance with “3-σ” rule with nn.init.trunc_normal_ may be helpful for numerical stability.\n",
      "\n",
      "4 Data Pipeline\n",
      "\n",
      "To pre-train an effective LLM, it is crucial to develop a robust data pipeline that comprehensively\n",
      "encompasses the key steps for curating the pre-training data. These steps include data collection,\n",
      "filtering, selection, mixing, and curriculum design. Below, we describe each step of the data pipeline\n",
      "in detail.\n",
      "\n",
      "4.1 Data Collection\n",
      "\n",
      "For pre-training data preparation, we primarily reference the data configuration of Yulan-3 [Zhu\n",
      "et al., 2024] and Llama-3-SynE [Chen et al., 2024], which encompasses a wide-ranging and diverse\n",
      "collection of data such as web pages, encyclopedias, books, mathematical corpora, code, general\n",
      "knowledge, and synthetic data. Table 4 presents an overall summary regarding the composition of\n",
      "our training data.\n",
      "\n",
      "4.2 Data Filtering\n",
      "\n",
      "As we aim for a data-efficient training approach, data quality is crucial to the final model’s perfor-\n",
      "mance. For this purpose, we implement a thorough data cleaning process to remove low-quality texts\n",
      "(Figure 8).\n",
      "\n",
      "De-duplication Data de-duplication is a crucial step in standard LLM training practices, as previous\n",
      "research has demonstrated that duplicate data can significantly degrade model performance [Tirumala\n",
      "et al., 2023]. We use the MinHash algorithm implemented by the Yulan-GARDEN library [Sun et al.,\n",
      "2024] to deduplicate the training data.\n",
      "\n",
      "Heuristic filtering We adopt heuristic methods to filter the data, some of which are listed as\n",
      "follows:\n",
      "\n",
      "• All: we remove the documents containing fewer than 20 tokens.\n",
      "\n",
      "14\n",
      "\n",
      "\fFigure 8: Illustration of our data filtering pipeline and synthetic generation for reasoning data. The\n",
      "filtering pipeline consists of six steps starting from data collection. Synthetic data generation includes\n",
      "both pretraining data (above the horizontal line) and instruction data (below the line).\n",
      "\n",
      "• Code: we apply filtering criteria based on code metrics (e.g., average line length, alphabetic\n",
      "characters ratio, and keyword statistics) similar to DeepSeek-Coder [Guo et al., 2024].\n",
      "\n",
      "• Synthetic data: we remove responses that are garbled or contain repeated content. For math\n",
      "texts, we remove response that do not contain an hightlited answer part (e.g., $box{}$).\n",
      "\n",
      "Topic-based text recall To enhance the model’s capabilities in specialized areas, it is essential to\n",
      "include ample knowledge documents related to mathematics, code, and reasoning. For this purpose,\n",
      "we extract relevant documents from unused web pages by training fasttext [Bojanowski et al.,\n",
      "2017] and TinyBert [Jiao et al., 2020] classifiers specifically tailored to these categories. From the\n",
      "FineWeb-Edu [Lozhkov et al., 2024a] and DCLM [Li et al., 2024b] web corpus, we extract 10.4B\n",
      "math text tokens, 1.11B code text tokens, and 1.01B reasoning text tokens. which are directly used for\n",
      "training or serve as seed data for synthesizing instruction data. Furthermore, we reuse the synthesized\n",
      "science data (1.5B) from Llama-3-SynE [Chen et al., 2024], which covers an extensive range of\n",
      "disciplines, such as math and physics.\n",
      "\n",
      "Model-based quality scoring For general web page data and mathematical pre-training data, we\n",
      "use the fineweb-edu-scorer released by FineWeb-Edu for data scoring. For Python code data, we\n",
      "use the python-edu-scorer released by FineWeb-Edu. To avoid language models favoring highly\n",
      "technical pages like arXiv abstracts and submitted papers, these two classifiers focus on knowledge at\n",
      "the elementary and middle school levels. Following the methodology of Penedo et al. [2024], we\n",
      "conduct quality assessments on all Python code data, most mathematical data, and web page data\n",
      "using scoring tools. We exclude data with scores of 1 and 2 and then heuristically sort data with\n",
      "scores from 3 to 5 (as detailed in Section 4.5).\n",
      "\n",
      "Decontamination To ensure the fairness of comparison, we perform decontamination based on the\n",
      "selected evaluation benchmarks. Initially, we tokenize both the training set and the benchmarks that\n",
      "require decontamination, such as GSM8K [Cobbe et al., 2021], MATH [Hendrycks et al., 2021b],\n",
      "HumanEval [Chen et al., 2021], and ARC [Yadav et al., 2019]. Next, we divide all the benchmarks\n",
      "using n-gram tokens to create a contamination set. We use tokens rather than words to form n-gram\n",
      "segment, which achieves a higher level of decontamination in the domains of mathematics and code.\n",
      "Additionally, we exclude 20-gram segments that occur more than four times, as they are typically\n",
      "not relevant to the questions or solutions. Ultimately, the contamination set comprises 1,917,428\n",
      "tuples. For each training document, if more than 10% of its generated 20-grams are present in the\n",
      "contamination set, we exclude that document from the final pre-training set.\n",
      "\n",
      "15\n",
      "\n",
      "Data CollectionDe-duplicationHeuristicFilteringTopic-basedText RecallDecontaminationModel-basedQuality Scoing        Math Doc         Competition         Code Doc         Math CoT          Program          Generated          OSS-Instruct          Science CoTMathematicsCodingScience        Formal         MathematicsData Filtering PipelineSynthetic Generation of Reasoning Data\f4.3 Synthetic Generation of Reasoning Data\n",
      "\n",
      "Reasoning is widely recognized as one of the most desirable capabilities for LLMs [Huang and Chang,\n",
      "2023]. However, unlike basic skills such as knowledge memorization, reasoning is more challenging\n",
      "to achieve and enhance in LLMs. One potential reason for this difficulty is the scarcity of high-quality\n",
      "texts containing logical or complex reasoning in real-world datasets. To address this limitation, a\n",
      "common approach is to generate data samples related to reasoning, such as chain-of-thought data,\n",
      "which focus on demonstrating the thought processes likely to lead to the final solution rather than\n",
      "directly producing question-answer pairs [Wei et al., 2022]. To enrich diversity and coverage, we\n",
      "consider a broad range of domains for synthetic data generation, as introduced below.\n",
      "\n",
      "4.3.1 Math Reasoning\n",
      "\n",
      "Mathematical reasoning data is relatively scarce in pre-training corpora, yet it is crucial for enhancing\n",
      "model capabilities [Wei et al., 2022]. To address this gap, we generate a diverse range of mathematical\n",
      "reasoning data, including documents, instructions, and formal mathematics data.\n",
      "\n",
      "Mathematical documents For mathematical documents, we generate descriptive content spanning\n",
      "various difficulty levels and thematic styles. It includes explanations of mathematical concepts and\n",
      "science education materials for primary school levels, as well as lecture scripts, tutorials, educational\n",
      "articles, and problem sets suitable for high school and college-level content. We primarily source\n",
      "math-related seed data from mathematical pre-training corpora, such as OpenWebMath [Paster et al.,\n",
      "2024], and from a self-compiled math dataset using classifiers described in Section 4.2.\n",
      "\n",
      "Chain-of-thought reasoning For instructional data, we employ three approaches for text synthesis.\n",
      "First, we use Qwen2.5-Math-7B-Instruct [Yang et al., 2024b] to generate thought processes (e.g.,\n",
      "chain-of-thought) for existing problems found in open-source datasets, such as Orca-Math [Mitra et al.,\n",
      "2024b], MetaMathQA [Yu et al., 2024], AMPS-Math [Hendrycks et al., 2021b], and NuminaMath [LI\n",
      "et al., 2024]. Second, following the method of JiuZhang3.0 [Zhou et al., 2024b], we utilize a\n",
      "finetuned Qwen2-Math-7B-Instruct [Ding et al., 2024b] to automatically generate new math\n",
      "problems (without the thought process) and then annotate the solutions in the same manner as the\n",
      "first approach. To obtain more extensive thought processes, we select more challenging data, such as\n",
      "from the NuminaMath dataset, and utilize slow-thinking model QwQ-32B-Preview [Qwen-Team,\n",
      "2024] for distillation. We obtain the long-form thought data from our o1-reproduction project “Slow\n",
      "thinking with LLMs” [Jiang et al., 2024, Min et al., 2024]. This data aims to enhance the mathematical\n",
      "capacities of our base model.\n",
      "\n",
      "Formal mathematical reasoning We also incorporate reasoning data from formal mathematics,\n",
      "such as formal theorem proving, which has been shown to enhance performance on general mathemat-\n",
      "ical tests like GSM8K and MATH. Specifically, we collect Lean tactic (e.g., intro, simp) dataset like\n",
      "DeepSeek-Prover [Xin et al., 2024] and its associated prover states to train the model in generating\n",
      "proof tactics. For the Lean GitHub dataset [Wu et al., 2024], we concatenate together the reasoning\n",
      "steps in each document to form a long reasoning chain. Additionally, inspired by LIME [Wu et al.,\n",
      "2021], we augment the Lean Workbook [Ying et al., 2024] datasets with three reasoning primitives:\n",
      "(1) Deduction: Statebefore, Tactic → Stateafter; (2) Abduction: Stateafter, Tactic → Statebefore; and (3)\n",
      "Induction: Statebefore, Stateafter → Tactic. Rather than directly predicting the next proof tactics, we\n",
      "train the model to predict the previous or next state based on the proof tactics.\n",
      "\n",
      "Program generated numerical reasoning LLMs perform reasoning in natural language, which is\n",
      "flexible but does not allow for the verification of the correctness and necessity of each step. To enhance\n",
      "the model’s basic numerical abilities, we select subsets of addition, subtraction, multiplication,\n",
      "division, and remainder operations from the DeepMind-Math dataset [Saxton et al., 2019]. Our\n",
      "goal is to transform simple mathematical expressions (e.g., “What is 0.079 - 162?”) into a\n",
      "corresponding calculation procedure consisting of individual steps. This enables the model to learn\n",
      "calculations in a manner similar to chain-of-thought reasoning. This approach is mainly applicable to\n",
      "simple and limited mathematical computations and is suitable only for the early and middle stages of\n",
      "training. Given that manually writing conversion code is time-consuming, we also utilize an agentic\n",
      "framework for the automatic generation of code.\n",
      "\n",
      "16\n",
      "\n",
      "\f4.3.2 Code Reasoning\n",
      "\n",
      "For code reasoning, we primarily synthesize two types of data: programming competition problems\n",
      "and real-world programming tasks. We also generate long-form reasoning thought data with slow-\n",
      "thinking models.\n",
      "\n",
      "Competition code synthesis through ICL To enhance existing programming competition datasets,\n",
      "like those from LeetCode8, we utilize the in-context learning (ICL) method by leveraging a small\n",
      "number of demonstrations. This approach allows us to generate additional examples, thereby\n",
      "expanding and diversifying the data. By introducing a broader range of challenging programming\n",
      "problems, we enrich the dataset significantly.\n",
      "\n",
      "OSS-Instruct Additionally, we generate real-world programming tasks and their corresponding\n",
      "solutions using the OSS-Instruct method, as detailed in previous work [Wei et al., 2024]. This process\n",
      "is guided by carefully crafted prompts (see Appendix C), ensuring that the generated tasks are both\n",
      "relevant and applicable to real-world scenarios. This approach significantly enhances the practical\n",
      "utility of the dataset.\n",
      "\n",
      "4.3.3 Scientific Reasoning\n",
      "\n",
      "Scientific reasoning is crucial for expanding the capabilities of LLMs. To acquire scientific reasoning\n",
      "data, we consider the following two approaches.\n",
      "\n",
      "Scientific chain-of-thought reasoning In Section 4.2, we train a scientific classifier to extract\n",
      "documents related to various scientific fields from the web page data of FineWeb-Edu and DCLM.\n",
      "We utilize these scientific data as the seed data, and apply a synthesis method similar to that used for\n",
      "generating math reasoning data to create science-related questions and answers [Chen et al., 2024].\n",
      "\n",
      "Scientific problems with slow-thinking processes We gather more difficult scientific questions\n",
      "from college entrance examinations and camel-ai, covering subjects such as physics, chemistry, and\n",
      "biology. Then we use QwQ-32B-Preview to answer questions and obtain pairs of answers to difficult\n",
      "scientific data questions.\n",
      "\n",
      "4.3.4 Reflection\n",
      "\n",
      "An important aspect of reasoning ability is the capacity to reflect on and backtrack from the current\n",
      "state [Shinn et al., 2023]. In this work, we explore the generation of reflection data to further enhance\n",
      "the model’s reasoning capabilities. First, we sample mathematical problems and collect both positive\n",
      "and negative responses by comparing them to the golden label. We then employ a powerful model,\n",
      "Qwen2.5-Math-7B-Instruct, to identify the first error in the negative response and truncate the\n",
      "content that follows. Instead of merely concatenating the positive response with the truncated negative\n",
      "one, we create error analyses and transitional statements to seamlessly and effectively connect the\n",
      "two responses.\n",
      "\n",
      "The prompts used for data synthesis are provided in Appendix C.\n",
      "\n",
      "4.4 Data Mix\n",
      "\n",
      "Our training process is divided into three main stages: warmup, stable training, and annealing.\n",
      "\n",
      "During the warmup and stable training stages, the dataset composition is as follows: 60% general\n",
      "English data (consisting of 45% from web and 15% from books, papers and other relevant sources),\n",
      "20% code data, 10% math data, and 10% general Chinese data. In the later stage of stable training,\n",
      "a small amount (<5%) of instruction data is introduced. We maintain a relatively consistent data\n",
      "distribution across different curriculum phases, and will slightly adjust the data proportions according\n",
      "to the the perplexity performance of the model on various benchmarks. We strive to avoid large shifts\n",
      "in data distribution, as significant changes can cause the loss to spike suddenly. To ensure training\n",
      "stability and account for testing inaccuracies, the change in data distribution between two consecutive\n",
      "phases is kept within 3%.\n",
      "\n",
      "8https://huggingface.co/datasets/greengerong/leetcode\n",
      "\n",
      "17\n",
      "\n",
      "\fFigure 9: The data mixture proportion of math, code, and general data. We keep the proportion of\n",
      "web data unchanged in stable stage, and then gradually decrease it in annealing stage. The entire\n",
      "process is divided into into three major stages: warmup, stable training, and annealing (i.e., beginning\n",
      "after the dashed line).\n",
      "\n",
      "During the annealing stage, the proportion of instruction data is increased to 19.19% in total: code-\n",
      "related instruction data constitutes approximately 11%, math-related instruction data accounts for\n",
      "about 7%, and general instruction data amounts to around 1%. Additionally, the proportion of long\n",
      "context data (i.e., those exceeding 8K tokens) is also increased, occupying approximately 14.21% of\n",
      "the tokens used.\n",
      "\n",
      "4.5 Data Curriculum\n",
      "\n",
      "Following previous studies [Kim and Lee, 2024, Chen et al., 2024], we adopt a curriculum-\n",
      "based method to prepare training data throughout the process. We use quality classifiers, such\n",
      "as fineweb-edu-scorer, to evaluate content based on educational difficulty, assigning higher\n",
      "scores to text content suitable for primary and secondary school levels. Manual inspection reveals\n",
      "that, for math and code pre-training data, lower scores often indicate higher difficulty. Conversely,\n",
      "for English webpage data, we find that staged training based on educational level scores significantly\n",
      "impacts the original distribution. Therefore, we train on math and code data in order of increasing\n",
      "difficulty, but do not apply curriculum learning to webpage data.\n",
      "\n",
      "Throughout the entire training process, we continuously monitor the model’s performance, as detailed\n",
      "in Section 6.3. We save a checkpoint and conduct an evaluation each 4B training tokens. For each\n",
      "40B tokens, we reassess and adjust the data ratio when transitioning between training phases based\n",
      "on the model’s overall performance in that phase. For example, if the model’s performance on\n",
      "the HumanEval benchmark does not improve or declines after a stage, we may consider slightly\n",
      "increasing the amount of code data in the subsequent stage.\n",
      "\n",
      "Additionally, to help the model adapt to a relatively high proportion of instruction data in the annealing\n",
      "stage, we gradually increase the amount of instruction data. However, throughout the entire stable\n",
      "training stage, this proportion will not exceed 5%.\n",
      "\n",
      "We present the data distribution scheduled in the training curriculum in Figure 9, and the detailed\n",
      "data composition for each phase are presented in Appendix E.\n",
      "\n",
      "18\n",
      "\n",
      "13020406080100%111315171921232527General-PretrainGeneral-SFTMath-PretrainMath-SFTCode-PretrainCode-SFTWebChineseCurriculum Phase\f5 Annealing\n",
      "\n",
      "As demonstrated in previous studies [Hu et al., 2024], the annealing stage is particularly effective in\n",
      "boosting model performance. We also implement effective training strategies during the annealing\n",
      "stage, which are described below.\n",
      "\n",
      "5.1 Optimization Setting\n",
      "\n",
      "In the annealing stage, we improve the performance of the model by using a high-quality data set and\n",
      "equip the model with long context processing capacities. Thus, we mainly consider the two settings,\n",
      "i.e., learning rate annealing and context window extension.\n",
      "\n",
      "Learning rate annealing Since we use the WSD scheduler, in the annealing stage, the learning\n",
      "rate will gradually decrease from that in the stable training stage. We need to select an appropriate\n",
      "learning rate annealing function. We investigate the performance of several typical learning rate\n",
      "annealing functions [Hägele et al., 2024], such as linear annealing, cosine annealing, 1-sqrt annealing.\n",
      "We empirically find that 1-sqrt performs the best, so we choose 1-sqrt as our learning rate annealing\n",
      "function, which is defined as follows:\n",
      "\n",
      "f (n; N, Nannealing) =\n",
      "\n",
      "1 −\n",
      "\n",
      "(cid:32)\n",
      "\n",
      "(cid:115)\n",
      "\n",
      "n − (N − Nannealing)\n",
      "Nannealing\n",
      "\n",
      "(cid:33)\n",
      "\n",
      ",\n",
      "\n",
      "where n, N and Nannealing denote the current number of steps, the total number of steps and the\n",
      "number of annealing steps, respectively.\n",
      "\n",
      "Following the work by Hu et al. [2024], we estimate the optimal annealing ratio to be 8%, i.e., 80\n",
      "billion tokens. We maintain the same batch size used during stable training, i.e., 4 million tokens.\n",
      "The learning rate is decreased from 10−2 to 5.22 × 10−5 over a span of 18,802 steps. Subsequently,\n",
      "the learning rate is held constant at 5.22 × 10−5 for the final 772 steps.\n",
      "\n",
      "Context Window Extension Previous research [Chen et al., 2023] has demonstrated that LLMs can\n",
      "hardly process texts exceeding their context windows due to the out-of-distribution (OOD) rotation\n",
      "angles in RoPE. To achieve the context window extension, increasing the base frequency of RoPE\n",
      "to migrate the OOD rotation angles and continual pre-training has been an effective method [Xiong\n",
      "et al., 2024]. Consequently, during the annealing stage, we increase the base frequency of RoPE θ\n",
      "from 10,000, employed during stable training, to 490,000 and train the model on long texts. This\n",
      "adjustment successfully extends the context length from 4,096 (4K) tokens to 28,672 (28K) tokens.\n",
      "\n",
      "5.2 Data Selection for Annealing Stage\n",
      "\n",
      "It is particularly important to select high-quality data during the annealing stage [Hu et al., 2024]. As\n",
      "the learning rate decreases, the model can rapidly enhance its performance by leveraging high-quality\n",
      "data. We consider selecting high-quality data for the annealing stage using the following methods:\n",
      "\n",
      "• We include a variety of high-quality data sources, particularly synthetic reasoning data\n",
      "\n",
      "discussed in Section 4.3.\n",
      "\n",
      "• We use a gradient-based data selection method, which accelerates and improves the LESS\n",
      "method [Xia et al., 2024], combining the method InsTag [Lu et al., 2024] for constructing a\n",
      "diversified target set.\n",
      "\n",
      "In particular, we incorporate formal mathematical reasoning (theorem proving in Lean) and advanced\n",
      "reasoning data (o1-like thought data) to improve the model’s performance on challenging math\n",
      "benchmarks, e.g., MATH-500, which have been shown in Table 6.\n",
      "\n",
      "We present the final data composition for the annealing stage in Table 5.\n",
      "\n",
      "5.3 Long Context Training\n",
      "\n",
      "During the annealing stage of the final 80B tokens, we adjust the base frequency of RoPE from\n",
      "10,000 to 490,000 and train on long sequences to extend the context length from 4,096 tokens to\n",
      "\n",
      "19\n",
      "\n",
      "\fTable 5: Detailed information of the training data in the annealing stage.\n",
      "\n",
      "Domain Type\n",
      "\n",
      "Dataset\n",
      "\n",
      "Mix\n",
      "Math\n",
      "\n",
      "Code\n",
      "\n",
      "Science\n",
      "\n",
      "Pretrain\n",
      "(1) CoT\n",
      "(2) Long CoT\n",
      "(3) Formal math\n",
      "(4) Curated\n",
      "(1) CoT\n",
      "(2) Curated\n",
      "(1) Long CoT\n",
      "(2) Curated\n",
      "\n",
      "FineWeb-Edu, CBook, arXiv\n",
      "Deepmind-Math, MathInstruct\n",
      "Numina, AMPS, Platypus\n",
      "Lean-GitHub, Lean-WorkBook, DeepSeek-Prover-V1\n",
      "Tulu v3, MathInstruct\n",
      "OSS-Instruct (seed: the-Stack-v2), OpenCoder-LLM\n",
      "LeetCode, XCoder-80K\n",
      "Camel-ai\n",
      "EvolKit-20k, Celestia, Supernova\n",
      "\n",
      "Total\n",
      "\n",
      "-\n",
      "\n",
      "-\n",
      "\n",
      "Volume\n",
      "\n",
      "64.65B\n",
      "3.07B\n",
      "0.61B\n",
      "0.10B\n",
      "1.42B\n",
      "6.66B\n",
      "2.39B\n",
      "0.04B\n",
      "1.06B\n",
      "\n",
      "80B\n",
      "\n",
      "28,672 tokens. We avoid training with long contexts in earlier stages because the computational\n",
      "cost of self-attention layers increases quadratically with sequence length, making it prohibitively\n",
      "expensive [Dubey et al., 2024].\n",
      "\n",
      "When training on long contexts, we observe a decline in the model’s performance on short-text\n",
      "benchmarks. To enhance the long-text capacities and preserve the short-text capacities, we carefully\n",
      "design the mixture of data. We upample books and concatenated GitHub code texts [Liu et al.,\n",
      "2024b] as long context data to capture long-term dependencies, while using high-quality short texts\n",
      "to preserve short-text capabilities. Additionally, inspired by previous studies Ding et al. [2024a], Gao\n",
      "et al. [2024], we also apply masked cross-document attention that prevents attention across different\n",
      "documents to preserve short-context capabilities.\n",
      "\n",
      "5.4 Other Strategies\n",
      "\n",
      "Packing Since the training data during the annealing stage includes some instruction data, using a\n",
      "traditional simple packing method for pre-training data could result in instruction data being split,\n",
      "thereby compromising its effectiveness. To address this, we propose a packing strategy designed\n",
      "to maintain training efficiency while minimizing the disruption of instruction data. This strategy\n",
      "involves different packing methods based on data type. Pre-training data is directly spliced, whereas\n",
      "for instruction data, if it is divided into two sequences, the remaining part of the previous sequence is\n",
      "padded directly, and this instruction data serves as the beginning of the second sequence. Subsequently,\n",
      "any redundant padding tokens are replaced with pre-training data tokens. By including the instruction\n",
      "data, our main goal is to learn the reasoning process rather than focusing solely on the question-and-\n",
      "answer format. Therefore, we employ the same data processing method used in pre-training, which\n",
      "directly includes question-answer pairs without relying on a chat template. When calculating the loss,\n",
      "the instruction and response are treated as a single document, and the loss for the instruction is not\n",
      "masked.\n",
      "\n",
      "Checkpoint merging Following the approach used in LLaMA3 [Dubey et al., 2024], we combine\n",
      "the last few checkpoints during the annealing stage to produce the final pre-trained model. While this\n",
      "strategy might result in a slight reduction in certain specific capabilities (e.g., GSM8K), it generally\n",
      "leads to a more well-rounded model.\n",
      "\n",
      "6 Evaluation\n",
      "\n",
      "In this section, we conduct the evaluation experiments to verify the effectiveness of our base model\n",
      "YuLan-Mini. We first set up the experiments for evaluation, and then present the experiment results.\n",
      "\n",
      "20\n",
      "\n",
      "\f6.1 Experimental Setup\n",
      "\n",
      "6.1.1 Evaluation Benchmarks\n",
      "\n",
      "For a comprehensive evaluation of LLMs performance, we select the benchmarks from the following\n",
      "aspects.\n",
      "\n",
      "• Language\n",
      "\n",
      "comprehension: We\n",
      "\n",
      "the widely-used English benchmarks\n",
      "MMLU [Hendrycks et al., 2021a], LAMBADA [Kazemi et al., 2023] and RACE [Lai et al.,\n",
      "2017], along with the Chinese benchmarks CMMLU [Li et al., 2024a] and CEval [Huang\n",
      "et al., 2023], to evaluate the bilingual comprehension capabilities of the LLM. These\n",
      "benchmarks span various domains, such as history, science, and culture.\n",
      "\n",
      "select\n",
      "\n",
      "• Code generation: We select Humaneval [Chen et al., 2021] and MBPP [Austin et al., 2021]\n",
      "to assess the capability of LLMs to generate accurate code snippets for natural language\n",
      "problems.\n",
      "\n",
      "• Mathematical reasoning: We utilize GSM8K [Cobbe et al., 2021] and MATH-\n",
      "500 [Hendrycks et al., 2021b, Lightman et al., 2024] to evaluate the mathematical rea-\n",
      "soning capabilities of LLMs. These benchmarks range from basic arithmetic to advanced\n",
      "mathematical problems.\n",
      "\n",
      "• Logical reasoning: We assess the logical reasoning capabilities of LLMs using ARC-\n",
      "E [Yadav et al., 2019], ARC-C [Yadav et al., 2019], which provide a comprehensive\n",
      "evaluation of logical reasoning across various knowledge domains.\n",
      "\n",
      "• Commonsense reasoning: We evaluate the LLM’s commonsense reasoning ability\n",
      "using WinoGrande [Sakaguchi et al., 2021], HellaSwag [Zellers et al., 2019], Sto-\n",
      "ryCloze [Mostafazadeh et al., 2016] which test the understanding and utilization of daily\n",
      "commonsense knowledge.\n",
      "\n",
      "• Long context understanding: We employ RULER [Hsieh et al., 2024] to evaluate the long\n",
      "context understanding ability, which measures its performance change as the sequence\n",
      "length increases. We perform evaluations of applicable models within a context length of\n",
      "28K tokens.\n",
      "\n",
      "6.1.2 Baseline Models\n",
      "\n",
      "To ensure a comprehensive evaluation, we select several small LLMs with comparable scales (i.e.,\n",
      "base models ranging from 0.5 to 3B, including embedding sizes) as baselines for comparison:\n",
      "\n",
      "• MiniCPM-2.4B [Hu et al., 2024]: MiniCPM-2.4B is pre-trained on 1.06T tokens and also\n",
      "employs the annealing training strategy. Despite its small size, it exhibits impressive\n",
      "performance in general tasks while supporting deployments with limited hardware resource.\n",
      "\n",
      "• Qwen series models [Qwen-Team, 2024, Yang et al., 2024a]: We select Qwen2-1.5B,\n",
      "Qwen2.5-0.5B, and Qwen2.5-1.5B for comparison. The Qwen series of small LLMs have\n",
      "been pre-trained on 18T tokens, and the training details have not been fully publicly released.\n",
      "They demonstrate strong performance in both general and domain-specific tasks.\n",
      "\n",
      "• StableLM2-1.6B [Bellagente et al., 2024]: StableLM2-1.6B is a small LLM proposed by\n",
      "StabilityAI. It has been pre-trained on a mixture of open-source datasets, which utilizes\n",
      "several small LLMs to determine the training data proportion.\n",
      "\n",
      "• SmolLM2-1.7B [Allal et al., 2024]: SmolLM2-1.7B is developed by HuggingFace TB\n",
      "Research based on its collected high-quality pre-training corpus, which has been trained on\n",
      "11T tokens, and maintains a good balance between speed and accuracy.\n",
      "\n",
      "• Llama3.2-3B [Dubey et al., 2024]: Llama3.2-3B is developed by MetaAI, which is trained\n",
      "on up to 9T tokens. It further distills the knowledge from LLaMA3.1-8B and 70B models\n",
      "by using their logits during the pretraining stage.\n",
      "\n",
      "• Gemma2-2.6B [Team, 2024]: Gemma2-2.6B is developed by Google, which is trained on\n",
      "\n",
      "2T tokens, mainly including web documents, code, and mathematical text.\n",
      "\n",
      "21\n",
      "\n",
      "\fTable 6: Performance on math, code, and long context benchmarks. Results marked with * are\n",
      "cited from their official paper or report. The best and second best results are bold and underlined,\n",
      "respectively.\n",
      "\n",
      "Model\n",
      "Size\n",
      "\n",
      "# Train\n",
      "Tokens\n",
      "\n",
      "Context\n",
      "Length\n",
      "\n",
      "MATH\n",
      "500\n",
      "\n",
      "GSM\n",
      "8K\n",
      "\n",
      "Models\n",
      "\n",
      "MiniCPM\n",
      "Qwen-2\n",
      "Qwen2.5\n",
      "Qwen2.5\n",
      "Gemma2\n",
      "StableLM2\n",
      "SmolLM2\n",
      "Llama3.2\n",
      "\n",
      "YuLan-Mini\n",
      "\n",
      "2.6B\n",
      "1.5B\n",
      "0.5B\n",
      "1.5B\n",
      "2.6B\n",
      "1.7B\n",
      "1.7B\n",
      "3.2B\n",
      "\n",
      "2.4B\n",
      "2.4B\n",
      "\n",
      "1.06T\n",
      "7T\n",
      "18T\n",
      "18T\n",
      "2T\n",
      "2T\n",
      "11T\n",
      "9T\n",
      "\n",
      "1.04T\n",
      "1.08T\n",
      "\n",
      "4K 15.00\n",
      "128K 22.60\n",
      "128K 23.60\n",
      "128K 45.40\n",
      "\n",
      "53.83\n",
      "46.90∗\n",
      "41.60∗\n",
      "68.50∗\n",
      "8K 18.30∗ 30.30∗\n",
      "4K\n",
      "20.62\n",
      "-\n",
      "8K 11.80\n",
      "-\n",
      "128K 7.40\n",
      "\n",
      "-\n",
      "\n",
      "4K 32.60\n",
      "28K 37.80\n",
      "\n",
      "66.65\n",
      "68.46\n",
      "\n",
      "61.60\n",
      "64.00\n",
      "\n",
      "Human\n",
      "Eval\n",
      "50.00∗\n",
      "34.80∗\n",
      "30.50∗\n",
      "37.20∗\n",
      "19.50∗\n",
      "8.50\n",
      "23.35\n",
      "29.30\n",
      "\n",
      "MBPP\n",
      "\n",
      "RACE\n",
      "Middle\n",
      "\n",
      "RACE\n",
      "High\n",
      "\n",
      "RULER\n",
      "\n",
      "47.31\n",
      "46.90∗\n",
      "39.30∗\n",
      "60.20∗\n",
      "42.10∗\n",
      "17.50\n",
      "45.00\n",
      "49.70\n",
      "\n",
      "66.70\n",
      "65.90\n",
      "\n",
      "56.61\n",
      "55.77\n",
      "52.36\n",
      "58.77\n",
      "-\n",
      "56.33\n",
      "55.77\n",
      "55.29\n",
      "\n",
      "55.71\n",
      "57.18\n",
      "\n",
      "44.27\n",
      "43.69\n",
      "40.31\n",
      "44.33\n",
      "-\n",
      "45.06\n",
      "43.06\n",
      "43.34\n",
      "\n",
      "43.58\n",
      "44.57\n",
      "\n",
      "N/A\n",
      "60.16\n",
      "49.23\n",
      "68.26\n",
      "N/A\n",
      "N/A\n",
      "N/A\n",
      "77.06\n",
      "\n",
      "N/A\n",
      "51.48\n",
      "\n",
      "6.1.3 Implementation Details\n",
      "\n",
      "To comprehensively compare the performance of different LLMs, we employ diverse evaluation\n",
      "settings and design specific methods for guaranteeing the fairness and efficiency.\n",
      "\n",
      "• Zero-shot and few-shot settings: Following existing work [Qwen-Team, 2024], For LAM-\n",
      "BADA, HumanEval, MBPP, RACE, StoryCloze and RULER, we adopt the zero-shot setting.\n",
      "For GSM8K and MATH, we adopt the 4-shot setting. For MMLU, CMMLU, WinoGrande\n",
      "and CEval, we adopt the 5-shot setting. For HellaSwag, we adopt the 10-shot setting. For\n",
      "ARC-E, ARC-C, we adopt the 25-shot setting.\n",
      "\n",
      "• Chain-of-Thought (CoT): For GSM8K and MATH, we follow previous work [Qwen-Team,\n",
      "2024] that uses CoT prompting to facilitate the LLM to perform step-by-step reasoning.\n",
      "Considering the potential performance variance caused by CoT prompts, we utilize both the\n",
      "short ones provided by the original dataset and the long ones generated by kimi-k0-math.\n",
      "For each model, we evaluate the performance using both prompt types, and select the one\n",
      "yielding the higher score as the result.\n",
      "\n",
      "• Evaluation metrics: For QA tasks, we employ maj@1 for GSM8K and MATH, pass@1\n",
      "for HumanEval and MBPP, and accuracy of the model response for remaining generation\n",
      "tasks. For multiple-choice questions, we primarily evaluate based on the accuracy of the\n",
      "generated answer, which is determined by selecting the choice with the lowest perplexity.\n",
      "However, for ARC-E and ARC-C, we utilize normalized accuracy [Brown et al., 2020]. To\n",
      "accurately measure the performance of MATH-500, we further use gpt-4o-mini to verify\n",
      "the correctness of the results generated by all models and conducted manual checks.\n",
      "\n",
      "• Maximum length: For GSM8K and MATH, since CoT prompting may result in longer\n",
      "outputs, we set the maximum generation length to 596 for short context (i.e., 4K) models\n",
      "and 2,048 for long context models. For HumanEval and MBPP, we set the maximum\n",
      "generation length to 400. For other generative tasks, we set it to 128 for efficiency.\n",
      "\n",
      "• Evaluation framework: For the majority of tasks, we employ LLMBox [Tang et al., 2024b] to\n",
      "assess performance. Specifically, for generation tasks, we enable vLLM [Kwon et al., 2023].\n",
      "However, to ensure reproducibility, we utilize EvalPlus [Liu et al., 2024a] for HumanEval\n",
      "and MBPP.\n",
      "\n",
      "Despite our considerable efforts, fully reproducing the results of these baseline models as originally\n",
      "reported remains challenging, due to the lack of detailed evaluation setup information. For a fair\n",
      "comparison, we report the performance results of the baselines as provided in their official technical\n",
      "reports.\n",
      "\n",
      "22\n",
      "\n",
      "\fTable 7: Performance on commonsense reasoning benchmarks. Results marked with * are cited from\n",
      "their official paper or report.\n",
      "\n",
      "Models\n",
      "\n",
      "LAMBADA MMLU CMMLU CEval\n",
      "\n",
      "Hella\n",
      "Swag\n",
      "\n",
      "Wino\n",
      "Grande\n",
      "\n",
      "Story\n",
      "Cloze\n",
      "\n",
      "ARC-e ARC-c\n",
      "\n",
      "MiniCPM-2.6B\n",
      "Qwen2-1.5B\n",
      "Qwen2.5-0.5B\n",
      "Qwen2.5-1.5B\n",
      "Gemma2-2.6B\n",
      "StableLM2-1.7B\n",
      "SmolLM2-1.7B\n",
      "Llama3.2-3B\n",
      "\n",
      "YuLan-Mini\n",
      "\n",
      "61.91\n",
      "64.68\n",
      "52.00\n",
      "62.12\n",
      "-\n",
      "66.15\n",
      "67.42\n",
      "69.08\n",
      "\n",
      "64.72\n",
      "65.67\n",
      "\n",
      "53.37\n",
      "55.90\n",
      "47.50\n",
      "60.71\n",
      "52.20∗\n",
      "40.37\n",
      "51.91\n",
      "63.40\n",
      "\n",
      "51.79\n",
      "49.10\n",
      "\n",
      "48.97\n",
      "70.76\n",
      "52.17\n",
      "67.82\n",
      "-\n",
      "29.29\n",
      "33.46\n",
      "44.44\n",
      "\n",
      "48.35\n",
      "45.45\n",
      "\n",
      "67.92\n",
      "48.24\n",
      "71.94\n",
      "66.11\n",
      "50.54\n",
      "54.27\n",
      "69.05\n",
      "67.18\n",
      "28.00∗ 74.60∗\n",
      "69.79\n",
      "26.99\n",
      "72.96\n",
      "35.10\n",
      "75.62\n",
      "44.49\n",
      "\n",
      "51.47\n",
      "48.23\n",
      "\n",
      "68.65\n",
      "67.22\n",
      "\n",
      "65.74\n",
      "66.14\n",
      "55.88\n",
      "64.48\n",
      "71.50∗\n",
      "64.64\n",
      "67.40\n",
      "67.48\n",
      "\n",
      "67.09\n",
      "67.24\n",
      "\n",
      "78.51\n",
      "77.60\n",
      "71.67\n",
      "76.80\n",
      "-\n",
      "78.56\n",
      "79.32\n",
      "76.80\n",
      "\n",
      "76.37\n",
      "75.89\n",
      "\n",
      "55.51\n",
      "62.21\n",
      "56.10\n",
      "71.51\n",
      "-\n",
      "54.00\n",
      "44.82\n",
      "70.12\n",
      "\n",
      "69.87\n",
      "67.47\n",
      "\n",
      "43.86\n",
      "42.92\n",
      "39.51\n",
      "53.41\n",
      "55.70∗\n",
      "40.78\n",
      "35.49\n",
      "48.81\n",
      "\n",
      "50.51\n",
      "49.32\n",
      "\n",
      "6.2 Main Results\n",
      "\n",
      "The experimental results of different models on the specified benchmarks are shown in Table 6 and\n",
      "Table 7. Furthermore, we have selected 4 benchmarks from each of the two tables to construct\n",
      "Figure 1. Based on these results, we can identify the following key observations:\n",
      "\n",
      "Superior training efficacy Overall, YuLan-Mini achieves highly competitive performance com-\n",
      "pared to leading small industry models, despite being trained on a significantly smaller corpus (1.08T\n",
      "tokens). To ensure successful pre-training with relatively limited data, we meticulously designed the\n",
      "data pipeline, effectively mitigated training instability, and implemented annealing training. Addi-\n",
      "tionally, most of our training data comes from open-source and synthetic datasets, demonstrating that\n",
      "with careful data cleaning, selection, and scheduling, we can develop a robust base model even with\n",
      "limited resources in a university-level laboratory setting. This highlights the superior data efficiency\n",
      "of our pre-training approach.\n",
      "\n",
      "Excellence in mathematical and coding On specific benchmarks for mathematical reasoning\n",
      "(MATH-500 and GSM8K) and coding generation (HumanEval and MBPP), YuLan-mini achieves\n",
      "leading average performance. This consistent superiority can be mainly attributed to the use of\n",
      "high-quality pre-training corpus and reasoning synthetic data (e.g., formal mathematics reasoning\n",
      "problems and o1-like long thought data). Our core idea is to extend the types of reasoning data and\n",
      "enhance the complex reasoning capacities of our base model, which leads to large improvements on\n",
      "mathematical benchmarks.\n",
      "\n",
      "Strong general capability Beyond specialized tasks, YuLan-mini also demonstrates strong per-\n",
      "formance on various general benchmarks, spanning from language modeling and commonsense\n",
      "reasoning, highlighting the versatility of the model. It indicates that our pre-training approach well\n",
      "balances the learning of diverse abilities, resulting in a robust general-purpose foundation model.\n",
      "This success can be attributed to our data mixture and curriculum-based adjustment strategies, which\n",
      "carefully balance the initial proportion of math, coding, and general knowledge related corpora, while\n",
      "gradually enhancing them during the annealing stage to better develop advanced capabilities.\n",
      "\n",
      "Moderate long context capability Due to limited computing resources, YuLan-Mini has had\n",
      "limited exposure to long context training samples. As a result, its ability to model long contexts is\n",
      "not yet on par with state-of-the-art LLMs, as reflected in the results from the RULER benchmark.\n",
      "Additionally, due to a lack of GPU resources, our current approach can only extend the context\n",
      "window up to 28K. This limitation could be addressed with additional resources to support further\n",
      "development.\n",
      "\n",
      "6.3 Evaluating Model Performance during Pre-Training\n",
      "\n",
      "During pre-training, it is crucial to continuously evaluate the model’s performance to monitor\n",
      "for any unstable or abnormal training issues. However, existing benchmarks rely on advanced\n",
      "\n",
      "23\n",
      "\n",
      "\f(a) Performance curve on HumanEval.\n",
      "\n",
      "(b) Performance curve on GSM8K.\n",
      "\n",
      "Figure 10: Performance comparison using perplexity (PPL) and accuracy-based metrics to monitor\n",
      "the code generation and math reasoning abilities of YuLan-Mini.\n",
      "\n",
      "abilities (e.g., instruction following), which often develop with sufficient data training. Thus, the\n",
      "model’s performance tends to remain at a low level on these benchmarks in the early stages, and\n",
      "directly evaluating the model’s performance on specific validation sets would not provide an accurate\n",
      "assessment.\n",
      "\n",
      "To address this, we have designed two monitoring strategies for different stages of training. In the early\n",
      "stages, we assess the model’s performance primarily through perplexity measures on the constructed\n",
      "validation datasets and LAMBADA benchmark. In the later stages, we shift to using performance on\n",
      "selected benchmarks (e.g., HumanEval and GSM8K) for more comprehensive evaluation. Next, we\n",
      "introduce how to construct the validation set for perplexity measurement at early stage of pre-training.\n",
      "\n",
      "To comprehensively evaluate the key abilities of our model, we create four validation sets from the\n",
      "following aspects, namely English understanding, Chinese understanding, code generation, and math\n",
      "reasoning. The detailed data composition is as follows.\n",
      "\n",
      "• English understanding: We randomly select 2,118 samples from FineWeb-Edu and compute\n",
      "\n",
      "the perplexity for ability evaluation.\n",
      "\n",
      "• Chinese understanding: We randomly select 1,679 samples from Chinese-FineWeb-Edu for\n",
      "\n",
      "computing the perplexity.\n",
      "\n",
      "• Code generation: We randomly select 2,067 samples from a widely-used code instruction\n",
      "\n",
      "datasets, Python-Code-Instructions-18k-Alpaca for perplexity evaluation.9\n",
      "\n",
      "• Math reasoning: We randomly sample 1,499 open-ended questions from MathInstruct [Yue\n",
      "\n",
      "et al., 2024] for perplexity.\n",
      "\n",
      "Once the advanced capabilities are well-developed, we can directly monitor the model’s performance\n",
      "by evaluating it on the selected benchmarks.\n",
      "\n",
      "7 Conclusion\n",
      "\n",
      "In this paper, we have introduced YuLan-Mini, a highly capable base model with 2.42B parameters.\n",
      "We provide all the essential technical details and resources necessary to reproduce our model, includ-\n",
      "ing improvements to the training method for enhanced stability and efficiency, as well as tokenized\n",
      "data sources organized with a specially developed training curriculum. Extensive experiments have\n",
      "demonstrated the effectiveness of YuLan-Mini, showing that it achieves performance comparable\n",
      "to its industry counterparts of a similar parameter scale. Our primary contribution is enabling the\n",
      "reproduction of competitive language models in a highly data-efficient manner, making it feasible\n",
      "\n",
      "9https://huggingface.co/datasets/iamtarun/python_code_instructions_18k_alpaca\n",
      "\n",
      "24\n",
      "\n",
      "0100200300400500\u00007\u0000U\u0000D\u0000L\u0000Q\u0000H\u0000G\u0000\u0003\u00007\u0000R\u0000N\u0000H\u0000Q\u0000V\u0000\u0003\u0000\u000b\u0000%\u0000\f0.02.55.07.510.012.515.017.520.0\u0000+\u0000X\u0000P\u0000D\u0000Q\u0000(\u0000Y\u0000D\u0000O\u0000\u0003\u0000\u000b\u0000\u0013\u0000\u0010\u0000V\u0000K\u0000R\u0000W\u0000\u000f\u0000\u0003\u0000S\u0000D\u0000V\u0000V\u0000#\u0000N\u0000 \u0000\u0014\u0000\fHumanEval (0-shot, pass@k=1)Code PPL1.01.21.41.61.82.0\u0000&\u0000R\u0000G\u0000H\u0000\u0003\u00003\u00003\u0000/0100200300400500\u00007\u0000U\u0000D\u0000L\u0000Q\u0000H\u0000G\u0000\u0003\u00007\u0000R\u0000N\u0000H\u0000Q\u0000V\u0000\u0003\u0000\u000b\u0000%\u0000\f246810121416\u0000*\u00006\u00000\u0000\u0010\u0000\u001b\u0000.\u0000\u000b\u0000\u001b\u0000\u0010\u0000V\u0000K\u0000R\u0000W\u0000\fGSM-8K(8-shot)Math PPL1.21.41.61.82.02.22.42.6\u00000\u0000D\u0000W\u0000K\u0000\u0003\u00003\u00003\u0000/\ffor university-level laboratories. Additionally, by aligning the training data with intermediate check-\n",
      "points, our approach facilitates in-depth research on LLMs, such as exploring how model capacities\n",
      "develop during pre-training.\n",
      "\n",
      "In future work, we plan to release an instruct version of YuLan-Mini. Furthermore, we aim to\n",
      "extend YuLan-Mini to other architectures and training methods and also explore its specialization in\n",
      "professional domains (e.g., math and coding).\n",
      "\n",
      "Acknowledgment\n",
      "\n",
      "We sincerely thank Weizheng Lu and Xu Han for their assistance with this work. We encourage more\n",
      "researchers to collaborate in uncovering the pre-training secrets of LLMs.\n",
      "\n",
      "References\n",
      "\n",
      "Gretel AI. Synthetically generated reasoning dataset (gsm8k-inspired) with enhanced diversity using gretel\n",
      "navigator and meta-llama/meta-llama-3.1-405b. https://huggingface.co/gretelai/synthetic-gsm8k-reflection-\n",
      "405b, 9 2024.\n",
      "\n",
      "Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebrón, and Sumit Sanghai.\n",
      "GQA: training generalized multi-query transformer models from multi-head checkpoints. In Houda Bouamor,\n",
      "Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural\n",
      "Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pages 4895–4901. Association for\n",
      "Computational Linguistics, 2023. doi: 10.18653/V1/2023.EMNLP-MAIN.298. URL https://doi.org/\n",
      "10.18653/v1/2023.emnlp-main.298.\n",
      "\n",
      "Loubna Ben Allal, Anton Lozhkov, Elie Bakouch, Gabriel Martín Blázquez, Lewis Tunstall, Agustín Piqueres,\n",
      "Andres Marafioti, Cyril Zakka, Leandro von Werra, and Thomas Wolf. Smollm2 - with great data, comes\n",
      "great performance, 2024.\n",
      "\n",
      "Jacob Austin, Augustus Odena, Maxwell I. Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen\n",
      "Jiang, Carrie J. Cai, Michael Terry, Quoc V. Le, and Charles Sutton. Program synthesis with large language\n",
      "models. CoRR, abs/2108.07732, 2021. URL https://arxiv.org/abs/2108.07732.\n",
      "\n",
      "Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen Marcus McAleer, Albert Q.\n",
      "Jiang, Jia Deng, Stella Biderman, and Sean Welleck. Llemma: An open language model for mathematics. In\n",
      "The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11,\n",
      "2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=4WnqRR915j.\n",
      "\n",
      "Lei Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization. CoRR, abs/1607.06450, 2016.\n",
      "\n",
      "URL http://arxiv.org/abs/1607.06450.\n",
      "\n",
      "Edward Beeching, Shengyi Costa Huang, Albert Jiang, Jia Li, Benjamin Lipkin, Zihan Qina, Kashif Rasul, Ziju\n",
      "Shen, Roman Soletskyi, and Lewis Tunstall. Numinamath 7b cot, 2024. URL http://faculty.bicmr.\n",
      "pku.edu.cn/~dongbin/Publications/numina_dataset.pdf.\n",
      "\n",
      "Marco Bellagente, Jonathan Tow, Dakota Mahan, Duy Phung, Maksym Zhuravinskyi, Reshinth Adithyan,\n",
      "James Baicoianu, Ben Brooks, Nathan Cooper, Ashish Datta, Meng Lee, Emad Mostaque, Michael Pieler,\n",
      "Nikhil Pinnaparaju, Paulo Rocha, Harry Saini, Hannah Teufel, Niccoló Zanichelli, and Carlos Riquelme.\n",
      "Stable LM 2 1.6b technical report. CoRR, abs/2402.17834, 2024. doi: 10.48550/ARXIV.2402.17834. URL\n",
      "https://doi.org/10.48550/arXiv.2402.17834.\n",
      "\n",
      "Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui Ding, Kai Dong,\n",
      "Qiushi Du, Zhe Fu, Huazuo Gao, Kaige Gao, Wenjun Gao, Ruiqi Ge, Kang Guan, Daya Guo, Jianzhong\n",
      "Guo, Guangbo Hao, Zhewen Hao, Ying He, Wenjie Hu, Panpan Huang, Erhang Li, Guowei Li, Jiashi Li,\n",
      "Yao Li, Y. K. Li, Wenfeng Liang, Fangyun Lin, Alex X. Liu, Bo Liu, Wen Liu, Xiaodong Liu, Xin Liu,\n",
      "Yiyuan Liu, Haoyu Lu, Shanghao Lu, Fuli Luo, Shirong Ma, Xiaotao Nie, Tian Pei, Yishi Piao, Junjie Qiu,\n",
      "Hui Qu, Tongzheng Ren, Zehui Ren, Chong Ruan, Zhangli Sha, Zhihong Shao, Junxiao Song, Xuecheng\n",
      "Su, Jingxiang Sun, Yaofeng Sun, Minghui Tang, Bingxuan Wang, Peiyi Wang, Shiyu Wang, Yaohui Wang,\n",
      "Yongji Wang, Tong Wu, Y. Wu, Xin Xie, Zhenda Xie, Ziwei Xie, Yiliang Xiong, Hanwei Xu, R. X. Xu,\n",
      "Yanhong Xu, Dejian Yang, Yuxiang You, Shuiping Yu, Xingkai Yu, B. Zhang, Haowei Zhang, Lecong Zhang,\n",
      "Liyue Zhang, Mingchuan Zhang, Minghua Zhang, Wentao Zhang, Yichao Zhang, Chenggang Zhao, Yao\n",
      "Zhao, Shangyan Zhou, Shunfeng Zhou, Qihao Zhu, and Yuheng Zou. Deepseek LLM: scaling open-source\n",
      "language models with longtermism. CoRR, abs/2401.02954, 2024. doi: 10.48550/ARXIV.2401.02954. URL\n",
      "https://doi.org/10.48550/arXiv.2401.02954.\n",
      "\n",
      "25\n",
      "\n",
      "\fPiotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. Enriching word vectors with subword\n",
      "ISSN\n",
      "\n",
      "information. Transactions of the Association for Computational Linguistics, 5:135–146, 2017.\n",
      "2307-387X.\n",
      "\n",
      "Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\n",
      "Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen\n",
      "Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter,\n",
      "Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark,\n",
      "Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models\n",
      "are few-shot learners, 2020. URL https://arxiv.org/abs/2005.14165.\n",
      "\n",
      "Jie Chen, Zhipeng Chen, Jiapeng Wang, Kun Zhou, Yutao Zhu, Jinhao Jiang, Yingqian Min, Wayne Xin Zhao,\n",
      "Zhicheng Dou, Jiaxin Mao, Yankai Lin, Ruihua Song, Jun Xu, Xu Chen, Rui Yan, Zhewei Wei, Di Hu,\n",
      "Wenbing Huang, and Ji-Rong Wen. Towards effective and efficient continual pre-training of large language\n",
      "models. CoRR, abs/2407.18743, 2024. doi: 10.48550/ARXIV.2407.18743. URL https://doi.org/10.\n",
      "48550/arXiv.2407.18743.\n",
      "\n",
      "Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Pondé de Oliveira Pinto, Jared Kaplan, Harri\n",
      "Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael\n",
      "Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov,\n",
      "Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such,\n",
      "Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen\n",
      "Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,\n",
      "William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan\n",
      "Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob\n",
      "McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language\n",
      "models trained on code. CoRR, abs/2107.03374, 2021. URL https://arxiv.org/abs/2107.03374.\n",
      "\n",
      "Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending context window of large\n",
      "language models via positional interpolation. CoRR, abs/2306.15595, 2023. doi: 10.48550/ARXIV.2306.\n",
      "15595. URL https://doi.org/10.48550/arXiv.2306.15595.\n",
      "\n",
      "Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul\n",
      "Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha\n",
      "Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prab-\n",
      "hakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael\n",
      "Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk\n",
      "Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito,\n",
      "David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani\n",
      "Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor\n",
      "Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang,\n",
      "Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck,\n",
      "Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language modeling with pathways. J. Mach. Learn.\n",
      "Res., 24:240:1–240:113, 2023. URL https://jmlr.org/papers/v24/22-1144.html.\n",
      "\n",
      "Woojin Chung, Jiwoo Hong, Na Min An, James Thorne, and Se-Young Yun. Stable language model pre-training\n",
      "by reducing embedding variability.\n",
      "In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors,\n",
      "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, EMNLP 2024,\n",
      "Miami, FL, USA, November 12-16, 2024, pages 10852–10863. Association for Computational Linguistics,\n",
      "2024. URL https://aclanthology.org/2024.emnlp-main.606.\n",
      "\n",
      "Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert,\n",
      "Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to\n",
      "solve math word problems. CoRR, abs/2110.14168, 2021. URL https://arxiv.org/abs/2110.14168.\n",
      "\n",
      "Gautier Dagan, Gabriel Synnaeve, and Baptiste Rozière. Getting the most out of your tokenizer for pre-\n",
      "training and domain adaptation. In Forty-first International Conference on Machine Learning, ICML 2024,\n",
      "Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=\n",
      "ZFYBnLljtT.\n",
      "\n",
      "Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning.\n",
      "\n",
      "In The Twelfth\n",
      "International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. Open-\n",
      "Review.net, 2024. URL https://openreview.net/forum?id=mZn2Xyh9Ec.\n",
      "\n",
      "Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. Flashattention: Fast and memory-\n",
      "efficient exact attention with io-awareness. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave,\n",
      "\n",
      "26\n",
      "\n",
      "\fK. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems 35: Annual Confer-\n",
      "ence on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, Novem-\n",
      "ber 28 - December 9, 2022, 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/\n",
      "67d57c32e20fd0a7a302cb81d36e40d5-Abstract-Conference.html.\n",
      "\n",
      "Alexandre de Brébisson and Pascal Vincent. The z-loss: a shift and scale invariant classification loss belonging\n",
      "\n",
      "to the spherical family. CoRR, abs/1604.08859, 2016. URL http://arxiv.org/abs/1604.08859.\n",
      "\n",
      "DeepSeek-AI, Aixin Liu, Bei Feng, Bin Wang, Bingxuan Wang, Bo Liu, Chenggang Zhao, Chengqi Deng, Chong\n",
      "Ruan, Damai Dai, Daya Guo, Dejian Yang, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fuli Luo, Guangbo\n",
      "Hao, Guanting Chen, Guowei Li, Hao Zhang, Hanwei Xu, Hao Yang, Haowei Zhang, Honghui Ding, Huajian\n",
      "Xin, Huazuo Gao, Hui Li, Hui Qu, J. L. Cai, Jian Liang, Jianzhong Guo, Jiaqi Ni, Jiashi Li, Jin Chen, Jingyang\n",
      "Yuan, Junjie Qiu, Junxiao Song, Kai Dong, Kaige Gao, Kang Guan, Lean Wang, Lecong Zhang, Lei Xu,\n",
      "Leyi Xia, Liang Zhao, Liyue Zhang, Meng Li, Miaojun Wang, Mingchuan Zhang, Minghua Zhang, Minghui\n",
      "Tang, Mingming Li, Ning Tian, Panpan Huang, Peiyi Wang, Peng Zhang, Qihao Zhu, Qinyu Chen, Qiushi\n",
      "Du, R. J. Chen, R. L. Jin, Ruiqi Ge, Ruizhe Pan, Runxin Xu, Ruyi Chen, S. S. Li, Shanghao Lu, Shangyan\n",
      "Zhou, Shanhuang Chen, Shaoqing Wu, Shengfeng Ye, Shirong Ma, Shiyu Wang, Shuang Zhou, Shuiping Yu,\n",
      "Shunfeng Zhou, Size Zheng, Tao Wang, Tian Pei, Tian Yuan, Tianyu Sun, W. L. Xiao, Wangding Zeng, Wei\n",
      "An, Wen Liu, Wenfeng Liang, Wenjun Gao, Wentao Zhang, X. Q. Li, Xiangyue Jin, Xianzu Wang, Xiao Bi,\n",
      "Xiaodong Liu, Xiaohan Wang, Xiaojin Shen, Xiaokang Chen, Xiaosha Chen, Xiaotao Nie, and Xiaowen Sun.\n",
      "Deepseek-v2: A strong, economical, and efficient mixture-of-experts language model. CoRR, abs/2405.04434,\n",
      "2024. doi: 10.48550/ARXIV.2405.04434. URL https://doi.org/10.48550/arXiv.2405.04434.\n",
      "\n",
      "Nolan Dey, Gurpreet Gosal, Zhiming Chen, Hemant Khachane, William Marshall, Ribhu Pathria, Marvin\n",
      "Tom, and Joel Hestness. Cerebras-gpt: Open compute-optimal language models trained on the cerebras\n",
      "wafer-scale cluster. CoRR, abs/2304.03208, 2023a. doi: 10.48550/ARXIV.2304.03208. URL https:\n",
      "//doi.org/10.48550/arXiv.2304.03208.\n",
      "\n",
      "Nolan Dey, Gurpreet Gosal, Zhiming, Chen, Hemant Khachane, William Marshall, Ribhu Pathria, Marvin\n",
      "Tom, and Joel Hestness. Cerebras-GPT: Open Compute-Optimal Language Models Trained on the Cerebras\n",
      "Wafer-Scale Cluster, April 2023b. URL http://arxiv.org/abs/2304.03208. arXiv:2304.03208 [cs].\n",
      "\n",
      "Hantian Ding, Zijian Wang, Giovanni Paolini, Varun Kumar, Anoop Deoras, Dan Roth, and Stefano Soatto.\n",
      "Fewer truncations improve language modeling. In Forty-first International Conference on Machine Learning,\n",
      "ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024a. URL https://openreview.net/\n",
      "forum?id=kRxCDDFNpp.\n",
      "\n",
      "Yuyang Ding, Xinyu Shi, Xiaobo Liang, Juntao Li, Qiaoming Zhu, and Min Zhang. Unleashing Reasoning\n",
      "Capability of LLMs via Scalable Question Synthesis from Scratch, October 2024b. URL http://arxiv.\n",
      "org/abs/2410.18693. arXiv:2410.18693 [cs].\n",
      "\n",
      "Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman,\n",
      "Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi\n",
      "Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurélien Rodriguez,\n",
      "Austen Gregerson, Ava Spataru, Baptiste Rozière, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte\n",
      "Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret,\n",
      "Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song,\n",
      "Danielle Pintz, Danny Livshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego\n",
      "Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael\n",
      "Smith, Filip Radenovic, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Graeme\n",
      "Nail, Grégoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo\n",
      "Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel M. Kloumann, Ishan Misra, Ivan Evtimov, Jade Copet,\n",
      "Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer\n",
      "Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao\n",
      "Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia,\n",
      "Kalyan Vasuden Alwala, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, and\n",
      "et al. The llama 3 herd of models. CoRR, abs/2407.21783, 2024. doi: 10.48550/ARXIV.2407.21783. URL\n",
      "https://doi.org/10.48550/arXiv.2407.21783.\n",
      "\n",
      "Tianyu Gao, Alexander Wettig, Howard Yen, and Danqi Chen. How to train long-context language models\n",
      "(effectively). CoRR, abs/2410.02660, 2024. doi: 10.48550/ARXIV.2410.02660. URL https://doi.org/\n",
      "10.48550/arXiv.2410.02660.\n",
      "\n",
      "Dirk Groeneveld, Iz Beltagy, Evan Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, Ananya Harsh\n",
      "Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, Shane Arora, David Atkinson, Russell Authur, Khy-\n",
      "athi Raghavi Chandu, Arman Cohan, Jennifer Dumas, Yanai Elazar, Yuling Gu, Jack Hessel, Tushar\n",
      "Khot, William Merrill, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew E.\n",
      "\n",
      "27\n",
      "\n",
      "\fPeters, Valentina Pyatkin, Abhilasha Ravichander, Dustin Schwenk, Saurabh Shah, Will Smith, Emma\n",
      "Strubell, Nishant Subramani, Mitchell Wortsman, Pradeep Dasigi, Nathan Lambert, Kyle Richardson,\n",
      "Luke Zettlemoyer, Jesse Dodge, Kyle Lo, Luca Soldaini, Noah A. Smith, and Hannaneh Hajishirzi.\n",
      "In Lun-Wei Ku, Andre Martins, and Vivek\n",
      "Olmo: Accelerating the science of language models.\n",
      "Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Lin-\n",
      "guistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pages 15789–\n",
      "15809. Association for Computational Linguistics, 2024. doi: 10.18653/V1/2024.ACL-LONG.841. URL\n",
      "https://doi.org/10.18653/v1/2024.acl-long.841.\n",
      "\n",
      "Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Y. Wu,\n",
      "Y. K. Li, Fuli Luo, Yingfei Xiong, and Wenfeng Liang. Deepseek-coder: When the large language model meets\n",
      "programming - the rise of code intelligence. CoRR, abs/2401.14196, 2024. doi: 10.48550/ARXIV.2401.14196.\n",
      "URL https://doi.org/10.48550/arXiv.2401.14196.\n",
      "\n",
      "Alexander Hägele, Elie Bakouch, Atli Kosson, Loubna Ben Allal, Leandro von Werra, and Martin Jaggi. Scaling\n",
      "laws and compute-optimal training beyond fixed training durations. CoRR, abs/2405.18392, 2024. doi:\n",
      "10.48550/ARXIV.2405.18392. URL https://doi.org/10.48550/arXiv.2405.18392.\n",
      "\n",
      "Xiaotian Han, Yiren Jian, Xuefeng Hu, Haogeng Liu, Yiqi Wang, Qihang Fan, Yuang Ai, Huaibo Huang, Ran\n",
      "He, Zhenheng Yang, and Quanzeng You. Infimm-webmath-40b: Advancing multimodal pre-training for\n",
      "enhanced mathematical reasoning. CoRR, abs/2409.12568, 2024. doi: 10.48550/ARXIV.2409.12568. URL\n",
      "https://doi.org/10.48550/arXiv.2409.12568.\n",
      "\n",
      "Conghui He, Zhenjiang Jin, Chao Xu, Jiantao Qiu, Bin Wang, Wei Li, Hang Yan, Jiaqi Wang, and Dahua Lin.\n",
      "Wanjuan: A comprehensive multimodal dataset for advancing english and chinese large models. CoRR,\n",
      "abs/2308.10755, 2023. doi: 10.48550/ARXIV.2308.10755. URL https://doi.org/10.48550/arXiv.\n",
      "2308.10755.\n",
      "\n",
      "Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Stein-\n",
      "hardt. Measuring massive multitask language understanding. In 9th International Conference on Learn-\n",
      "ing Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021a. URL\n",
      "https://openreview.net/forum?id=d7KBjmI3GmQ.\n",
      "\n",
      "Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song,\n",
      "and Jacob Steinhardt. Measuring mathematical problem solving with the MATH dataset.\n",
      "In Joaquin\n",
      "Vanschoren and Sai-Kit Yeung, editors, Proceedings of the Neural Information Processing Systems\n",
      "Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, vir-\n",
      "tual, 2021b. URL https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/\n",
      "be83ab3ecd0db773eb2dc1b0a17836a1-Abstract-round2.html.\n",
      "\n",
      "Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, Yang Zhang,\n",
      "and Boris Ginsburg. RULER: what’s the real context size of your long-context language models? CoRR,\n",
      "abs/2404.06654, 2024. doi: 10.48550/ARXIV.2404.06654. URL https://doi.org/10.48550/arXiv.\n",
      "2404.06654.\n",
      "\n",
      "Pin-Lun Hsu, Yun Dai, Vignesh Kothapalli, Qingquan Song, Shao Tang, Siyu Zhu, Steven Shimizu, Shivam\n",
      "Sahni, Haowen Ning, and Yanning Chen. Liger kernel: Efficient triton kernels for LLM training. CoRR,\n",
      "abs/2410.10989, 2024. doi: 10.48550/ARXIV.2410.10989. URL https://doi.org/10.48550/arXiv.\n",
      "2410.10989.\n",
      "\n",
      "Shengding Hu, Yuge Tu, Xu Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei Fang, Yuxiang\n",
      "Huang, Weilin Zhao, Xinrong Zhang, Zhen Leng Thai, Kai Zhang, Chongyi Wang, Yuan Yao, Chenyang\n",
      "Zhao, Jie Zhou, Jie Cai, Zhongwu Zhai, Ning Ding, Chao Jia, Guoyang Zeng, Dahai Li, Zhiyuan Liu, and\n",
      "Maosong Sun. Minicpm: Unveiling the potential of small language models with scalable training strategies.\n",
      "CoRR, abs/2404.06395, 2024. doi: 10.48550/ARXIV.2404.06395. URL https://doi.org/10.48550/\n",
      "arXiv.2404.06395.\n",
      "\n",
      "Jie Huang and Kevin Chen-Chuan Chang. Towards reasoning in large language models: A survey. In Anna\n",
      "Rogers, Jordan L. Boyd-Graber, and Naoaki Okazaki, editors, Findings of the Association for Computational\n",
      "Linguistics: ACL 2023, Toronto, Canada, July 9-14, 2023, pages 1049–1065. Association for Computational\n",
      "Linguistics, 2023. doi: 10.18653/V1/2023.FINDINGS-ACL.67. URL https://doi.org/10.18653/v1/\n",
      "2023.findings-acl.67.\n",
      "\n",
      "Siming Huang, Tianhao Cheng, J. K. Liu, Jiaran Hao, Liuyihan Song, Yang Xu, J. Yang, J. H. Liu, Chenchen\n",
      "Zhang, Linzheng Chai, Ruifeng Yuan, Zhaoxiang Zhang, Jie Fu, Qian Liu, Ge Zhang, Zili Wang, Yuan Qi,\n",
      "Yinghui Xu, and Wei Chu. OpenCoder: The Open Cookbook for Top-Tier Code Large Language Models.\n",
      "CoRR, abs/2411.04905, 2024. doi: 10.48550/ARXIV.2308.10755. URL https://doi.org/10.48550/\n",
      "arXiv.2411.04905.\n",
      "\n",
      "28\n",
      "\n",
      "\fYuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng\n",
      "Lv, Yikai Zhang, Jiayi Lei, Yao Fu, Maosong Sun, and Junxian He. C-eval: A multi-level multi-discipline\n",
      "chinese evaluation suite for foundation models. In Advances in Neural Information Processing Systems, 2023.\n",
      "\n",
      "Jinhao Jiang, Zhipeng Chen, Yingqian Min, Jie Chen, Xiaoxue Cheng, Jiapeng Wang, Yiru Tang, Haoxiang\n",
      "Sun, Jia Deng, Wayne Xin Zhao, Zheng Liu, Dong Yan, Jian Xie, Zhongyuan Wang, and Ji-Rong Wen.\n",
      "Technical report: Enhancing llm reasoning with reward-guided tree search. CoRR, abs/2411.11694, 2024.\n",
      "doi: 10.48550/ARXIV.2411.11694. URL https://doi.org/10.48550/arXiv.2411.11694.\n",
      "\n",
      "Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, and Qun Liu. TinyBERT:\n",
      "Distilling BERT for Natural Language Understanding, October 2020. URL http://arxiv.org/abs/1909.\n",
      "10351. Issue: arXiv:1909.10351 1097 citations (Semantic Scholar/arXiv) [2023-07-31] arXiv:1909.10351\n",
      "[cs].\n",
      "\n",
      "Marek Kadlcík, Michal Stefánik, Ondrej Sotolár, and Vlastimil Martinek. Calc-x and calcformers: Em-\n",
      "powering arithmetical chain-of-thought through interaction with symbolic systems. In Houda Bouamor,\n",
      "Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in\n",
      "Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pages 12101–12108.\n",
      "Association for Computational Linguistics, 2023. doi: 10.18653/V1/2023.EMNLP-MAIN.742. URL\n",
      "https://doi.org/10.18653/v1/2023.emnlp-main.742.\n",
      "\n",
      "J. Kaplan,\n",
      "\n",
      "Sam McCandlish,\n",
      "\n",
      "T. Henighan,\n",
      "\n",
      "Child, Scott Gray, Alec Radford,\n",
      "ArXiv,\n",
      "ral Language Models.\n",
      "org/paper/Scaling-Laws-for-Neural-Language-Models-Kaplan-McCandlish/\n",
      "e6c561d02500b2596a230b341a8eb8b921ca5bf2.\n",
      "\n",
      "Jeff Wu, and Dario Amodei.\n",
      "January 2020.\n",
      "\n",
      "Tom B. Brown, Benjamin Chess, Rewon\n",
      "Scaling Laws for Neu-\n",
      "URL https://www.semanticscholar.\n",
      "\n",
      "Mehran Kazemi, Najoung Kim, Deepti Bhatia, Xin Xu, and Deepak Ramachandran. LAMBADA: backward\n",
      "In Anna Rogers, Jordan L. Boyd-Graber, and\n",
      "chaining for automated reasoning in natural language.\n",
      "Naoaki Okazaki, editors, Proceedings of the 61st Annual Meeting of the Association for Computational\n",
      "Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, pages 6547–6568.\n",
      "Association for Computational Linguistics, 2023. doi: 10.18653/V1/2023.ACL-LONG.361. URL https:\n",
      "//doi.org/10.18653/v1/2023.acl-long.361.\n",
      "\n",
      "Jisu Kim and Juhwan Lee. Strategic data ordering: Enhancing large language model performance through\n",
      "curriculum learning. CoRR, abs/2405.07490, 2024. doi: 10.48550/ARXIV.2405.07490. URL https:\n",
      "//doi.org/10.48550/arXiv.2405.07490.\n",
      "\n",
      "Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao\n",
      "Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention.\n",
      "In Jason Flinn, Margo I. Seltzer, Peter Druschel, Antoine Kaufmann, and Jonathan Mace, editors, Proceedings\n",
      "of the 29th Symposium on Operating Systems Principles, SOSP 2023, Koblenz, Germany, October 23-26,\n",
      "2023, pages 611–626. ACM, 2023. doi: 10.1145/3600006.3613165. URL https://doi.org/10.1145/\n",
      "3600006.3613165.\n",
      "\n",
      "Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. Race: Large-scale reading comprehension\n",
      "\n",
      "dataset from examinations, 2017. URL https://arxiv.org/abs/1704.04683.\n",
      "\n",
      "Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester\n",
      "James V. Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, Yuling Gu, Saumya Malik, Victoria Graf, Jena D.\n",
      "Hwang, Jiangjiang Yang, Ronan Le Bras, Oyvind Tafjord, Chris Wilhelm, Luca Soldaini, Noah A. Smith,\n",
      "Yizhong Wang, Pradeep Dasigi, and Hannaneh Hajishirzi. Tulu 3: Pushing frontiers in open language\n",
      "model post-training. CoRR, abs/2411.15124, 2024. doi: 10.48550/ARXIV.2411.15124. URL https:\n",
      "//doi.org/10.48550/arXiv.2411.15124.\n",
      "\n",
      "Joonhyung Lee, Jeongin Bae, Byeongwook Kim, Se Jung Kwon, and Dongsoo Lee. To FP8 and back again:\n",
      "Quantifying the effects of reducing precision on LLM training stability. CoRR, abs/2405.18710, 2024. doi:\n",
      "10.48550/ARXIV.2405.18710. URL https://doi.org/10.48550/arXiv.2405.18710.\n",
      "\n",
      "Conglong Li, Minjia Zhang, and Yuxiong He. The stability-efficiency dilemma: Investigating sequence\n",
      "length warmup for training GPT models. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave,\n",
      "K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems 35: Annual Confer-\n",
      "ence on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, Novem-\n",
      "ber 28 - December 9, 2022, 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/\n",
      "aac02401755a65904cf977a33136af4a-Abstract-Conference.html.\n",
      "\n",
      "29\n",
      "\n",
      "\fHaonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan Duan, and Timothy Baldwin.\n",
      "CMMLU: measuring massive multitask language understanding in chinese. In Lun-Wei Ku, Andre Martins,\n",
      "and Vivek Srikumar, editors, Findings of the Association for Computational Linguistics, ACL 2024, Bangkok,\n",
      "Thailand and virtual meeting, August 11-16, 2024, pages 11260–11285. Association for Computational\n",
      "Linguistics, 2024a. doi: 10.18653/V1/2024.FINDINGS-ACL.671. URL https://doi.org/10.18653/\n",
      "v1/2024.findings-acl.671.\n",
      "\n",
      "Jeffrey Li, Alex Fang, Georgios Smyrnis, Maor Ivgi, Matt Jordan, Samir Yitzhak Gadre, Hritik Bansal, Etash Ku-\n",
      "mar Guha, Sedrick Keh, Kushal Arora, Saurabh Garg, Rui Xin, Niklas Muennighoff, Reinhard Heckel, Jean\n",
      "Mercat, Mayee Chen, Suchin Gururangan, Mitchell Wortsman, Alon Albalak, Yonatan Bitton, Marianna\n",
      "Nezhurina, Amro Abbas, Cheng-Yu Hsieh, Dhruba Ghosh, Josh Gardner, Maciej Kilian, Hanlin Zhang, Rulin\n",
      "Shao, Sarah M. Pratt, Sunny Sanyal, Gabriel Ilharco, Giannis Daras, Kalyani Marathe, Aaron Gokaslan,\n",
      "Jieyu Zhang, Khyathi Raghavi Chandu, Thao Nguyen, Igor Vasiljevic, Sham M. Kakade, Shuran Song, Sujay\n",
      "Sanghavi, Fartash Faghri, Sewoong Oh, Luke Zettlemoyer, Kyle Lo, Alaaeldin El-Nouby, Hadi Pouransari,\n",
      "Alexander Toshev, Stephanie Wang, Dirk Groeneveld, Luca Soldaini, Pang Wei Koh, Jenia Jitsev, Thomas Kol-\n",
      "lar, Alexandros G. Dimakis, Yair Carmon, Achal Dave, Ludwig Schmidt, and Vaishaal Shankar. Datacomp-lm:\n",
      "In search of the next generation of training sets for language models. CoRR, abs/2406.11794, 2024b. doi:\n",
      "10.48550/ARXIV.2406.11794. URL https://doi.org/10.48550/arXiv.2406.11794.\n",
      "\n",
      "Jia LI, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Costa Huang, Kashif Rasul,\n",
      "Longhui Yu, Albert Jiang, Ziju Shen, Zihan Qin, Bin Dong, Li Zhou, Yann Fleureau, Guillaume Lample,\n",
      "and Stanislas Polu. Numinamath. [https://github.com/project-numina/aimo-progress-prize]\n",
      "(https://github.com/project-numina/aimo-progress-prize/blob/main/report/numina_\n",
      "dataset.pdf), 2024.\n",
      "\n",
      "Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc\n",
      "Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo, Thomas\n",
      "Wang, Olivier Dehaene, Mishig Davaadorj, Joel Lamy-Poirier, João Monteiro, Oleh Shliazhko, Nicolas\n",
      "Gontier, Nicholas Meade, Armel Zebaze, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Benjamin\n",
      "Lipkin, Muhtasham Oblokulov, Zhiruo Wang, Rudra Murthy V, Jason T. Stillerman, Siva Sankalp Patel,\n",
      "Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, Nour Fahmy, Urvashi Bhattacharyya, Wenhao\n",
      "Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas, Maxim Kunakov, Fedor Zhdanov, Manuel Romero, Tony\n",
      "Lee, Nadav Timor, Jennifer Ding, Claire Schlesinger, Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra,\n",
      "Alex Gu, Jennifer Robinson, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva Reddy,\n",
      "Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Muñoz Ferrandis, Sean Hughes, Thomas Wolf, Arjun\n",
      "Guha, Leandro von Werra, and Harm de Vries. Starcoder: may the source be with you! Trans. Mach. Learn.\n",
      "Res., 2023, 2023. URL https://openreview.net/forum?id=KoFOg41haE.\n",
      "\n",
      "Wing Lian, Guan Wang, Bleys Goodson, Eugene Pentland, Austin Cook, Chanvichet Vong, and \"Teknium\".\n",
      "Slimorca: An open dataset of gpt-4 augmented flan reasoning traces, with verification, 2023. URL https:\n",
      "//https://huggingface.co/Open-Orca/SlimOrca.\n",
      "\n",
      "Xinyu Lian, Sam Ade Jacobs, Lev Kurilenko, Masahiro Tanaka, Stas Bekman, Olatunji Ruwase, and Minjia\n",
      "Zhang. Universal checkpointing: Efficient and flexible checkpointing for large scale distributed training.\n",
      "CoRR, abs/2406.18820, 2024. doi: 10.48550/ARXIV.2406.18820. URL https://doi.org/10.48550/\n",
      "arXiv.2406.18820.\n",
      "\n",
      "Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John\n",
      "Schulman, Ilya Sutskever, and Karl Cobbe. Let’s verify step by step. In The Twelfth International Conference\n",
      "on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL\n",
      "https://openreview.net/forum?id=v8L0pN6EOi.\n",
      "\n",
      "Haohan Lin, Zhiqing Sun, Yiming Yang, and Sean Welleck. Lean-star: Learning to interleave thinking and\n",
      "proving. CoRR, abs/2407.10040, 2024. doi: 10.48550/ARXIV.2407.10040. URL https://doi.org/10.\n",
      "48550/arXiv.2407.10040.\n",
      "\n",
      "Jiawei Liu, Songrun Xie, Junhao Wang, Yuxiang Wei, Yifeng Ding, and Lingming Zhang. Evaluating language\n",
      "models for efficient code generation. In First Conference on Language Modeling, 2024a. URL https:\n",
      "//openreview.net/forum?id=IBCBMeAhmC.\n",
      "\n",
      "Xiaoran Liu, Kai Lv, Qipeng Guo, Hang Yan, Conghui He, Xipeng Qiu, and Dahua Lin. Longwanjuan:\n",
      "Towards systematic measurement for long text quality. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung\n",
      "Chen, editors, Findings of the Association for Computational Linguistics: EMNLP 2024, Miami, Florida,\n",
      "USA, November 12-16, 2024, pages 5709–5725. Association for Computational Linguistics, 2024b. URL\n",
      "https://aclanthology.org/2024.findings-emnlp.327.\n",
      "\n",
      "30\n",
      "\n",
      "\fZechun Liu, Changsheng Zhao, Forrest N. Iandola, Chen Lai, Yuandong Tian, Igor Fedorov, Yunyang Xiong,\n",
      "Ernie Chang, Yangyang Shi, Raghuraman Krishnamoorthi, Liangzhen Lai, and Vikas Chandra. Mobilellm:\n",
      "Optimizing sub-billion parameter language models for on-device use cases. In Forty-first International\n",
      "Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024c.\n",
      "URL https://openreview.net/forum?id=EIGbXbxcUQ.\n",
      "\n",
      "Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In 7th International Conference on\n",
      "Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. URL\n",
      "https://openreview.net/forum?id=Bkg6RiCqY7.\n",
      "\n",
      "Anton Lozhkov, Loubna Ben Allal, Leandro von Werra, and Thomas Wolf. Fineweb-edu, May 2024a. URL\n",
      "\n",
      "https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu.\n",
      "\n",
      "Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao Tang,\n",
      "Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, Tianyang Liu, Max Tian, Denis Kocetkov, Arthur Zucker, Younes\n",
      "Belkada, Zijian Wang, Qian Liu, Dmitry Abulkhanov, Indraneil Paul, Zhuang Li, Wen-Ding Li, Megan\n",
      "Risdal, Jia Li, Jian Zhu, Terry Yue Zhuo, Evgenii Zheltonozhskii, Nii Osae Osae Dade, Wenhao Yu, Lucas\n",
      "Krauß, Naman Jain, Yixuan Su, Xuanli He, Manan Dey, Edoardo Abati, Yekun Chai, Niklas Muennighoff,\n",
      "Xiangru Tang, Muhtasham Oblokulov, Christopher Akiki, Marc Marone, Chenghao Mou, Mayank Mishra,\n",
      "Alex Gu, Binyuan Hui, Tri Dao, Armel Zebaze, Olivier Dehaene, Nicolas Patry, Canwen Xu, Julian J.\n",
      "McAuley, Han Hu, Torsten Scholak, Sébastien Paquet, Jennifer Robinson, Carolyn Jane Anderson, Nicolas\n",
      "Chapados, and et al. Starcoder 2 and the stack v2: The next generation. CoRR, abs/2402.19173, 2024b. doi:\n",
      "10.48550/ARXIV.2402.19173. URL https://doi.org/10.48550/arXiv.2402.19173.\n",
      "\n",
      "Keming Lu, Hongyi Yuan, Zheng Yuan, Runji Lin, Junyang Lin, Chuanqi Tan, Chang Zhou, and Jingren\n",
      "Zhou. #instag: Instruction tagging for analyzing supervised fine-tuning of large language models. In The\n",
      "Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024.\n",
      "OpenReview.net, 2024. URL https://openreview.net/forum?id=pszewhybU9.\n",
      "\n",
      "Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane\n",
      "Rivière, Mihir Sanjay Kale, Juliette Love, Pouya Tafti, Léonard Hussenot, Aakanksha Chowdhery, Adam\n",
      "Roberts, Aditya Barua, Alex Botev, Alex Castro-Ros, Ambrose Slone, Amélie Héliou, Andrea Tacchetti,\n",
      "Anna Bulanova, Antonia Paterson, Beth Tsai, Bobak Shahriari, Charline Le Lan, Christopher A. Choquette-\n",
      "Choo, Clément Crepy, Daniel Cer, Daphne Ippolito, David Reid, Elena Buchatskaya, Eric Ni, Eric Noland,\n",
      "Geng Yan, George Tucker, George-Cristian Muraru, Grigory Rozhdestvenskiy, Henryk Michalewski, Ian\n",
      "Tenney, Ivan Grishchenko, Jacob Austin, James Keeling, Jane Labanowski, Jean-Baptiste Lespiau, Jeff\n",
      "Stanway, Jenny Brennan, Jeremy Chen, Johan Ferret, Justin Chiu, and et al. Gemma: Open models based on\n",
      "gemini research and technology. CoRR, abs/2403.08295, 2024. doi: 10.48550/ARXIV.2403.08295. URL\n",
      "https://doi.org/10.48550/arXiv.2403.08295.\n",
      "\n",
      "Yingqian Min, Zhipeng Chen, Jinhao Jiang, Jie Chen, Jia Deng, Yiwen Hu, Yiru Tang, Jiapeng Wang,\n",
      "Xiaoxue Cheng, Huatong Song, Wayne Xin Zhao, Zheng Liu, Zhongyuan Wang, and Ji-Rong Wen.\n",
      "Imitate, explore, and self-improve: A reproduction report on slow-thinking reasoning systems. arXiv\n",
      "preprint arXiv:2412.09413, abs/2412.09413, 2024. doi: 10.48550/ARXIV.2412.09413. URL https:\n",
      "//doi.org/10.48550/arXiv.2412.09413.\n",
      "\n",
      "Arindam Mitra, Luciano Del Corro, Guoqing Zheng, Shweti Mahajan, Dany Rouhana, Andrés Codas, Yadong\n",
      "Lu, Weige Chen, Olga Vrousgos, Corby Rosset, Fillipe Silva, Hamed Khanpour, Yash Lara, and Ahmed\n",
      "Awadallah. Agentinstruct: Toward generative teaching with agentic flows. CoRR, abs/2407.03502, 2024a.\n",
      "doi: 10.48550/ARXIV.2407.03502. URL https://doi.org/10.48550/arXiv.2407.03502.\n",
      "\n",
      "Arindam Mitra, Hamed Khanpour, Corby Rosset, and Ahmed Awadallah. Orca-math: Unlocking the potential\n",
      "of slms in grade school math. CoRR, abs/2402.14830, 2024b. doi: 10.48550/ARXIV.2402.14830. URL\n",
      "https://doi.org/10.48550/arXiv.2402.14830.\n",
      "\n",
      "Igor Molybog, Peter Albert, Moya Chen, Zachary DeVito, David Esiobu, Naman Goyal, Punit Singh Koura,\n",
      "Sharan Narang, Andrew Poulton, Ruan Silva, Binh Tang, Diana Liskovich, Puxin Xu, Yuchen Zhang, Melanie\n",
      "Kambadur, Stephen Roller, and Susan Zhang. A theory on adam instability in large-scale machine learning.\n",
      "CoRR, abs/2304.09871, 2023. doi: 10.48550/ARXIV.2304.09871. URL https://doi.org/10.48550/\n",
      "arXiv.2304.09871.\n",
      "\n",
      "Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv Batra, Lucy Vanderwende, Push-\n",
      "meet Kohli, and James Allen. A corpus and evaluation framework for deeper understanding of commonsense\n",
      "stories, 2016. URL https://arxiv.org/abs/1604.01696.\n",
      "\n",
      "31\n",
      "\n",
      "\fKosuke Nishida, Kyosuke Nishida, and Kuniko Saito. Initialization of large language models via reparameteriza-\n",
      "tion to mitigate loss spikes. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, Proceedings\n",
      "of the 2024 Conference on Empirical Methods in Natural Language Processing, EMNLP 2024, Miami, FL,\n",
      "USA, November 12-16, 2024, pages 22699–22714. Association for Computational Linguistics, 2024. URL\n",
      "https://aclanthology.org/2024.emnlp-main.1264.\n",
      "\n",
      "OpenAI. GPT-4 technical report. CoRR, abs/2303.08774, 2023. doi: 10.48550/ARXIV.2303.08774. URL\n",
      "\n",
      "https://doi.org/10.48550/arXiv.2303.08774.\n",
      "\n",
      "Opencsg.\n",
      "\n",
      "·\n",
      "https://huggingface.co/datasets/opencsg/chinese-fineweb-edu.\n",
      "\n",
      "Opencsg/chinese-fineweb-edu\n",
      "\n",
      "Datasets\n",
      "\n",
      "at\n",
      "\n",
      "Hugging\n",
      "\n",
      "Face.\n",
      "\n",
      "Keiran Paster, Marco Dos Santos, Zhangir Azerbayev, and Jimmy Ba. Openwebmath: An open dataset of\n",
      "high-quality mathematical web text. In The Twelfth International Conference on Learning Representations,\n",
      "ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL https://openreview.net/\n",
      "forum?id=jKHmjlpViu.\n",
      "\n",
      "Guilherme Penedo, Hynek Kydlíˇcek, Loubna Ben allal, Anton Lozhkov, Margaret Mitchell, Colin Raffel,\n",
      "Leandro Von Werra, and Thomas Wolf. The FineWeb Datasets: Decanting the Web for the Finest Text Data\n",
      "at Scale, October 2024. URL http://arxiv.org/abs/2406.17557. arXiv:2406.17557.\n",
      "\n",
      "Ofir Press and Lior Wolf. Using the output embedding to improve language models. In Mirella Lapata, Phil\n",
      "Blunsom, and Alexander Koller, editors, Proceedings of the 15th Conference of the European Chapter of the\n",
      "Association for Computational Linguistics, EACL 2017, Valencia, Spain, April 3-7, 2017, Volume 2: Short\n",
      "Papers, pages 157–163. Association for Computational Linguistics, 2017. doi: 10.18653/V1/E17-2025. URL\n",
      "https://doi.org/10.18653/v1/e17-2025.\n",
      "\n",
      "Ivan Provilkov, Dmitrii Emelianenko, and Elena Voita. Bpe-dropout: Simple and effective subword regularization.\n",
      "In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel R. Tetreault, editors, Proceedings of the 58th Annual\n",
      "Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pages 1882–\n",
      "1892. Association for Computational Linguistics, 2020. doi: 10.18653/V1/2020.ACL-MAIN.170. URL\n",
      "https://doi.org/10.18653/v1/2020.acl-main.170.\n",
      "\n",
      "Qwen-Team. Qwen2.5: A party of foundation models, September 2024. URL https://qwenlm.github.io/\n",
      "\n",
      "blog/qwen2.5/.\n",
      "\n",
      "Qwen-Team. Qwq: Reflect deeply on the boundaries of the unknown, November 2024. URL https://qwenlm.\n",
      "\n",
      "github.io/blog/qwq-32b-preview/.\n",
      "\n",
      "Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: memory optimizations toward\n",
      "training trillion parameter models. In Christine Cuicchi, Irene Qualters, and William T. Kramer, editors,\n",
      "Proceedings of the International Conference for High Performance Computing, Networking, Storage and\n",
      "Analysis, SC 2020, Virtual Event / Atlanta, Georgia, USA, November 9-19, 2020, page 20. IEEE/ACM, 2020.\n",
      "doi: 10.1109/SC41405.2020.00024. URL https://doi.org/10.1109/SC41405.2020.00024.\n",
      "\n",
      "Oleg Rybakov, Mike Chrzanowski, Peter Dykas, Jinze Xue, and Ben Lanir. Methods of improving LLM training\n",
      "stability. CoRR, abs/2410.16682, 2024. doi: 10.48550/ARXIV.2410.16682. URL https://doi.org/10.\n",
      "48550/arXiv.2410.16682.\n",
      "\n",
      "Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: an adversarial\n",
      "winograd schema challenge at scale. Commun. ACM, 64(9):99–106, 2021. doi: 10.1145/3474381. URL\n",
      "https://doi.org/10.1145/3474381.\n",
      "\n",
      "David Saxton, Edward Grefenstette, Felix Hill, and Pushmeet Kohli. Analysing mathematical reasoning abilities\n",
      "of neural models. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA,\n",
      "USA, May 6-9, 2019. OpenReview.net, 2019. URL https://openreview.net/forum?id=H1gR5iR5FX.\n",
      "\n",
      "Teven Le Scao, Thomas Wang, Daniel Hesslow, Lucile Saulnier, Stas Bekman, M. Saiful Bari, Stella Biderman,\n",
      "Hady Elsahar, Niklas Muennighoff, Jason Phang, Ofir Press, Colin Raffel, Victor Sanh, Sheng Shen, Lintang\n",
      "Sutawika, Jaesung Tae, Zheng Xin Yong, Julien Launay, and Iz Beltagy. What Language Model to Train\n",
      "if You Have One Million GPU Hours?, November 2022. URL http://arxiv.org/abs/2210.15424.\n",
      "arXiv:2210.15424 [cs].\n",
      "\n",
      "Noam Shazeer. GLU variants improve transformer. CoRR, abs/2002.05202, 2020. URL https://arxiv.org/\n",
      "\n",
      "abs/2002.05202.\n",
      "\n",
      "32\n",
      "\n",
      "\fNoah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: lan-\n",
      "guage agents with verbal reinforcement learning. In Alice Oh, Tristan Naumann, Amir Globerson, Kate\n",
      "Saenko, Moritz Hardt, and Sergey Levine, editors, Advances in Neural Information Processing Systems\n",
      "36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA,\n",
      "USA, December 10 - 16, 2023, 2023. URL http://papers.nips.cc/paper_files/paper/2023/hash/\n",
      "1b44b878bb782e6954cd888628510e90-Abstract-Conference.html.\n",
      "\n",
      "Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro.\n",
      "Megatron-lm: Training multi-billion parameter language models using model parallelism, 2020. URL\n",
      "https://arxiv.org/abs/1909.08053.\n",
      "\n",
      "Damien Sileo. Scaling synthetic logical reasoning datasets with context-sensitive declarative grammars. In Yaser\n",
      "Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, Proceedings of the 2024 Conference on Empirical\n",
      "Methods in Natural Language Processing, EMNLP 2024, Miami, FL, USA, November 12-16, 2024, pages\n",
      "5275–5283. Association for Computational Linguistics, 2024. URL https://aclanthology.org/2024.\n",
      "emnlp-main.301.\n",
      "\n",
      "Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Ben\n",
      "Bogin, Khyathi Raghavi Chandu, Jennifer Dumas, Yanai Elazar, Valentin Hofmann, Ananya Harsh Jha,\n",
      "Sachin Kumar, Li Lucy, Xinxi Lyu, Nathan Lambert, Ian Magnusson, Jacob Morrison, Niklas Muennighoff,\n",
      "Aakanksha Naik, Crystal Nam, Matthew E. Peters, Abhilasha Ravichander, Kyle Richardson, Zejiang\n",
      "Shen, Emma Strubell, Nishant Subramani, Oyvind Tafjord, Pete Walsh, Luke Zettlemoyer, Noah A. Smith,\n",
      "Hannaneh Hajishirzi, Iz Beltagy, Dirk Groeneveld, Jesse Dodge, and Kyle Lo. Dolma: an open corpus\n",
      "of three trillion tokens for language model pretraining research.\n",
      "In Lun-Wei Ku, Andre Martins, and\n",
      "Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational\n",
      "Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pages 15725–\n",
      "15788. Association for Computational Linguistics, 2024. doi: 10.18653/V1/2024.ACL-LONG.840. URL\n",
      "https://doi.org/10.18653/v1/2024.acl-long.840.\n",
      "\n",
      "Yiding Sun, Feng Wang, Yutao Zhu, Wayne Xin Zhao, and Jiaxin Mao. An integrated data processing framework\n",
      "for pretraining foundation models. In Grace Hui Yang, Hongning Wang, Sam Han, Claudia Hauff, Guido\n",
      "Zuccon, and Yi Zhang, editors, Proceedings of the 47th International ACM SIGIR Conference on Research and\n",
      "Development in Information Retrieval, SIGIR 2024, Washington DC, USA, July 14-18, 2024, pages 2713–2718.\n",
      "ACM, 2024. doi: 10.1145/3626772.3657671. URL https://doi.org/10.1145/3626772.3657671.\n",
      "\n",
      "Sho Takase, Shun Kiyono, Sosuke Kobayashi, and Jun Suzuki. Spike no more: Stabilizing the pre-training of\n",
      "large language models. CoRR, abs/2312.16903, 2023. doi: 10.48550/ARXIV.2312.16903. URL https:\n",
      "//doi.org/10.48550/arXiv.2312.16903.\n",
      "\n",
      "Liping Tang, Nikhil Ranjan, Omkar Pangarkar, Xuezhi Liang, Zhen Wang, Li An, Bhaskar Rao, Linghao Jin,\n",
      "Huijuan Wang, Zhoujun Cheng, Suqi Sun, Cun Mu, Victor Miller, Xuezhe Ma, Yue Peng, Zhengzhong Liu,\n",
      "and Eric P. Xing. Txt360: A top-quality llm pre-training dataset requires the perfect blend, 2024a.\n",
      "\n",
      "Tianyi Tang, Hu Yiwen, Bingqian Li, Wenyang Luo, ZiJing Qin, Haoxiang Sun, Jiapeng Wang, Shiyi Xu,\n",
      "Xiaoxue Cheng, Geyang Guo, Han Peng, Bowen Zheng, Yiru Tang, Yingqian Min, Yushuo Chen, Jie\n",
      "Chen, Ranchi Zhao, Luran Ding, Yuhao Wang, Zican Dong, Xia Chunxuan, Junyi Li, Kun Zhou, Xin\n",
      "Zhao, and Ji-Rong Wen. LLMBox: A Comprehensive Library for Large Language Models.\n",
      "In Yixin\n",
      "Cao, Yang Feng, and Deyi Xiong, editors, Proceedings of the 62nd Annual Meeting of the Association\n",
      "for Computational Linguistics (Volume 3: System Demonstrations), pages 388–399, Bangkok, Thailand,\n",
      "2024b. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-demos.37. URL https:\n",
      "//aclanthology.org/2024.acl-demos.37.\n",
      "\n",
      "Zhengyang Tang, Xingxing Zhang, Benyou Wang, and Furu Wei. Mathscale: Scaling instruction tuning\n",
      "for mathematical reasoning. In Forty-first International Conference on Machine Learning, ICML 2024,\n",
      "Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024c. URL https://openreview.net/forum?id=\n",
      "Kjww7ZN47M.\n",
      "\n",
      "Gemma Team. Gemma. 2024. doi: 10.34740/KAGGLE/M/3301. URL https://www.kaggle.com/m/3301.\n",
      "\n",
      "Kushal Tirumala, Daniel Simig, Armen Aghajanyan, and Ari Morcos. D4:\n",
      "\n",
      "improving LLM pretraining\n",
      "via document de-duplication and diversification. In Alice Oh, Tristan Naumann, Amir Globerson, Kate\n",
      "Saenko, Moritz Hardt, and Sergey Levine, editors, Advances in Neural Information Processing Systems\n",
      "36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA,\n",
      "USA, December 10 - 16, 2023, 2023. URL http://papers.nips.cc/paper_files/paper/2023/hash/\n",
      "a8f8cbd7f7a5fb2c837e578c75e5b615-Abstract-Datasets_and_Benchmarks.html.\n",
      "\n",
      "Howe Tissue, Venus Wang, and Lu Wang. Scaling law with learning rate annealing. CoRR, abs/2408.11029,\n",
      "2024. doi: 10.48550/ARXIV.2408.11029. URL https://doi.org/10.48550/arXiv.2408.11029.\n",
      "\n",
      "33\n",
      "\n",
      "\fShubham Toshniwal,\n",
      "\n",
      "Ivan Moshkov, Sean Narenthiran, Daria Gitman, Fei Jia, and Igor Gitman.\n",
      "Openmathinstruct-1: A 1.8 million math instruction tuning dataset. CoRR, abs/2402.10176, 2024. doi:\n",
      "10.48550/ARXIV.2402.10176. URL https://doi.org/10.48550/arXiv.2402.10176.\n",
      "\n",
      "Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Bap-\n",
      "tiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurélien Rodriguez, Armand Joulin, Edouard Grave,\n",
      "and Guillaume Lample. Llama: Open and efficient foundation language models. CoRR, abs/2302.13971,\n",
      "2023. doi: 10.48550/arXiv.2302.13971. URL https://doi.org/10.48550/arXiv.2302.13971.\n",
      "\n",
      "Dixuan Wang, Yanda Li, Junyuan Jiang, Zepeng Ding, Guochao Jiang, Jiaqing Liang, and Deqing Yang.\n",
      "Tokenization matters! degrading large language models through challenging their tokenization. CoRR,\n",
      "abs/2405.17067, 2024a. doi: 10.48550/ARXIV.2405.17067. URL https://doi.org/10.48550/arXiv.\n",
      "2405.17067.\n",
      "\n",
      "Ke Wang, Houxing Ren, Aojun Zhou, Zimu Lu, Sichun Luo, Weikang Shi, Renrui Zhang, Linqi Song, Mingjie\n",
      "Zhan, and Hongsheng Li. Mathcoder: Seamless code integration in llms for enhanced mathematical reasoning.\n",
      "In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11,\n",
      "2024. OpenReview.net, 2024b. URL https://openreview.net/forum?id=z8TW0ttBPp.\n",
      "\n",
      "Yejie Wang, Keqing He, Dayuan Fu, Zhuoma Gongque, Heyang Xu, Yanxu Chen, Zhexu Wang, Yujia Fu,\n",
      "Guanting Dong, Muxi Diao, Jingang Wang, Mengdi Zhang, Xunliang Cai, and Weiran Xu. How do your code\n",
      "llms perform? empowering code instruction tuning with really good data. In Yaser Al-Onaizan, Mohit Bansal,\n",
      "and Yun-Nung Chen, editors, Proceedings of the 2024 Conference on Empirical Methods in Natural Language\n",
      "Processing, EMNLP 2024, Miami, FL, USA, November 12-16, 2024, pages 14027–14043. Association for\n",
      "Computational Linguistics, 2024c. URL https://aclanthology.org/2024.emnlp-main.777.\n",
      "\n",
      "Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le,\n",
      "and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. In Sanmi Koyejo,\n",
      "S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information\n",
      "Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022,\n",
      "New Orleans, LA, USA, November 28 - December 9, 2022, 2022. URL http://papers.nips.cc/paper_\n",
      "files/paper/2022/hash/9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html.\n",
      "\n",
      "Yuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, and Lingming Zhang. Magicoder: Empowering code\n",
      "generation with oss-instruct. In Forty-first International Conference on Machine Learning, ICML 2024,\n",
      "Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=\n",
      "XUeoOBid3x.\n",
      "\n",
      "Mitchell Wortsman, Peter J. Liu, Lechao Xiao, Katie E. Everett, Alexander A. Alemi, Ben Adlam, John D.\n",
      "Co-Reyes, Izzeddin Gur, Abhishek Kumar, Roman Novak, Jeffrey Pennington, Jascha Sohl-Dickstein, Kelvin\n",
      "Xu, Jaehoon Lee, Justin Gilmer, and Simon Kornblith. Small-scale proxies for large-scale transformer\n",
      "training instabilities. In The Twelfth International Conference on Learning Representations, ICLR 2024,\n",
      "Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=\n",
      "d8w0pmvXbZ.\n",
      "\n",
      "Yuhuai Wu, Markus N. Rabe, Wenda Li, Jimmy Ba, Roger B. Grosse, and Christian Szegedy. LIME: learning\n",
      "inductive bias for primitives of mathematical reasoning. In Marina Meila and Tong Zhang, editors, Proceedings\n",
      "of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event,\n",
      "volume 139 of Proceedings of Machine Learning Research, pages 11251–11262. PMLR, 2021. URL\n",
      "http://proceedings.mlr.press/v139/wu21c.html.\n",
      "\n",
      "Zijian Wu, Jiayu Wang, Dahua Lin, and Kai Chen. Lean-github: Compiling github LEAN repositories\n",
      "for a versatile LEAN prover. CoRR, abs/2407.17227, 2024. doi: 10.48550/ARXIV.2407.17227. URL\n",
      "https://doi.org/10.48550/arXiv.2407.17227.\n",
      "\n",
      "Mengzhou Xia, Sadhika Malladi, Suchin Gururangan, Sanjeev Arora, and Danqi Chen. LESS: selecting\n",
      "influential data for targeted instruction tuning. In Forty-first International Conference on Machine Learning,\n",
      "ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024. URL https://openreview.net/\n",
      "forum?id=PG5fV50maR.\n",
      "\n",
      "Huajian Xin, Daya Guo, Zhihong Shao, Zhizhou Ren, Qihao Zhu, Bo Liu, Chong Ruan, Wenda Li, and Xiaodan\n",
      "Liang. Deepseek-prover: Advancing theorem proving in llms through large-scale synthetic data. CoRR,\n",
      "abs/2405.14333, 2024. doi: 10.48550/ARXIV.2405.14333. URL https://doi.org/10.48550/arXiv.\n",
      "2405.14333.\n",
      "\n",
      "34\n",
      "\n",
      "\fRuibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan,\n",
      "Liwei Wang, and Tie-Yan Liu. On layer normalization in the transformer architecture. In Proceedings\n",
      "of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event,\n",
      "volume 119 of Proceedings of Machine Learning Research, pages 10524–10533. PMLR, 2020. URL\n",
      "http://proceedings.mlr.press/v119/xiong20b.html.\n",
      "\n",
      "Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui Hou, Louis Martin, Rashi\n",
      "Rungta, Karthik Abinav Sankararaman, Barlas Oguz, Madian Khabsa, Han Fang, Yashar Mehdad, Sharan\n",
      "Narang, Kshitiz Malik, Angela Fan, Shruti Bhosale, Sergey Edunov, Mike Lewis, Sinong Wang, and Hao\n",
      "Ma. Effective long-context scaling of foundation models. In Kevin Duh, Helena Gómez-Adorno, and Steven\n",
      "Bethard, editors, Proceedings of the 2024 Conference of the North American Chapter of the Association for\n",
      "Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), NAACL 2024, Mexico\n",
      "City, Mexico, June 16-21, 2024, pages 4643–4663. Association for Computational Linguistics, 2024. doi: 10.\n",
      "18653/V1/2024.NAACL-LONG.260. URL https://doi.org/10.18653/v1/2024.naacl-long.260.\n",
      "\n",
      "Zhangchen Xu, Fengqing Jiang, Luyao Niu, Yuntian Deng, Radha Poovendran, Yejin Choi, and Bill Yuchen\n",
      "Lin. Magpie: Alignment data synthesis from scratch by prompting aligned llms with nothing. CoRR,\n",
      "abs/2406.08464, 2024. doi: 10.48550/ARXIV.2406.08464. URL https://doi.org/10.48550/arXiv.\n",
      "2406.08464.\n",
      "\n",
      "Vikas Yadav, Steven Bethard, and Mihai Surdeanu. Quick and (not so) dirty: Unsupervised selection of\n",
      "justification sentences for multi-hop question answering. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun\n",
      "Wan, editors, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\n",
      "and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong\n",
      "Kong, China, November 3-7, 2019, pages 2578–2589. Association for Computational Linguistics, 2019. doi:\n",
      "10.18653/V1/D19-1260. URL https://doi.org/10.18653/v1/D19-1260.\n",
      "\n",
      "Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Ce Bian, Chao Yin, Chenxu Lv, Da Pan, Dian\n",
      "Wang, Dong Yan, Fan Yang, Fei Deng, Feng Wang, Feng Liu, Guangwei Ai, Guosheng Dong, Haizhou\n",
      "Zhao, Hang Xu, Haoze Sun, Hongda Zhang, Hui Liu, Jiaming Ji, Jian Xie, Juntao Dai, Kun Fang, Lei\n",
      "Su, Liang Song, Lifeng Liu, Liyun Ru, Luyao Ma, Mang Wang, Mickel Liu, MingAn Lin, Nuolan Nie,\n",
      "Peidong Guo, Ruiyang Sun, Tao Zhang, Tianpeng Li, Tianyu Li, Wei Cheng, Weipeng Chen, Xiangrong\n",
      "Zeng, Xiaochuan Wang, Xiaoxi Chen, Xin Men, Xin Yu, Xuehai Pan, Yanjun Shen, Yiding Wang, Yiyu\n",
      "Li, Youxin Jiang, Yuchen Gao, Yupeng Zhang, Zenan Zhou, and Zhiying Wu. Baichuan 2: Open large-\n",
      "scale language models. CoRR, abs/2309.10305, 2023. doi: 10.48550/ARXIV.2309.10305. URL https:\n",
      "//doi.org/10.48550/arXiv.2309.10305.\n",
      "\n",
      "An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li,\n",
      "Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang,\n",
      "Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jianxin Yang, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang\n",
      "Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng\n",
      "Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao\n",
      "Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei,\n",
      "Xuancheng Ren, Xuejing Liu, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu\n",
      "Cui, Zhenru Zhang, Zhifang Guo, and Zhihao Fan. Qwen2 technical report. CoRR, abs/2407.10671, 2024a.\n",
      "doi: 10.48550/ARXIV.2407.10671. URL https://doi.org/10.48550/arXiv.2407.10671.\n",
      "\n",
      "An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu,\n",
      "Jingren Zhou, Junyang Lin, Keming Lu, Mingfeng Xue, Runji Lin, Tianyu Liu, Xingzhang Ren, and Zhenru\n",
      "Zhang. Qwen2.5-math technical report: Toward mathematical expert model via self-improvement. CoRR,\n",
      "abs/2409.12122, 2024b. doi: 10.48550/ARXIV.2409.12122. URL https://doi.org/10.48550/arXiv.\n",
      "2409.12122.\n",
      "\n",
      "Greg Yang, Edward J. Hu, Igor Babuschkin, Szymon Sidor, Xiaodong Liu, David Farhi, Nick Ryder, Jakub\n",
      "Pachocki, Weizhu Chen, and Jianfeng Gao. Tensor programs V: tuning large neural networks via zero-\n",
      "shot hyperparameter transfer. CoRR, abs/2203.03466, 2022. doi: 10.48550/ARXIV.2203.03466. URL\n",
      "https://doi.org/10.48550/arXiv.2203.03466.\n",
      "\n",
      "Greg Yang, Dingli Yu, Chen Zhu, and Soufiane Hayou. Tensor programs VI: feature learning in infinite\n",
      "depth neural networks. In The Twelfth International Conference on Learning Representations, ICLR 2024,\n",
      "Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024c. URL https://openreview.net/forum?id=\n",
      "17pVDnpwwl.\n",
      "\n",
      "Huaiyuan Ying, Zijian Wu, Yihan Geng, Jiayu Wang, Dahua Lin, and Kai Chen. Lean workbook: A large-scale\n",
      "lean problem set formalized from natural language math problems. CoRR, abs/2406.03847, 2024. doi:\n",
      "10.48550/ARXIV.2406.03847. URL https://doi.org/10.48550/arXiv.2406.03847.\n",
      "\n",
      "35\n",
      "\n",
      "\fAndy B. Yoo, Morris A. Jette, and Mark Grondona. SLURM: simple linux utility for resource management. In\n",
      "Dror G. Feitelson, Larry Rudolph, and Uwe Schwiegelshohn, editors, Job Scheduling Strategies for Parallel\n",
      "Processing, 9th International Workshop, JSSPP 2003, Seattle, WA, USA, June 24, 2003, Revised Papers,\n",
      "volume 2862 of Lecture Notes in Computer Science, pages 44–60. Springer, 2003. doi: 10.1007/10968987\\_3.\n",
      "URL https://doi.org/10.1007/10968987_3.\n",
      "\n",
      "Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T. Kwok, Zhenguo Li,\n",
      "Adrian Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematical questions for large language\n",
      "models. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria,\n",
      "May 7-11, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=N8N0hgNDRt.\n",
      "\n",
      "Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mammoth:\n",
      "Building math generalist models through hybrid instruction tuning. In The Twelfth International Conference\n",
      "on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL\n",
      "https://openreview.net/forum?id=yLClGs770I.\n",
      "\n",
      "Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really\n",
      "finish your sentence? In Anna Korhonen, David R. Traum, and Lluís Màrquez, editors, Proceedings of the\n",
      "57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August\n",
      "2, 2019, Volume 1: Long Papers, pages 4791–4800. Association for Computational Linguistics, 2019. doi:\n",
      "10.18653/V1/P19-1472. URL https://doi.org/10.18653/v1/p19-1472.\n",
      "\n",
      "Biao Zhang and Rico Sennrich.\n",
      "\n",
      "Root mean square layer normalization.\n",
      "\n",
      "In Hanna M. Wal-\n",
      "lach, Hugo Larochelle, Alina Beygelzimer, Florence d’Alché-Buc, Emily B. Fox, and Roman Gar-\n",
      "nett, editors, Advances in Neural Information Processing Systems 32: Annual Conference on Neu-\n",
      "ral Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC,\n",
      "Canada, pages 12360–12371, 2019. URL https://proceedings.neurips.cc/paper/2019/hash/\n",
      "1e8a19426224ca89e83cef47f1e7f53b-Abstract.html.\n",
      "\n",
      "Ge Zhang, Scott Qu, Jiaheng Liu, Chenchen Zhang, Chenghua Lin, Chou Leuang Yu, Danny Pan, Esther Cheng,\n",
      "Jie Liu, Qunshu Lin, Raven Yuan, Tuney Zheng, Wei Pang, Xinrun Du, Yiming Liang, Yinghao Ma, Yizhi Li,\n",
      "Ziyang Ma, Bill Y. Lin, Emmanouil Benetos, Huan Yang, Junting Zhou, Kaijing Ma, Minghao Liu, Morry Niu,\n",
      "Noah Wang, Quehry Que, Ruibo Liu, Sine Liu, Shawn Guo, Soren Gao, Wangchunshu Zhou, Xinyue Zhang,\n",
      "Yizhi Zhou, Yubo Wang, Yuelin Bai, Yuhan Zhang, Yuxiang Zhang, Zenith Wang, Zhenzhu Yang, Zijian Zhao,\n",
      "Jiajun Zhang, Wanli Ouyang, Wenhao Huang, and Wenhu Chen. Map-neo: Highly capable and transparent\n",
      "bilingual large language model series. CoRR, abs/2405.19327, 2024a. doi: 10.48550/ARXIV.2405.19327.\n",
      "URL https://doi.org/10.48550/arXiv.2405.19327.\n",
      "\n",
      "Yifan Zhang, Yifan Luo, Yang Yuan, and Andrew Chi-Chih Yao. Automathtext: Autonomous data selection with\n",
      "language models for mathematical texts. CoRR, abs/2402.07625, 2024b. doi: 10.48550/ARXIV.2402.07625.\n",
      "URL https://doi.org/10.48550/arXiv.2402.07625.\n",
      "\n",
      "Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen\n",
      "Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang,\n",
      "Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. A survey of\n",
      "large language models. CoRR, abs/2303.18223, 2023. doi: 10.48550/ARXIV.2303.18223. URL https:\n",
      "//doi.org/10.48550/arXiv.2303.18223.\n",
      "\n",
      "Tianyu Zheng, Ge Zhang, Tianhao Shen, Xueling Liu, Bill Yuchen Lin, Jie Fu, Wenhu Chen, and Xiang\n",
      "Yue. Opencodeinterpreter: Integrating code generation with execution and refinement. In Lun-Wei Ku,\n",
      "Andre Martins, and Vivek Srikumar, editors, Findings of the Association for Computational Linguistics, ACL\n",
      "2024, Bangkok, Thailand and virtual meeting, August 11-16, 2024, pages 12834–12859. Association for\n",
      "Computational Linguistics, 2024. doi: 10.18653/V1/2024.FINDINGS-ACL.762. URL https://doi.org/\n",
      "10.18653/v1/2024.findings-acl.762.\n",
      "\n",
      "Fan Zhou, Zengzhi Wang, Qian Liu, Junlong Li, and Pengfei Liu. Programming every example: Lifting pre-\n",
      "training data quality like experts at scale. CoRR, abs/2409.17115, 2024a. doi: 10.48550/ARXIV.2409.17115.\n",
      "URL https://doi.org/10.48550/arXiv.2409.17115.\n",
      "\n",
      "Kun Zhou, Beichen Zhang, Jiapeng Wang, Zhipeng Chen, Wayne Xin Zhao, Jing Sha, Zhichao Sheng, Shijin\n",
      "Wang, and Ji-Rong Wen. Jiuzhang3.0: Efficiently improving mathematical reasoning by training small\n",
      "data synthesis models. CoRR, abs/2405.14365, 2024b. doi: 10.48550/ARXIV.2405.14365. URL https:\n",
      "//doi.org/10.48550/arXiv.2405.14365.\n",
      "\n",
      "Yutao Zhu, Kun Zhou, Kelong Mao, Wentong Chen, Yiding Sun, Zhipeng Chen, Qian Cao, Yihan Wu, Yushuo\n",
      "Chen, Feng Wang, Lei Zhang, Junyi Li, Xiaolei Wang, Lei Wang, Beichen Zhang, Zican Dong, Xiaoxue\n",
      "Cheng, Yuhan Chen, Xinyu Tang, Yupeng Hou, Qiangqiang Ren, Xincheng Pang, Shufang Xie, Wayne Xin\n",
      "\n",
      "36\n",
      "\n",
      "\fZhao, Zhicheng Dou, Jiaxin Mao, Yankai Lin, Ruihua Song, Jun Xu, Xu Chen, Rui Yan, Zhewei Wei,\n",
      "Di Hu, Wenbing Huang, Ze-Feng Gao, Yueguo Chen, Weizheng Lu, and Ji-Rong Wen. Yulan: An open-\n",
      "source large language model. CoRR, abs/2406.19853, 2024. doi: 10.48550/ARXIV.2406.19853. URL\n",
      "https://doi.org/10.48550/arXiv.2406.19853.\n",
      "\n",
      "Barret Zoph, Irwan Bello, Sameer Kumar, Nan Du, Yanping Huang, Jeff Dean, Noam Shazeer, and William\n",
      "Fedus. Designing effective sparse expert models. CoRR, abs/2202.08906, 2022. URL https://arxiv.\n",
      "org/abs/2202.08906.\n",
      "\n",
      "37\n",
      "\n",
      "\fA Definition of Variables\n",
      "\n",
      "We list the detailed definition of all the used variables from Section 3 in Table 8.\n",
      "\n",
      "Table 8: Definition of the variables for computing the hyperparameters.\n",
      "\n",
      "Variables Meaning\n",
      "\n",
      "nlayers\n",
      "nheads\n",
      "nkv_heads\n",
      "dmodel\n",
      "dhead\n",
      "dffn\n",
      "σbase\n",
      "ηbase\n",
      "\n",
      "The num of model’s layers\n",
      "\n",
      "The num of model’s attention heads\n",
      "\n",
      "The num of model’s kv-heads used in GQA\n",
      "\n",
      "Model dimension, i.e., hidden size\n",
      "\n",
      "Dimension of attention head\n",
      "\n",
      "The hidden size of feed-forward network\n",
      "\n",
      "Initialization standard deviation for each matrix\n",
      "\n",
      "Learning rate, i.e., max learning rate\n",
      "\n",
      "dmodel_proxy\n",
      "mwidth\n",
      "\n",
      "dmodel for proxy model, i.e., the 0.05B model\n",
      "Width scaling factor in µP, i.e., dmodel/dmodel_proxy\n",
      "\n",
      "B Training Stability\n",
      "\n",
      "B.1 Experiment Setup\n",
      "\n",
      "We employ a relatively large learning rate with the intention of revealing the instability within the\n",
      "model. Unless otherwise specified, a trial learning rate of 0.01 is adopted. We sample 20B tokens\n",
      "from our pre-training dataset, ensuring consistency with the source used in the final training.\n",
      "\n",
      "The detailed architecture settings are listed in Table 9. The 0.2B proxy model is utilized for general\n",
      "exploratory experiments, whereas the 0.05B and 0.4B model are employed for µP experiments. The\n",
      "latter proxy model has the same number of layers as the final model.\n",
      "\n",
      "Table 9: Small proxy models used to explore the training dynamics.\n",
      "\n",
      "Model\n",
      "\n",
      "YuLan-Mini\n",
      "\n",
      "Proxy model (0.05B)\n",
      "Proxy model (0.2B)\n",
      "Proxy model (0.4B)\n",
      "\n",
      "LR\n",
      "\n",
      "0.01\n",
      "\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "\n",
      "nlayers\n",
      "\n",
      "dmodel\n",
      "\n",
      "dffn\n",
      "\n",
      "nheads\n",
      "\n",
      "nkv_heads\n",
      "\n",
      "56\n",
      "\n",
      "32\n",
      "30\n",
      "56\n",
      "\n",
      "1920\n",
      "\n",
      "256\n",
      "576\n",
      "576\n",
      "\n",
      "4800\n",
      "\n",
      "640\n",
      "1536\n",
      "1536\n",
      "\n",
      "30\n",
      "\n",
      "2\n",
      "9\n",
      "9\n",
      "\n",
      "6\n",
      "\n",
      "2\n",
      "3\n",
      "3\n",
      "\n",
      "C Prompts for Generating Synthetic Data\n",
      "\n",
      "C.1 Math\n",
      "\n",
      "Prompt for Mathematical Documents Synthesis A\n",
      "\n",
      "## Instruction\n",
      "Please gain inspiration from the following content to create a lecture script for a college-level mathematics course.\n",
      "\n",
      "## Content\n",
      "{Content Placeholder}\n",
      "Begin without using titles or introductions.\n",
      "\n",
      "38\n",
      "\n",
      "\fPrompt for Mathematical Documents Synthesis B\n",
      "\n",
      "Write an educational piece suited for college students related to the following text snippet:\n",
      "{Content Placeholder}\n",
      "\n",
      "Do not just list concepts, but develop each one in detail before moving to the next, as we prioritize depth of understand-\n",
      "ing and comprehensive exploration of the subject matter over breadth. Focus on:\n",
      "\n",
      "- Rigor: Ensure in-depth coverage of the concepts/sections.\n",
      "- Engagement: Write with an academic, professional and engaging tone that captivates interest.\n",
      "- Application: Incorporate specific, practical examples, such as proofs in calculus or critical dates and figures in history.\n",
      "\n",
      "Do not include a title or an introduction, simply write the content without headlines and introductory phrases.\n",
      "\n",
      "Prompt for Mathematical Documents Synthesis C\n",
      "\n",
      "Write an educational piece suited for middle school students related to the following text snippet:\n",
      "{Content Placeholder}\n",
      "\n",
      "Do not just list concepts, but develop each one in detail before moving to the next, as we prioritize depth of understand-\n",
      "ing and comprehensive exploration of the subject matter over breadth. Focus on:\n",
      "\n",
      "- Rigor: Ensure in-depth coverage of the concepts/sections.\n",
      "- Engagement: Write with an academic, professional and engaging tone that captivates interest.\n",
      "- Application: Incorporate specific, practical examples, such as proofs in calculus or critical dates and figures in history.\n",
      "\n",
      "Do not include a title or an introduction, simply write the content without headlines and introductory phrases.\n",
      "\n",
      "Prompt for Mathematical Documents Synthesis D\n",
      "\n",
      "## Instruction\n",
      "Please gain inspiration from the following content to draft a mathematics textbook chapter suitable for college students:\n",
      "\n",
      "## Content\n",
      "{Content Placeholder}\n",
      "\n",
      "Write in a clear, structured manner that is easy for students to follow and understand.\n",
      "\n",
      "Prompt for Mathematical Documents Synthesis E\n",
      "\n",
      "## Instruction\n",
      "Please gain inspiration from the following content to draft a mathematics textbook chapter suitable for middle school students:\n",
      "\n",
      "## Content\n",
      "{Content Placeholder}\n",
      "\n",
      "Write in a clear, structured manner that is easy for students to follow and understand.\n",
      "\n",
      "Prompt for Mathematical Documents Synthesis F\n",
      "\n",
      "## Instruction\n",
      "Please gain inspiration from the following content to design a problem set with solutions.\n",
      "\n",
      "## Content\n",
      "{Content Placeholder}\n",
      "\n",
      "## Guidelines\n",
      "- Formulate a series of problems that test understanding and application of the concepts.\n",
      "- Provide detailed solutions for each problem, explaining the reasoning and calculations involved.\n",
      "- Vary the difficulty of the problems to cater to students with different levels of proficiency.\n",
      "\n",
      "The problem set should challenge students to apply their knowledge in practical scenarios.\n",
      "\n",
      "39\n",
      "\n",
      "\fPrompt for Mathematical Instruction Synthesis\n",
      "\n",
      "You are exceptionally skilled at crafting high-quality math problems and offering precise solutions.\n",
      "\n",
      "Please gain inspiration from the following random math content to create a high-quality math problem and solve it step\n",
      "by step with clear logic. Present your output in two distinct sections:\n",
      "\n",
      "[Problem Description] and [Solution]\n",
      "\n",
      "Math content for inspiration:\n",
      "\n",
      "{Seed Content Placeholder}\n",
      "\n",
      "Guidelines for each section:\n",
      "\n",
      "1. [Problem Description]: This should be **completely self-contained**, providing all the contextual information one needs to\n",
      "understand and solve the problem.\n",
      "2. [Solution]: Offer a comprehensive, **correct** solution that accurately addresses the [Problem Description] you provided step by\n",
      "step with clear logic. Please ensure that the Solution only involves answering the Problem, **without addressing the requirements I\n",
      "provided.**\n",
      "\n",
      "C.2 Code\n",
      "\n",
      "Prompt for Code Instruction Synthesis A\n",
      "\n",
      "You are a teaching assistant helping to create a Python programming task from a given code snippet. You must provide the best\n",
      "response to the Python programming task, including reasoning thought and reference solutions.\n",
      "\n",
      "[Code Snippet]\n",
      "{Code Placeholder}\n",
      "Your response must have these parts:\n",
      "\n",
      "[Task]\n",
      "{Create an independent and detailed Python programming task}\n",
      "\n",
      "[Analysis]\n",
      "{Analyze the task and reason about the given task step by step}\n",
      "\n",
      "[Solution]\n",
      "{Write a high-quality reference solution in a self-contained script that solves the task}\n",
      "\n",
      "Prompt for Code Instruction Synthesis B\n",
      "\n",
      "You are exceptionally skilled at crafting high-quality Python programming problems and offering precise solutions.\n",
      "\n",
      "Please gain inspiration from the following random code snippet to create a high-quality programming problem. Present your output in\n",
      "two distinct sections:\n",
      "\n",
      "[Problem Description] and [Solution]\n",
      "\n",
      "Code snippet for inspiration:\n",
      "{Code Placeholder}\n",
      "\n",
      "Guidelines for each section:\n",
      "1. [Problem Description]: This should be **completely self-contained**, providing all the contextual information one needs to\n",
      "understand and solve the problem. Assume common programming knowledge, but ensure that any specific context, variables, or code\n",
      "snippets pertinent to this problem are explicitly included.\n",
      "2. [Solution]: Offer a comprehensive, **correct** solution that accurately addresses the [Problem Description] you provided. Please\n",
      "ensure that the Solution only involves answering the Problem, **without addressing the requirements I provided!**\n",
      "\n",
      "40\n",
      "\n",
      "\fC.3 Science\n",
      "\n",
      "Prompt for Scientific QA Synthesis\n",
      "\n",
      "Instruction\n",
      "Please gain inspiration from the following {Discipline Placeholder} content to create a high-quality {Discipline Placeholder} problem\n",
      "and solution. Present your output in two distinct sections: [Problem] and [Solution].\n",
      "\n",
      "{Discipline Placeholder} Content\n",
      "{Seed Snippet Placeholder}\n",
      "\n",
      "Guidelines\n",
      "[Problem]: This should be **completely self-contained**, providing all the contextual information one needs to understand and solve\n",
      "the problem.\n",
      "\n",
      "[Solution]: Present a comprehensive, step-by-step solution that solves the problem **correctly** and educates the student, around\n",
      "250-350 words long. Clearly articulate the reasoning and methods used at each step, providing insight into the problem-solving\n",
      "process. Take care to format any equations properly using LaTeX or appropriate notation.\n",
      "\n",
      "Prompt for Topic Labeling\n",
      "\n",
      "I am categorizing a series of articles according to the following 11 topics. Next, I will give you an article, please select only one topic\n",
      "that the article is the most related to:\n",
      "\n",
      "[Topics]: {Topic List Placeholder}\n",
      "\n",
      "[Article]: {Web Page Content Placeholder}\n",
      "\n",
      "Please only return the most related topic:\n",
      "\n",
      "C.4 Data Selection\n",
      "\n",
      "Prompt for Instruction Tag Labeling\n",
      "\n",
      "Please identify the relevant tags representing the user’s intentions in the following Problem and Solution. Focus on the reasoning\n",
      "behind the solution. Please ONLY respond with tags in a Python list.\n",
      "\n",
      "Problem:\n",
      "{Question Placeholder}\n",
      "Solution:\n",
      "{Answer Placeholder}\n",
      "\n",
      "D Open-Source Datasets Used during Pre-training\n",
      "\n",
      "Table 10: Comprehensive list of all open-source datasets used. For datasets that are only available via\n",
      "links, we also offer additional guidance on our project website https://github.com/RUC-GSAI/\n",
      "YuLan-Mini.\n",
      "\n",
      "Domain\n",
      "\n",
      "Dataset\n",
      "\n",
      "General\n",
      "\n",
      "Code\n",
      "\n",
      "chinese-fineweb-edu [Opencsg]\n",
      "llm360-txt360 [Tang et al., 2024a]\n",
      "wanjuan [He et al., 2023]\n",
      "dclm [Li et al., 2024b]\n",
      "fineweb-edu [Penedo et al., 2024]\n",
      "dolma [Soldaini et al., 2024]\n",
      "tulu3 [Lambert et al., 2024]\n",
      "magpie-reasoning-150k [Xu et al., 2024]\n",
      "\n",
      "the-stack-v2 [Lozhkov et al., 2024b]\n",
      "starcoderdata [Li et al., 2023]\n",
      "opencoder-llm [Huang et al., 2024]\n",
      "longwanjuan-github [Liu et al., 2024b]\n",
      "mathcodeinstruct [Wang et al., 2024b]\n",
      "codefeedback-filtered-instruction [Zheng et al., 2024]\n",
      "xcoder-80k [Wang et al., 2024c]\n",
      "\n",
      "41\n",
      "\n",
      "\fMath\n",
      "\n",
      "proof-pile-2 [Azerbayev et al., 2024]\n",
      "automathtext [Zhang et al., 2024b]\n",
      "open-web-math-pro [Zhou et al., 2024a]\n",
      "deepmind-math [Saxton et al., 2019]\n",
      "orca-math [Mitra et al., 2024b]\n",
      "metamathqa [Yu et al., 2024]\n",
      "numina [Beeching et al., 2024]\n",
      "silmorca [Lian et al., 2023]\n",
      "scalequest-math [Ding et al., 2024b]\n",
      "infimm-webmath-40b [Han et al., 2024]\n",
      "lean-star [Lin et al., 2024]\n",
      "lean-github [Wu et al., 2024]\n",
      "lean-workbook [Ying et al., 2024]\n",
      "lean-deepseek-v1 [Xin et al., 2024]\n",
      "ape210k [Kadlcík et al., 2023]\n",
      "mathinstruct [Yue et al., 2024]\n",
      "openmathinstruct-1 [Toshniwal et al., 2024]\n",
      "mathscaleqa-2m [Tang et al., 2024c]\n",
      "orca-agentinstruct [Mitra et al., 2024a]\n",
      "fol-nli [Sileo, 2024]\n",
      "gretel-math-gsm8k-v1 [AI, 2024]\n",
      "\n",
      "E Detailed Data Composition by Training Phases\n",
      "\n",
      "The specific composition of data for each course stage is shown in Table 12, where black represents\n",
      "English web pages and general content, green represents Chinese, blue represents code, and red\n",
      "represents mathematics. The first 10 billion tokens of Phase 1 are used during the warm-up stage.\n",
      "The next 30 billion tokens of Phase 1, along with Phases 2 through 25, are employed in the stable\n",
      "training stage. Phases 26 and 27 constitute the annealing stage.\n",
      "\n",
      "Table 12: Detailed data composition by training curriculum phases.\n",
      "\n",
      "Phase\n",
      "\n",
      "Data composition by phase (in billions of tokens)\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "dclm (1.80), fineweb-edu (16.20), english-books (1.60),\n",
      "pes2o (0.80), arxiv (1.20), wikipedia (0.40), dolma (1.24),\n",
      "cicg-news (0.76), cn-baike (0.39), mnbvc-news (0.08), cn-book (0.24),\n",
      "cn-legal-case-law (0.36), zhihu-qa (0.12), the-stack-v2 (4.91),\n",
      "starcoder (2.92), smollm-python (0.20), proof-pile-2 (1.52),\n",
      "automathtext (1.12), open-web-math-pro (0.20), cosmopedia (1.01),\n",
      "mathtext (0.12)\n",
      "\n",
      "dclm (1.80), fineweb-edu (16.20), english-books (1.60),\n",
      "pes2o (0.80), arxiv (1.20), wikipedia (0.40), dolma (1.24),\n",
      "cicg-news (0.76), cn-baike (0.39), mnbvc-news (0.08), cn-book (0.24),\n",
      "cn-legal-case-law (0.36), zhihu-qa (0.12), the-stack-v2 (4.90),\n",
      "starcoder (2.92), smollm-python (0.20), proof-pile-2 (1.52),\n",
      "automathtext (1.12), open-web-math-pro (0.20), cosmopedia (1.02),\n",
      "mathtext (0.12)\n",
      "\n",
      "dclm (1.80), fineweb-edu (16.20), english-books (1.60),\n",
      "pes2o (0.80), arxiv (1.20), wikipedia (0.40), dolma (1.24),\n",
      "cicg-news (0.76), cn-baike (0.39), mnbvc-news (0.08), cn-book (0.24),\n",
      "cn-legal-case-law (0.36), zhihu-qa (0.12), the-stack-v2 (4.86),\n",
      "starcoder (2.92), smollm-python (0.20), proof-pile-2 (1.52),\n",
      "automathtext (1.12), open-web-math-pro (0.20), cosmopedia (1.02),\n",
      "mathtext (0.12)\n",
      "\n",
      "42\n",
      "\n",
      "\f4\n",
      "\n",
      "5\n",
      "\n",
      "6\n",
      "\n",
      "7\n",
      "\n",
      "8\n",
      "\n",
      "9\n",
      "\n",
      "10\n",
      "\n",
      "dclm (1.80), fineweb-edu (16.20), english-books (1.60),\n",
      "pes2o (0.80), arxiv (1.20), wikipedia (0.40), dolma (1.24),\n",
      "cicg-news (0.76), cn-baike (0.39), mnbvc-news (0.08), cn-book (0.24),\n",
      "cn-legal-case-law (0.36), zhihu-qa (0.12), the-stack-v2 (4.14),\n",
      "starcoder (2.92), smollm-python (0.20), proof-pile-2 (1.52),\n",
      "automathtext (1.12), open-web-math-pro (0.24), cosmopedia (0.98),\n",
      "mathtext (0.12)\n",
      "\n",
      "dclm (1.80), fineweb-edu (16.20), english-books (1.60),\n",
      "pes2o (0.80), arxiv (1.20), wikipedia (0.40), dolma (1.24),\n",
      "cicg-news (0.76), cn-baike (0.39), mnbvc-news (0.08), cn-book (0.24),\n",
      "cn-legal-case-law (0.36), zhihu-qa (0.12), the-stack-v2 (4.90),\n",
      "starcoder (2.92), smollm-python (0.20), proof-pile-2 (1.54),\n",
      "automathtext (1.14), open-web-math-pro (0.24), cosmopedia (0.94),\n",
      "mathtext (0.12)\n",
      "\n",
      "dclm (1.80), fineweb-edu (16.20), english-books (1.60),\n",
      "pes2o (0.80), arxiv (1.20), wikipedia (0.40), dolma (1.24),\n",
      "cicg-news (0.76), cn-baike (0.39), mnbvc-news (0.08), cn-book (0.24),\n",
      "cn-legal-case-law (0.36), zhihu-qa (0.12), the-stack-v2 (4.90),\n",
      "starcoder (2.92), smollm-python (0.20), proof-pile-2 (1.54),\n",
      "automathtext (1.16), open-web-math-pro (0.26), cosmopedia (0.82),\n",
      "mathtext (0.12), metamathqa (0.02), orca-math (0.02),\n",
      "yulan-mini-syn-math-inst (0.04)\n",
      "\n",
      "dclm (1.80), fineweb-edu (16.20), english-books (1.60),\n",
      "pes2o (0.80), arxiv (1.20), wikipedia (0.40), dolma (1.24),\n",
      "cicg-news (0.76), cn-baike (0.39), mnbvc-news (0.08), cn-book (0.24),\n",
      "cn-legal-case-law (0.36), zhihu-qa (0.12), the-stack-v2 (4.90),\n",
      "starcoder (2.92), smollm-python (0.20), proof-pile-2 (1.60),\n",
      "automathtext (1.17), open-web-math-pro (0.28), cosmopedia (0.77),\n",
      "mathtext (0.12), metamathqa (0.01), orca-math (0.01),\n",
      "yulan-mini-syn-math-inst (0.02)\n",
      "\n",
      "dclm (1.80), fineweb-edu (16.20), english-books (1.60),\n",
      "pes2o (0.80), arxiv (1.20), wikipedia (0.40), dolma (1.24),\n",
      "cicg-news (0.76), cn-baike (0.39), mnbvc-news (0.08), cn-book (0.24),\n",
      "cn-legal-case-law (0.36), zhihu-qa (0.12), the-stack-v2 (4.90),\n",
      "starcoder (2.92), smollm-python (0.20), proof-pile-2 (1.64),\n",
      "automathtext (1.17), open-web-math-pro (0.32), cosmopedia (0.53),\n",
      "fineweb-math (0.16), mathtext (0.12), metamathqa (0.01),\n",
      "orca-math (0.01), yulan-mini-syn-math-inst (0.02)\n",
      "\n",
      "dclm (1.80), fineweb-edu (16.20), english-books (1.20), pes2o (0.80),\n",
      "arxiv (1.20), wikipedia (0.40), dolma (1.24), cosmopedia-v2 (0.40),\n",
      "cicg-news (0.76), cn-baike (0.39), mnbvc-news (0.08),\n",
      "cn-book (0.24), cn-legal-case-law (0.36), zhihu-qa (0.12),\n",
      "the-stack-v2 (4.86), starcoder (2.92), smollm-python (0.20),\n",
      "proof-pile-2 (1.64), automathtext (1.17), open-web-math-pro (0.32),\n",
      "cosmopedia (0.33), fineweb-math (0.16), mathtext (0.12),\n",
      "metamathqa (0.01), orca-math (0.01), yulan-mini-syn-math-inst (0.02),\n",
      "yulan-mini-syn-math-doc (0.22)\n",
      "\n",
      "dclm (1.80), fineweb-edu (16.20), english-books (1.00), pes2o (0.80),\n",
      "arxiv (1.20), wikipedia (0.40), dolma (1.24), cosmopedia-v2 (0.60),\n",
      "cicg-news (0.76), cn-baike (0.39), mnbvc-news (0.08), cn-book (0.24),\n",
      "cn-legal-case-law (0.36), zhihu-qa (0.12), the-stack-v2 (4.85),\n",
      "starcoder (2.92), smollm-python (0.20), yulan-mini-syn-code-inst (0.03),\n",
      "proof-pile-2 (1.64), automathtext (1.17), open-web-math-pro (0.32),\n",
      "cosmopedia (0.29), fineweb-math (0.20), mathtext (0.12),\n",
      "metamathqa (0.01), orca-math (0.01), yulan-mini-syn-math-inst (0.02),\n",
      "yulan-mini-syn-math-doc (0.22)\n",
      "\n",
      "43\n",
      "\n",
      "\f11\n",
      "\n",
      "12\n",
      "\n",
      "13\n",
      "\n",
      "14\n",
      "\n",
      "15\n",
      "\n",
      "dclm (1.80), fineweb-edu (16.20), english-books (0.82), pes2o (0.80),\n",
      "arxiv (1.20), wikipedia (0.40), dolma (1.24), cosmopedia-v2 (0.78),\n",
      "cicg-news (0.76), cn-baike (0.39), mnbvc-news (0.08), cn-book (0.24),\n",
      "cn-legal-case-law (0.36), zhihu-qa (0.12), the-stack-v2 (4.56),\n",
      "starcoder (2.92), smollm-python (0.20), mnbvc-code (0.04),\n",
      "yulan-mini-syn-code-inst (0.28), proof-pile-2 (1.64),\n",
      "automathtext (0.93), open-web-math-pro (0.41), cosmopedia (0.16),\n",
      "fineweb-math (0.33), dclm-math (0.12), mathtext (0.12),\n",
      "metamathqa (0.01), orca-math (0.01), yulan-mini-syn-math-inst (0.02),\n",
      "yulan-mini-syn-math-doc (0.25)\n",
      "\n",
      "dclm (1.80), fineweb-edu (16.20), english-books (0.82), pes2o (0.80),\n",
      "arxiv (1.20), wikipedia (0.40), dolma (1.24), cosmopedia-v2 (0.78),\n",
      "cicg-news (0.76), cn-baike (0.39), mnbvc-news (0.08), cn-book (0.24),\n",
      "cn-legal-case-law (0.36), zhihu-qa (0.12), the-stack-v2 (4.44),\n",
      "starcoder (2.92), smollm-python (0.20), mnbvc-code (0.16),\n",
      "yulan-mini-syn-code-inst (0.28), proof-pile-2 (1.64),\n",
      "automathtext (0.63), open-web-math-pro (0.41), cosmopedia (0.03),\n",
      "fineweb-math (0.50), dclm-math (0.11), mathtext (0.12),\n",
      "basic-math-10m (0.04), metamathqa (0.01), orca-math (0.01),\n",
      "yulan-mini-syn-math-inst (0.06), yulan-mini-syn-math-doc (0.44)\n",
      "\n",
      "dclm (1.80), fineweb-edu (16.20), english-books (0.82), pes2o (0.80),\n",
      "arxiv (1.20), wikipedia (0.40), dolma (1.24), cosmopedia-v2 (0.78),\n",
      "cicg-news (0.76), cn-baike (0.39), mnbvc-news (0.08), cn-book (0.24),\n",
      "cn-legal-case-law (0.36), zhihu-qa (0.12), the-stack-v2 (4.44),\n",
      "starcoder (2.92), smollm-python (0.20), mnbvc-code (0.16),\n",
      "yulan-mini-syn-code-inst (0.28), proof-pile-2 (1.64),\n",
      "automathtext (0.58), open-web-math-pro (0.41), fineweb-math (0.51),\n",
      "dclm-math (0.15), mathtext (0.12), basic-math-10m (0.04),\n",
      "metamathqa (0.01), yulan-mini-syn-math-inst (0.08),\n",
      "yulan-mini-syn-math-doc (0.44)\n",
      "\n",
      "dclm (1.80), fineweb-edu (16.20), english-books (0.82), pes2o (0.80),\n",
      "arxiv (1.20), wikipedia (0.40), dolma (1.24), cosmopedia-v2 (0.78),\n",
      "cicg-news (0.76), cn-baike (0.33), mnbvc-news (0.08),\n",
      "cn-book (0.24), cn-legal-case-law (0.36), zhihu-qa (0.12),\n",
      "the-stack-v2 (4.44), starcoder (2.92), smollm-python (0.20),\n",
      "mnbvc-code (0.16), yulan-mini-syn-code-inst (0.28),\n",
      "proof-pile-2 (1.64), automathtext (0.57), open-web-math-pro (0.41),\n",
      "cosmopedia (0.02), fineweb-math (0.51), dclm-math (0.15),\n",
      "mathtext (0.12), basic-math-10m (0.04), metamathqa (0.01),\n",
      "yulan-mini-syn-math-inst (0.09), yulan-mini-syn-math-doc (0.44)\n",
      "\n",
      "dclm (1.80), fineweb-edu (16.20), english-books (0.82), pes2o (0.80),\n",
      "arxiv (1.20), wikipedia (0.40), dolma (1.24), cosmopedia-v2 (0.78),\n",
      "cicg-news (0.76), cn-baike (0.27), mnbvc-news (0.08),\n",
      "cn-book (0.24), cn-legal-case-law (0.36), zhihu-qa (0.12),\n",
      "the-stack-v2 (4.37), starcoder (2.32), smollm-python (0.20),\n",
      "mnbvc-code (0.83), yulan-mini-syn-code-inst (0.28),\n",
      "proof-pile-2 (1.34), automathtext (0.65), open-web-math-pro (0.41),\n",
      "cosmopedia (0.02), fineweb-math (0.42), dclm-math (0.40),\n",
      "mathtext (0.12), basic-math-10m (0.04), metamathqa (0.01),\n",
      "yulan-mini-syn-math-inst (0.13), yulan-mini-syn-math-doc (0.46)\n",
      "\n",
      "44\n",
      "\n",
      "\f16\n",
      "\n",
      "17\n",
      "\n",
      "18\n",
      "\n",
      "19\n",
      "\n",
      "20\n",
      "\n",
      "dclm (1.80), fineweb-edu (16.20), english-books (0.82), pes2o (0.80),\n",
      "arxiv (1.20), wikipedia (0.40), dolma (1.24), cosmopedia-v2 (0.78),\n",
      "cicg-news (0.76), cn-baike (0.27), mnbvc-news (0.10), cn-book (0.24),\n",
      "cn-legal-case-law (0.36), zhihu-qa (0.12), the-stack-v2 (4.27),\n",
      "starcoder (2.12), smollm-python (0.20), mnbvc-code (1.13),\n",
      "yulan-mini-syn-code-inst (0.28), proof-pile-2 (1.03),\n",
      "automathtext (0.68), open-web-math-pro (0.36), cosmopedia (0.02),\n",
      "fineweb-math (0.46), dclm-math (0.51), mathtext (0.12),\n",
      "basic-math-10m (0.04), yulan-mini-syn-math-inst (0.20),\n",
      "yulan-mini-syn-math-doc (0.57)\n",
      "\n",
      "dclm (1.80), fineweb-edu (16.20), english-books (0.82), pes2o (0.80),\n",
      "arxiv (1.20), wikipedia (0.40), dolma (1.24), cosmopedia-v2 (0.78),\n",
      "cicg-news (0.76), cn-baike (0.27), mnbvc-news (0.10), cn-book (0.24),\n",
      "cn-legal-case-law (0.36), zhihu-qa (0.12), the-stack-v2 (4.45),\n",
      "starcoder (2.12), smollm-python (0.20), mnbvc-code (1.13),\n",
      "yulan-mini-syn-code-inst (0.28), proof-pile-2 (0.55),\n",
      "automathtext (0.68), cosmopedia (0.02), fineweb-math (0.46),\n",
      "dclm-math (1.02), mathtext (0.12), basic-math-10m (0.04),\n",
      "yulan-mini-syn-math-inst (0.36), yulan-mini-syn-math-doc (0.57)\n",
      "\n",
      "dclm (1.80), fineweb-edu (16.20), english-books (0.82), pes2o (0.80),\n",
      "arxiv (1.20), wikipedia (0.40), dolma (1.24), cosmopedia-v2 (0.78),\n",
      "cicg-news (0.76), cn-baike (0.27), mnbvc-news (0.10), cn-book (0.24),\n",
      "cn-legal-case-law (0.36), zhihu-qa (0.12), the-stack-v2 (4.45),\n",
      "starcoder (2.12), smollm-python (0.20), mnbvc-code (1.13),\n",
      "yulan-mini-syn-code-inst (0.31), opencoder-llm-math-web (1.54),\n",
      "automathtext (0.00), cosmopedia (0.02), fineweb-math (0.17),\n",
      "dclm-math (0.82), mathtext (0.12), basic-math-10m (0.04),\n",
      "yulan-mini-syn-math-inst (0.52), yulan-mini-syn-math-doc (0.56)\n",
      "\n",
      "dclm (1.80), fineweb-edu (16.20), english-books (0.82), pes2o (0.80),\n",
      "arxiv (1.20), wikipedia (0.40), dolma (1.24), cosmopedia-v2 (0.78),\n",
      "cicg-news (0.76), cn-baike (0.02), mnbvc-news (0.02), cn-book (0.24),\n",
      "cn-legal-case-law (0.36), zhihu-qa (0.12), the-stack-v2 (4.41),\n",
      "starcoder (2.12), smollm-python (0.20), mnbvc-code (1.06),\n",
      "opencoder-llm-annealing (0.08), yulan-mini-syn-code-inst (0.31),\n",
      "opencoder-llm-math-web (0.38), cosmopedia (0.02), fineweb-math (0.10),\n",
      "dclm-math (0.82), infimm-webmath (1.23), mathtext (0.12),\n",
      "basic-math-10m (0.04), yulan-mini-syn-math-inst (0.55),\n",
      "yulan-mini-syn-math-doc (0.56)\n",
      "\n",
      "dclm (1.80), fineweb-edu (16.20), english-books (0.82),\n",
      "pes2o (0.80), arxiv (1.20), wikipedia (0.40), dolma (0.84),\n",
      "opencoder-llm-fineweb-corpus (0.40), cosmopedia-v2 (0.78),\n",
      "cicg-news (0.76), mnbvc-news (0.02), cn-book (0.26),\n",
      "cn-legal-case-law (0.36), zhihu-qa (0.12), the-stack-v2 (4.33),\n",
      "starcoder (2.12), smollm-python (0.20), mnbvc-code (0.92),\n",
      "opencoder-llm-annealing (0.16), yulan-mini-syn-code-inst (0.45),\n",
      "opencoder-llm-math-web (0.24), cosmopedia (0.02), fineweb-math (0.04),\n",
      "dclm-math (0.56), infimm-webmath (1.62), mathtext (0.12),\n",
      "basic-math-10m (0.01), yulan-mini-syn-math-inst (0.67),\n",
      "yulan-mini-syn-math-doc (0.53)\n",
      "\n",
      "45\n",
      "\n",
      "\f21\n",
      "\n",
      "22\n",
      "\n",
      "23\n",
      "\n",
      "24\n",
      "\n",
      "25\n",
      "\n",
      "dclm (1.80), fineweb-edu (16.20), english-books (0.82),\n",
      "pes2o (0.80), arxiv (1.20), wikipedia (0.30), dolma (0.84),\n",
      "opencoder-llm-fineweb-corpus (0.50), cosmopedia-v2 (0.78),\n",
      "cicg-news (0.76), mnbvc-news (0.02), cn-book (0.26),\n",
      "cn-legal-case-law (0.36), zhihu-qa (0.12), the-stack-v2 (4.19),\n",
      "starcoder (2.12), smollm-python (0.20), mnbvc-code (0.80),\n",
      "opencoder-llm-annealing (0.41), yulan-mini-syn-code-inst (0.45),\n",
      "opencoder-llm-math-web (0.24), cosmopedia (0.02), dclm-math (0.23),\n",
      "infimm-webmath (2.00), mathtext (0.12), yulan-mini-syn-math-inst (0.70),\n",
      "yulan-mini-syn-math-doc (0.53)\n",
      "\n",
      "dclm (1.80), fineweb-edu (16.20), english-books (0.82), pes2o (0.80),\n",
      "arxiv (1.20), dolma (0.84), opencoder-llm-fineweb-corpus (0.80),\n",
      "cosmopedia-v2 (0.78), cicg-news (0.76), mnbvc-news (0.02),\n",
      "cn-book (0.26), cn-legal-case-law (0.36), zhihu-qa (0.12),\n",
      "the-stack-v2 (4.60), starcoder (1.10), smollm-python (0.20),\n",
      "mnbvc-code (1.13), opencoder-llm-sft-s1 (0.20), opencoder-llm-annealing (0.38),\n",
      "yulan-mini-syn-code-inst (0.56), opencoder-llm-math-web (0.24),\n",
      "cosmopedia (0.02), dclm-math (0.22), infimm-webmath (1.98),\n",
      "mathtext (0.06), yulan-mini-syn-math-inst (0.78),\n",
      "yulan-mini-syn-math-doc (0.53)\n",
      "\n",
      "dclm (1.80), fineweb-edu (16.20), english-books (0.82), pes2o (0.80),\n",
      "arxiv (1.20), dolma (0.84), opencoder-llm-fineweb-corpus (0.80),\n",
      "cosmopedia-v2 (0.78), cicg-news (0.76), mnbvc-news (0.02),\n",
      "cn-book (0.26), cn-legal-case-law (0.36), zhihu-qa (0.12),\n",
      "the-stack-v2 (4.16), starcoder (0.80), mnbvc-code (0.85),\n",
      "opencoder-llm-sft-s1 (0.20), opencoder-llm-sft-s2 (0.15),\n",
      "opencoder-llm-annealing (1.20), yulan-mini-syn-code-inst (0.56),\n",
      "opencoder-llm-math-web (0.24), cosmopedia (0.02), dclm-math (0.22),\n",
      "infimm-webmath (2.06), mathtext (0.04), lean (0.02),\n",
      "yulan-mini-syn-math-inst (0.92), yulan-mini-syn-math-doc (0.56)\n",
      "\n",
      "dclm (1.80), fineweb-edu (16.20), english-books (0.82), pes2o (0.80),\n",
      "arxiv (1.20), dolma (0.84), opencoder-llm-fineweb-corpus (0.80),\n",
      "cosmopedia-v2 (0.78), cicg-news (0.76), mnbvc-news (0.02),\n",
      "cn-book (0.26), cn-legal-case-law (0.36), zhihu-qa (0.12),\n",
      "the-stack-v2 (4.07), starcoder (0.80), mnbvc-code (0.85),\n",
      "opencoder-llm-sft-s1 (0.25), opencoder-llm-sft-s2 (0.08),\n",
      "opencoder-llm-annealing (1.27), yulan-mini-syn-code-inst (0.56),\n",
      "opencoder-llm-math-web (0.24), cosmopedia (0.02), dclm-math (0.19),\n",
      "infimm-webmath (2.11), lean (0.04), yulan-mini-syn-math-inst (0.96),\n",
      "yulan-mini-syn-math-doc (0.56)\n",
      "\n",
      "dclm (1.80), fineweb-edu (16.20), english-books (0.82), pes2o (0.80),\n",
      "arxiv (1.20), dolma (0.84), opencoder-llm-fineweb-corpus (0.80),\n",
      "cosmopedia-v2 (0.78), cicg-news (0.76), mnbvc-news (0.02),\n",
      "cn-book (0.26), cn-legal-case-law (0.36), zhihu-qa (0.12),\n",
      "the-stack-v2 (4.11), starcoder (0.80), mnbvc-code (0.78),\n",
      "opencoder-llm-sft-s1 (0.25), opencoder-llm-sft-s2 (0.08),\n",
      "opencoder-llm-annealing (1.30), ioccc (0.00), yulan-mini-syn-code-inst (0.54),\n",
      "opencoder-llm-math-web (0.24), cosmopedia (0.02), fineweb-math (0.20),\n",
      "dclm-math (0.17), infimm-webmath (2.11), lean (0.04),\n",
      "yulan-mini-syn-math-inst (0.93), yulan-mini-syn-math-doc (0.45)\n",
      "\n",
      "46\n",
      "\n",
      "\f26\n",
      "(Decay)\n",
      "\n",
      "27\n",
      "(Decay)\n",
      "\n",
      "dclm (1.62), fineweb-edu (14.58), english-books (0.74), pes2o (0.72),\n",
      "arxiv (1.08), dolma (0.48), opencoder-llm-fineweb-corpus (1.00),\n",
      "cosmopedia-v2 (0.70), cicg-news (0.68), wizardlm-evol-instruct-v2-196k (0.04),\n",
      "less-data (0.04), claude-3-opus-claude-3.5-sonnnet-9k (0.01),\n",
      "slimorca (0.16), tulu-v3.1-mix-preview-4096-olmoe (0.20),\n",
      "supernova (0.03), magpie-reasoning-150k (0.09), spurline (0.01),\n",
      "celestia (0.04), mnbvc-news (0.01), cn-book (1.00),\n",
      "zhihu-qa (0.05), chinese-porety (0.03), the-stack-v2 (3.16),\n",
      "starcoder (0.18), mnbvc-code (0.70), opencoder-llm-sft-s1 (0.62),\n",
      "opencoder-llm-annealing (1.09), magicoder-oss (0.06),\n",
      "textbook-quality-programming (0.06), yulan-mini-syn-code-inst (3.00),\n",
      "code-290k-sharegpt (0.08), evol-codealpaca-v1 (0.04),\n",
      "magicoder-evol-instruct-110k (0.04), mathcodeinstruct\n",
      "(0.03), codefeedback-filtered-instruction (0.08),\n",
      "python-code-23k-sharegpt (0.01), evol-instruct-code-80k-v1 (0.03),\n",
      "codeexercise-python-27k (0.02), xcoder-80k (0.04),\n",
      "leetcode-solution-python (0.00), tachibana (0.03),\n",
      "opencoder-llm-math-web (0.24), cosmopedia (0.12), infimm-webmath (1.33),\n",
      "ape210k (0.01), polytope (0.03), yulan-mini-syn-math-inst (1.44),\n",
      "yulan-mini-syn-math-doc (0.58), mammothmathinstruct (0.04),\n",
      "openmathinstruct-1 (0.35), fol-nli (0.12), mathscaleqa-2m (0.28)\n",
      "\n",
      "dclm (1.44), fineweb-edu (12.96), english-books (1.48), pes2o (0.64),\n",
      "arxiv (0.96), dolma (0.20), opencoder-llm-fineweb-corpus (1.08),\n",
      "cosmopedia-v2 (0.70), cicg-news (0.61), wizardlm-evol-instruct-v2-196k (0.04),\n",
      "long-cot (0.65), slimorca (0.04), tulu-v3.1-mix-preview-4096-olmoe (0.25),\n",
      "evolkit-20k (0.02), orca-agentinstruct (0.49), transcript (0.01),\n",
      "spurline (0.01), titanium (0.02), celestia (0.06), cn-book (1.40),\n",
      "zhihu-qa (0.05), ruozhiba (0.00), chinese-porety (0.04),\n",
      "the-stack-v2 (1.50), mnbvc-code (0.38), opencoder-llm-sft-s1 (0.16),\n",
      "opencoder-llm-annealing (1.13), magicoder-oss (0.11),\n",
      "textbook-quality-programming (0.05), longwanjuan-github (1.82),\n",
      "yulan-mini-syn-code-inst (3.75), code-290k-sharegpt (0.04),\n",
      "evol-codealpaca-v1 (0.03), magicoder-evol-instruct-110k (0.03),\n",
      "mathcodeinstruct (0.02), codefeedback-filtered-instruction (0.01),\n",
      "self-oss-instruct-sc2-exec-filter-50k (0.02), xcoder-80k (0.04),\n",
      "tulu-code (0.02), codefuse-evol-instruct-clean (0.03),\n",
      "proof-pile-2 (0.60), opencoder-llm-math-web (0.45),\n",
      "cosmopedia (0.02), dclm-math (0.06), infimm-webmath (1.34),\n",
      "yulan-mini-syn-math-inst (1.72), yulan-mini-syn-math-doc (0.25),\n",
      "mammothmathinstruct (0.02), openmathinstruct-1 (0.02),\n",
      "tulu-math (0.14), tulu-math-grade (0.03), tulu-algebra (0.02),\n",
      "fol-nli (0.12), reasoning-0.01 (0.02), gretel-math-gsm8k-v1 (0.01),\n",
      "mathscaleqa-2m (0.40)\n",
      "\n",
      "47\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from markitdown import MarkItDown\n",
    "\n",
    "md = MarkItDown()\n",
    "result = md.convert(\"/Users/dylanli/Downloads/2412.17743v2.pdf\")\n",
    "print(result.text_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arxiv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
